{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3gJPo76AQpBAQViWBnuUr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install scikit-learn-extra"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf5NUf50UwX8","executionInfo":{"status":"ok","timestamp":1709402506093,"user_tz":480,"elapsed":12350,"user":{"displayName":"Krutik Pathak","userId":"04769521830706183158"}},"outputId":"cef4f59e-8994-4a7b-ea3a-bcf5e0d38086"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn-extra\n","  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.3.0)\n","Installing collected packages: scikit-learn-extra\n","Successfully installed scikit-learn-extra-0.3.0\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ1IsZyoUHhw","executionInfo":{"status":"ok","timestamp":1709405850119,"user_tz":480,"elapsed":1779,"user":{"displayName":"Krutik Pathak","userId":"04769521830706183158"}},"outputId":"bb4511cb-db94-4cb5-d346-ff99ae56edae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","texts count=10\n","texts=['a b s t r a c t feelings of belonging are integral in peoples choice of what career to pursue women and men are dispropor tionately represented across careers starting with academic training the present research focuses on two fields that are similar in their history and subject matter but feature inverse gender gapspsychology more women than men and philosophy more men than womento investigate how theorized explanations for academic gender gaps contribute to feelings of belonging specifically we simultaneously model the relative contribution of theoretically relevant individual differences empathizing systematizing and intellectual combativeness as well as life goals prioritization of family money and status to feelings of belonging and majoring in psychology or philosophy we find that men report higher intellectual combativeness than women and intellectual combat iveness predicts feelings of belonging and majoring in philosophy over psychology although systematizing and empathizing are predictive of belonging and in turn majoring in psychology and philosophy respectively when other factors are taken into account women and men do not differ in empathizing and systematizing women more than men report prioritizing having a family wealth and status in choosing a career and these directly or indirectly feed into feelings of belonging and majoring in psychology in contrast to prior theory together these findings suggest that students perceptions of their own combativeness and the extent to which they desire money and status play essential roles in womens feeling they belong in psychology and mens feeling they belong in philosophy women represent the majority of students in higher education but they are disproportionately represented across disciplines com pared to men women are overrepresented in fields such as human ities and psychology and underrepresented in others such as stem science technology engineering math and philosophy these gen der disparities are problematic both because they limit the perspec tives talents and diverse skills brought to bear on the work and be cause these disparities emerge despite women and mens equal capa bilities across various types of work not only can this inequity pre clude productivity and innovation eg deszs and ross 2012 but the lack of parity itself is unjust to better understand the factors that drive individuals towards or away from different fields and control for many of the typical confounds such as history and topic we in vestigate the inverse gender gaps across two fields that are similar in history ie were once one field and subject matter eg human na ture the mind ethics group interactions knowledge perceptions of  preregistration is available on the open science framework  corresponding author at heather maranges department of psychology winstonsalem nc 27109 email address hmarangesgmailcom hm maranges reality morality psychology and philosophy this comparison is par ticularly illuminating because both pipelines leak early with women not enrolling in or leaving philosophy and men not enrolling in or leaving psychology after introductory classes eg paxton et al 2012 yu et al 2020 the extent to which people feel that they belong within a particular field ie the extent to which individuals feel accepted by and similar to their group has a strong influence on whether they pursue a particular career path eg cheryan and plaut 2010 good et al 2012 holland and gottfredson 1976 walton  cohen 2011 yet to date little work has investigated which factors contribute to belonging in psychology versus philosophy but see thompson et al 2016 this is an important limitation and the focus of the current work because a better under standing of such factors has the potential to benefit academia broadly by informing interventions that would increase diversity  the goal of many universities professional societies and social organizations httpsdoiorg101016jcrbeha2023100097 received 3 october 2022 received in revised form 20 december 2022 accepted 7 january 2023 26665182 2023 the authors published by elsevier bv this is an open access article under the cc byncnd license httpcreativecommonsorglicensesbyncnd40 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 humans are universally motivated to connect form interpersonal relationships seek group membership and attain social acceptance by others ie they need to belong vandenbos 2007apa dictionary 2021 baumeister and leary 1995 these connections are facilitated by perceptions that one is similar to others or has the traits and prior ities necessary to succeed in a group or context eg in terms of de mographics personality interest priorities or ability evidence from stem fields suggests that satisfaction of the need to belong encourages students to participate and persist in an academic field whereas stu dents are discouraged from continuing when the need to belong is not met eg cheryan and plaut 2010 good et al 2012 walton  co hen 2011 for instance compared to men women who make up the minority of philosophy students tend not to believe that people like me could succeed in philosophy and perceive themselves as having lit tle in common with their philosophy instructors tutors or the typical philosophy major and are less likely to enroll in additional philoso phy courses thompson et al 2016 for similar results see baron et al 2015 feelings of belonging emerge from perceiving fit between oneself and the field eg morganson et al 2010 pler and hell 2012 prediger 1982 yet relatively little is known about the factors that de termine students sense of belonging we draw from two related domains of influences that have been theorized as explanations for gender gaps in academic disciplines the first domain of influences relates to individ ual differences including students levels of empathizing systematizing and intellectual combativeness the second domain of influences relates to life goals including prioritization of having a family having wealth and gaining social status specifically we tested the extent to which indi vidual difference and life goal factors contribute to feelings of belonging and in turn to major choice in the academic disciplines of psychology and philosophy in the population where the gender gaps emerge ie undergraduates while modeling the two domains of factors simultane ously the inverse gender gaps women outnumber men in psychology at north american universi ties women constitute over 70 of psychology graduates at each level of schooling  bachelors 79 masters 80 and doctorate 74 averaged across 201718 and 1819 national center for education statistics 2019  and 57 of faculty zippia career data 2021 in contrast in philosophy women are in the minority constituting less than 40 of philosophy students at each levelbachelors 39 mas ters 35 and doctorate 33 american academy of arts and sci ences humanities indicators 2016 paxton et al 2012  and 21 of faculty zippia career data 2021 these gaps first emerge at the undergraduate level both for initial enrolment and after introductory classes eg paxton et al 2012 yu et al 2020 we confirmed this early gender gap with data from concordia university in canada which allowed for sampling from a more diverse population1 across four aca demic years 201415 to 201718 see supplemental materials hence forth sm for detailed analyses and results women make up 81 of psychology majors whereas men make up only 19 in contrast men make up 66 of philosophy majors whereas women make up only 34 however adding some nuance to when the training pipeline leaks we find the gap appears to widen most for philosophy after the first year ie average dropout of women of 32 largely replicating past find ings and after the second year for psychology ie average dropout of men of 12 the inverse gender gaps of psychology and philosophy are striking given that the two fields were once one sharing historical roots and focused on similar topics determining what makes students feel they belong is likely central to understanding why students enroll and major in psychology versus philosophy 1 compared eg to convenience samples gurven 2018 theoretical accounts of the inverse gender gaps one domain of explanations for the inverse gender gaps across aca demic fields relates to individual differences specifically gender differ ences in the tendency towards empathizing ie motivation and abil ity to identify and respond to others emotions versus systematizing ie engaging with abstract ideas logic and interested in systems2 baroncohen et al 2003 and the combative nature of maledominated fields eg moulton 1983 beebee 2013 have been explored accord ing to this individual difference perspective women and men fit with and choose fields stereotyped as feminine eg psychology or mascu line eg philosophy because on average women score higher on em pathizing and men score higher on systematizing eg focquaert et al 2007 greenberg et al 2018 although there is empirical support for this argument eg focquaert et al 2007 greenberg et al 2018 see also pler and hell 2012 zell et al 2015 it has received criticism for ignoring the differential socialization of women and men concep tual issues surrounding the measure and questions regarding the pre dictive power of the measures over other factors eg leslie et al 2015 maranges et al 2021 moreover the gender divide in constructs related to empathizing vs systematizing shrinks when the constructs are mea sured in more naturalistic contexts eg eisenberg and lennon 1983 see fine 2010 importantly whether there is a realworld difference between women and men on these individual difference dimensions and if so whether that difference is the result of nature or nurture might be less important than the question of whether people believe that there is a difference that is even if women and men do not differ on empathiz ing and systematizing in a nonsexist society there is no doubt that both are useful measures insofar as they capture current perceptions expec tations and internalized stereotypes for example philosophy under graduates including women explicitly associate philosophy with some one who is intelligent intellectual logical curious and who is male di bella et al 2016 similarly when students are instructed to think about someone who is logical they think of a man beebee 2013 in contrast psychology is perceived as a field that does not require bril liance leslie et al 2015 or systematizing eg amsel et al 2014 pettijohn et al 2015 and is associated with stereotypically feminine traits eg emotional empathetic and motivations eg altruism in timacy boysen et al 2021 given the related associations between these two individual differences gender and academic fields we in clude empathizing and systematizing as candidate mediators in the link between gender and feelings of belonging in psychology versus philos ophy another individual difference that may influence feelings of belong ings relates to comfort with intellectual combativeness compared to psychology work in philosophy is solitary rather than collaborative and when scholars do come together to share their work the dis course is typically argumentative and confrontational which may be construed as better fitting for men than women based on gender stereo types moulton 1983 beebee 2013 indeed compared to men women predicted they would feel less comfortable participating in introduc tory philosophy class discussions than men baron et al 2015 but see thompson et al 2016 hence the extent to which gender influences students views of themselves as intellectually combative may be asso ciated with feelings of belonging in philosophy but not psychology a separate domain of explanations that vary with gender and may influence feelings of belonging in diverse academic disciplines relates to life goals traditionally aged undergraduates are identifying and navi gating life goals including those related to career and family careers may vary in the extent to which they facilitate or are perceived to facil 2 according to baroncohen et al 2003 systems take inputs which can then be operated on in variable ways to deliver different outputs in a rule governed way 2 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 itate the goals of having a family financial security andor social sta tus moreover people vary in the extent to which they prioritize those factors accordingly some scholars have suggested that peoples prior itization of family wealth and status can help us understand why stu dents choose to pursue one discipline over others eg kessels 2005 montmarquette et al 2002 morgan et al 2013 when it comes to balancing a career with family men and women face considerably different challenges and social expectations though men are increasingly invested in coparenting and often praised for want ing to balance a career with family life women still shoulder the ma jority of the childcare responsibilities craig 2006 kotila et al 2013 importantly women are typically not praised for desiring worklife bal ance correll et al 2007 see also luhr 2020 but rather are expected to provide the lions share of childcare as a result women are more likely to expect workfamily conflict across a variety of careers coyle et al 2015 although findings suggest that prioritization of a family does not directly predict the gender gap research in stem demonstrates that the perception that a field is less supportive of having a family may lower ones feelings of belongingness morgan et al 2013 because fields as sociated with women are perceived as allowing one to prioritize hav ing a family eg weisgram et al 2011 psychology may be viewed as being more familyfriendly than philosophy moreover many peo ple may associate a degree in philosophy with an academic career and academic careers are perceived as unfriendly to having a family eg tanwilson and stamp 2015 whereas a psychology degree is thought to offer flexible career options eg counselor assessor to this end we test whether psychology compared to philosophy is viewed as fa cilitating having a family beyond whether ones career is considered compatible with fam ily demands students career choices may be influenced by the extent to which a field is viewed as supportive of the goal of financial secu rity young people tend to be attracted to and choose majors that they expect to provide them with wealth eg national longitudinal sur vey of youth data 1993 as assessed by montmarquette et al 2002 and this is especially true for men for metaanalysis see konrad et al 2000 but see montmarquette et al 2002 there appears to be more economic opportunity for psychology majors than for philosophy ma jors carnevale et al 2015 and even when new opportunities arise for philosophy majors their perceptions lag behind weinberg 2018 based on such findings we could expect that students perceive psy chology versus philosophy as allowing one to prioritize money in ones career and as a result men would be more likely to choose psychol ogy over philosophy yet this prediction opposes the observed gender gaps on average ie that men are attracted to philosophy over psychol ogy perhaps men are more likely to view academia overall including both psychology and philosophy as a viable financially secure career path which would help explain the violation of such predictions al ternatively it may be that mens prioritization of wealth from a career serves as a buffer against gendered perceptions for belonging in psychol ogymen may experience stronger feelings of belonging in psychology to the extent that it relates to money because they value money more than women here we test whether men prioritize money more than women and whether such a priority serves to buffer mens feelings of belonging in psychology the last life goal we considered relates to having a prestigious rep utation or position relative to others in society an affordance which varies across fields and careers desire for social status is a univer sal human motivation anderson et al 2015 and men are perceived as more attractive to the extent that they demonstrate this motivation buss 1999 buss et al 2020 dewall and maner 2008 prioritization of status is a primary factor that drives choices about careers for uni versity students eg haase and lautenschlager 2011 in particular students appear driven to study in stereotypeconsistent fields to acquire status eg kessels 2005 thus it may be that women who strongly desire status feel a stronger sense of belonging in psychology stereo typed as feminine and men who strongly desire status feel a stronger sense of belonging in philosophy stereotyped as masculine with major choices aligning with these feelings of belonging likewise we expect that men will view philosophy compared to psychology as garnering more social status compared to women and vice versa3 summary of hypotheses that more women than men study psychol ogy and more men than women study philosophy is likely driven by the extent to which students feel they belong in and are therefore more inclined to major in a field the current study assesses to what extent competing explanatory factors contribute to feelings of belonging in psy chology and philosophy specifically we aim to understand whether stu dents individual differences ie empathizing systematizing and intel lectual combativeness as well as life goals ie prioritization of family money and status mediate the link between gender and belonging and in turn choice to major in psychology or philosophy we hypothesize that women will view themselves as more empathizing less systematiz ing and less combative and will report greater prioritization of family and weaker prioritization of money compared to men we expect that relative to women men will perceive themselves as less empathizing more systematizing and more combative and report weaker prioritiza tion of family and stronger prioritization of wealth moreover we do not expect a gender difference in prioritization of status rather we expect that men who desire high status may feel more belonging in philoso phy whereas women who desire high status may feel more belonging in psychology we also expect that higher perceptions of empathizing and prioritization of family and wealth will predict majoring in psychol ogy over philosophy and higher systematizing and combativeness will predict majoring in philosophy over psychology it is important to keep in mind that these factors are unlikely to be in nate differences and are likely the result of both socialization and stereo types eg bian et al 2017 2018ab hentschel et al 2019 kite et al 2008 thus the results of our model can inform practical interventions aimed at closing the gender gaps method the present study was part of a preregistered empirical project on the open science foundation osf httpsosfio7yauqview_only 875a58705ca74c2b95204dbaf1ec69ed participants students enrolled in philosophy or psychology classes in diverse eg large state small liberal arts and ivy league universi ties across the united states and canada were invited to respond to an online survey about career choices and their future4 we excluded peo ple who began but did not complete the majority of the survey n  179 failed the attention check ie i would say that i am paying attention as evidenced by my choosing the most negative option now n  109 asked to withdraw their data n  2 andor were not philosophy or psy chology majors n  1415 our final sample included 241 psychology and philosophy majors 181 women 62 men6 mage  2163 sd  519 654 white 169 hispanic or latino 66 chinese 66 black or african 46 indian 16 native or indigenous 58 indian 16 iranian 33 other see sm for more details on recruitment of partic ipants 3 there is also work that suggests womens participation in highstatus fields has increased over time and that psychology is not the highest status field eg lippa et al 2014 such that women who are highly statusprioritizing may be more likely to major in another field than psychology or philosophy 4 data were collected in a single wave in fall 2020 via survey_v4 see osf 5 we retain these participants for one test of a preregistered hypothesis that womens level of prioritization of status would predict major such that women higher in prioritization of status elect to major in something other than psychol ogy of philosophy 6 without enough statistical power to analyze and make conclusions from data of participants who identify as noncis men and women we did not analyze data from people who identified as transgender men n  1 postgender n  2 non binary n  7 genderqueer n  1 genderfluid n  1 demifemale n  1 3 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 procedure and materials after participants provided consent they reported their major npsych  167 nphilo  74 and demographic de tails participants then responded to measures of belonging in psychol ogy and philosophy and reported on their own empathizing system atizing and combativeness as well as the extent to which they prioritize family financial resources and status in choosing a career participants also reported on which of the two fields they believe better facilitates prioritization of family money and status items were presented in a random order see sm for all additional items and details related to scale development feelings of belonging participants responded to 3 items that captured their feelings of be longing in psychology and 3 nearly identical items that captured their feelings of belonging in philosophy eg i feel comfortable in the aca demic environment of psychology philosophy on a 1 strongly disagree to 7 strongly agree scale psychology m  485 sd  141   083 philosophy m  364 sd  173   089 we adapted these items from walton  cohen 2007 for all items see sm individual difference factors empathizing participants responded to 7 items that measured to what extent they are motivated to identify and respond to other peoples emo tions baroncohen et al 2003 such as i can tune into how someone feels on a scale from 1 strongly disagree to 7 strongly agree m  507 sd  083   071 systematizing participants responded to 9 items that measured to what extent they are motivated to analyze or construct systems ie that which takes and operates on inputs to deliver outputs according to some particular rules baroncohen et al 2003 such as i like to think about abstract ideas on a scale from 1 strongly disagree to 7 strongly agree m  453 sd  089   077 intellectual combativeness participants responded to 7 items that cap tured the extent to which they enjoy and engage in intellectual de batesconfrontation at the cost of social comfort on a scale from 1 strongly disagree to 7 strongly agree for example items include in defending my ideas i can sometimes frustrate other people and if a de bate or a theoretical discussion gets heated i feel uncomfortable reversed m  471 sd  111   081 life goal factors prioritization of family participants responded to 4 items that cap tured the extent to which they personally desire a career that facilitates having and time with a family using a 1 strongly disagree to 7 strongly agree scale eg i prefer a job that would allow for flexible work hours for family and children m  472 sd  165   088 prioritization of money participants responded to 3 items that re flected their personal desire for wealth on a 1 strongly disagree to 7 strongly agree scale eg i want to make a lot of money in my career m  487 sd  146   087 prioritization of status participants responded to 3 items that mea sured the extent to which they desire status and prestige via their ca reer using a 1 strongly disagree to 7 strongly agree scale eg i want a career that gets respect and admiration m  475 sd  124   078 perceptions of the fields prioritization of family participants responded to 5 items that cap tured the extent to which they believe a field facilitates having a family by choosing between psychology and philosophy eg a ca reer in ______ facilitates the worklife balance essential for having a fam ily we calculated perceptions of fit between prioritization of family and psychology as compared to philosophy by dividing the number of times psychology was chosen over philosophy by the total number of items ie 5 and vice versa to calculate perceptions of philoso phy as compared to psychology such that these values are inversely related prioritization of money participants responded to 3 items that cap tured the extent to which they believe a field facilitates making money by choosing between psychology and philosophy eg _____ is well suited to those who want to make a lot of money in their career per ceptions of fit between prioritization of money and psychology as compared to philosophy was calculated by dividing the number of times psychology was chosen over philosophy by the total number of items ie 3 and vice versa to calculate perceptions of philoso phy as compared to psychology such that these values are inversely related prioritization of status participants responded to 5 items that cap tured the extent to which they believe a field facilitates gaining so cial status by choosing between psychology and philosophy eg work ing in ______ would allow a person to build a name and reputation for himherself we calculated perceptions that status can be gained in psychology as compared to philosophy by dividing the number of times psychology was chosen over philosophy by the total number of items ie 5 and vice versa to calculate perceptions of philoso phy as compared to psychology such that these values are inversely related results and discussion to assess the relations between gender field of study feelings of belonging and factors related to individual differences ie empathiz ing sympathizing and combativeness as well as life goals ie priori tization of family wealth and status we examined the bivariate zero order associations between our variables table 1 we then examined participants views as to how psychology versus philosophy facilitate said goals paired ttests how such factors predict feelings of belong ing and majoring in psychology versus philosophy and how these rela tions vary by gender ie interaction effects see sm we test whether gender and the different factors predict feelings of belonging and major choice using a series of regressions ie linear regression for feelings of belonging as outcome and binomial logistic regression for major as outcome finally a structural equation model sem namely a path analysis was used to simultaneously assess how individual difference and life goal factors contribute to the gender gaps in psychology and philosophy ie partially explain variance in the link between gender and belonging in philosophy and psychology which in turn predict major we employ sem instead of basic mediation because sem allows for complex se quential mediation and moderation and simultaneous assessment of the associations among gender individual difference factors ie empathiz ing systematizing combativeness life goal factors ie prioritization of family prioritization of money and status and feelings of belonging in psychology and philosophy as well as their relative contributions to major see fig 1 gender and field of study there was a positive correlation between majoring in psychology vs philosophy and identifying as a woman vs man see table 1 such that women were more likely to major in psychology and men in phi losophy of psychology majors 83 were women however perhaps because women were overall more willing to respond to our survey we collected more data from women 56 than men 44 in philos ophy nonetheless our pilot study of aggregated enrollment data from 201415 201516 201617 and 20171 at our institution replicates this pattern more women than men enroll in classes in psychology when beginning university and more men than women enroll in philosophy classes when beginning university see figs 1s  4s 4 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 table 1 correlations among primary variables 1 2 3 4 5 6 7 8 9 1 major 0philosophy 1psychology  2 gender 0man 1woman 29  3 belonging in psychology 72 26  4 belonging in philosophy 065 025 046  5 empathizing 26 17 35 010  6 systematizing 043 026 038 55 095  7 intellectual combativeness 033 019 011 33 001 026  8 prioritization of family 11 004 20 014 22 026 01  9 prioritization of money 36 13 35 035 08 021 08 27  10 prioritization of status 01 002 08 02 02 07 021 01 47 note p  10 p  05 p  01 p  001 fig 1 belonging in psychology and philosophy path model note contributors to feelings of belonging in psychology are represented in italics and contributors to feelings of belonging in philosophy are represented in bold p  05 p  01 p  001 feelings of belonging feelings of belonging in psychology were negatively associated with feelings of belonging in philosophy see table 1 suggesting that the more students feel they belong in psychology the less they feel they belong in philosophy and vice versa gender was also positively asso ciated with belonging in psychology and negatively with belonging in philosophy women compared to men report feeling more belonging in psychology and men compared to women report feeling more belong ing in philosophy gender and individual difference factors for each individual difference factor we first describe significant bi variate associations with gender and belonging in psychology and phi losophy see table 1 for correlations and then describe interaction anal yses ie between gender and individual differences in predicting be longing and majoring see sm empathizing women compared to men perceived themselves as more empathizing per the positive association between gender and em pathizing see table 1 for correlations and table 2 for means and stan dard deviations moreover empathizing was positively associated with belonging and majoring in psychology table 1 such that the more empathizing people perceive themselves to be the more they feel they belong in psychology and the more likely they are to major in that field table 2 means and standard deviations for individual difference factors major individual differences philosophy psychology empathizing 476 101a 521 070 men 450 095 520 056 women 496 103 521 073 systematizing 510 067 428 086 men 530 060 450 056 women 494 069 423 090 combativeness 524 102 447 107 men 513 090 498 113 women 533 111 437 103 note amean sd responses were on a 7point scale 1  strongly disagree to 7  strongly agree empathizing and gender interacted to predict feelings of belonging in philosophy but did not interact to predict feelings of belonging in psy chology see analysesin smstarting on page 10 at low not high levels of empathizing there is an effect of gender on feelings of belonging in philosophy for people who report higher empathizing tendencies men and women are no different in how much they feel they belong in phi losophy but for people low in empathizing men feel like they belong in philosophy more than women who feel they belong in psychology ex amining the interaction between gender and empathizing another way within gender we see that among men there is an effect of empathiz 5 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 ing such that men who reported higher selfperceptions of empathizing felt less belonging in philosophy compared to men who reported lower empathizing whereas there was no difference among women in other words perceptions of empathizing predicted feelings of belonging in philosophy for men but not women finally gender and empathizing did not interact to predict in majoring in either psychology or philoso phy systematizing men compared to women perceived themselves as more systematizing see positive correlation between gender and sys tematizing in tables 1 and 2 for means and standard deviations sys tematizing was associated negatively with feelings of belonging in psy chology positively with feelings of belonging in philosophy and nega tively with majoring in psychology over philosophy or positively with majoring in philosophy over psychology table 1 systematizing and gender interacted to predict feelings of belonging in psychology but not feelings of belonging in philosophy see sm start ing on page 14 at high but not low levels of systematizing gender predicts feelings of belonging in psychology that is for people low in systematizing men and women do not differ in their feelings of belong ing in psychology but for people who are high in systematizing men feel less belonging and women feel more belonging in psychology for both women and men there is an effect of systematizing such that the more systematizing a student perceives themselves to be the less they feel they belong in psychology but this effect appears larger for men combativeness men selfreported more intellectual combativeness than did women per negative correlation between gender and combat iveness tables 1 see 2 for means and standard deviations moreover combativeness was positively associated with feelings of belonging and majoring in philosophy table 1 combativeness interacted with gender to predict feelings of belong ing and majoring in philosophy but not psychology see sm starting on page 17 at low not high levels of combativeness gender predicts feelings of belonging and majoring in philosophy that is for people who perceive themselves as being low in combativeness men feel more belonging and are more likely to major in philosophy compared to women however this gender difference disappears for people high in combativeness such that women high in combativeness are just as likely to feel like they belong or go into philosophy as men high in combat iveness probed another way women who perceive themselves as more combative are more likely to feel they belong and major in philosophy but combativeness is not predictive for men in sum compared to men women view themselves as more em pathizing and less systematizing and combative higher empathizing and lower combativeness and systematizing were associated with ma joring in psychology over philosophy inversely lower empathizing and higher combativeness and systematizing were associated with major ing in philosophy over psychology students felt stronger feelings of belonging in psychology when they perceived themselves to be more empathizing and less systematizing and to a nonsignificant extent less combative while feelings of belonging in philosophy were associated with higher systematizing and combativeness gender and factors related to life goals prioritization of family as predicted psychology m  065 sd  036 was seen as facilitating having a family more than was philos ophy m  034 sd  035 t240  688 p  001 men and women did not differ in the extent to which they prioritize family in choos ing a career as evidenced by no significant association between gender and prioritization of family see table 1 for correlations and table 3 for means and standard deviations prioritization of family predicted feel ings of belonging in psychology positively but in philosophy negatively however prioritization of family was not significantly directly associ ated with majoring in psychology over philosophy ie this association was marginal table 1 table 3 means and standard deviations for life goals factors major life goals philosophy psychology prioritization of family 447 168a 484 164 men 434 152 538 100 women 457 180 472 173 prioritization of money 408 171 520 120 men 370 153 546 107 women 437 180 515 122 prioritization of status 469 143 475 114 men 441 128 519 093 women 492 151 467 117 note amean sd responses were on a 7point scale 1  strongly disagree to 7  strongly agree moreover prioritization of family interacted with gender to predict belonging in psychology but not philosophy see sm starting on page 22 at low but not high levels of prioritization of family there is an effect of gender for people who highly value having a family men and women do not differ in their feelings of belonging in psychology the more people prioritized family the more they felt they belonged in psy chology for people who prioritize family in their career choice to a comparatively low extent women are more likely than men to feel they belong in psychology similarly prioritization of family interacted with gender to predict major at low but not high levels of prioritizing a family in career choice women are more likely than men to major in psychology over philosophy and vice versa no such difference was found among people who highly value having a family probing this another way prioritiza tion of family predicts major for men but not women men who strongly prioritize being able to have a family in a career are more likely to major in psychology over philosophy compared to men who do not prioritize having a family in a career prioritization of money as predicted psychology m  089 sd  027 was seen as the career that provides financial resources rather than philosophy m  011 sd  027 t238  2242 p  001 unexpectedly women more than men prioritized gaining wealth from a career and that prioritization of money was associated with feelings of belonging and majoring in psychology see table 1 for correlations and table 3 for means and standard deviations in moderation analyses see sm starting on page 27 prioritization of money interacted with gender to predict belonging in psychology but did not predict belonging in philosophy specifically at low but not high levels of prioritization of money in ones career women more than men feel they belong in psychology there is no gender difference in feelings of belonging in psychology among people who highly pri oritize making a lot of money from their career that is for both men and women prioritizing making a lot of money in their career predicts belonging in psychology but this effect is stronger for men according to analyses probing the interaction by gender prioritization of money and gender also interacted to predict major in a similar pattern for people who highly prioritize gaining wealth from their career there is no gender difference in choice of major however for people who prioritize making money from their career to a lower degree women are more likely than men to major in psychology over philosophy probing the interaction another way ie by gender the more students prioritize making a lot of money in a career the more likely they are to major in psychology over philosophy but this effect is stronger for men than women prioritization of status in contrast to predictions overall psychology m  062 sd  027 was seen as providing more social status than philosophy m  037 sd  027 t240  733 p  001 both men t59  431 p  001 and women t179  621 p  001 evinced 6 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 the same differential perceptions of the fields7 men and women did not differ with respect to prioritizing status from their career nor did prioritization of status predict feelings of belonging or majoring in either field see table 1 for correlations and table 3 for means and standard deviations prioritization of status and gender interacted to predict belonging and majoring in psychology over philosophy but they did not inter act to predict feelings of belonging in philosophy see sm starting on page 32 at low but not high levels of prioritizing status in choos ing a career there is an effect of gender with lower status seeking women having stronger feelings of belonging in psychology compared to lower status seeking men in contrast for people who highly prior itize gaining status via their careers there is no gender difference in belonging in psychology probing this another way for men but not women there is an effect of prioritizing status from a career such that the more they desire status the more men feel they belong in psychol ogy prioritization of status and gender interacted to predict majoring in psychology over philosophy in a similar way at low levels of pri oritization of status women are more likely to major in psychology than men but there is no effect of gender for higher status seekers though probing the interaction another way status is more predic tive of major for men than for women with higher prioritization of sta tus predicting majoring in psychology over philosophy for men but not women in sum across all life goals by gender ie prioritization of family money and status interaction analyses a similar pattern emergesat high levels men and women do not differ in belonging or major but at low levels they do this pattern suggests that when peoples prioriti zation of family or money or status is relatively low and therefore not guiding their belonging and major they seem to feel a stronger sense of belonging and are more likely to major in stereotypeconsistent majors with women favoring psychology and men favoring philosophy structural equation model sem method and model we built an sem in order to assess whether and how the relevant individual difference and life goal factors medi ate or moderate the association between gender and belonging and in turn majoring in psychology or philosophy when simultaneously mod elled the model contained all significant bivariate associations see table 1 and significant interactions see moderation analyses sm be tween gender and the six explanatory factors based on theory we modelled individual differences ie empathizing systematizing and intellectual combativeness as preceding life goals ie prioritization of family money and status in contributing to feelings of belonging and subsequently major we then removed all nonsignificant interac tions ie empathizing x gender  belonging in philosophy combat iveness x gender  belonging in psychology prioritization of family x gender  belonging in psychology and recalculated pathway es timates we then removed nonsignificant pathways from the stream lined model ie gender  major gender  empathizing combat iveness  belonging in psychology prioritization of family  belong ing in psychology prioritization of family  belonging in philoso phy see sm page 37 which yielded our final model displayed in fig 1 all variables were measured ie none were latent factors our analyses can hence be described as a path analysis the path analy sis was computed using amos arbuckle 2014 and a markov chain monte carlo mcmc bayesian technique because the model included dichotomous variables ie gender major which disallows normally 7 these patterns of results for perceptions of the fields replicated when includ ing nonpsychology and philosophy majors to ensure our results were not an artifact of having more psychology student participants prioritization of family t384  952 p  001 prioritization of money t382  2530 p  001 prioritization of status t384  807 p  001 distributed outcome variables see choi and levy 2017 mcmc is a simulationbased estimation method it empirically samples from the current data monte carlo methods build on random simulation and markov chain methods build on samples that are independent from each other our mcmc model was specified in the following way burnin ob servations were set to 500 the random walk tuning parameter to 07 the number of batches for bath means to 20 and the convergence criterion to 100 interpreting model see fig 1 we interpret the model from left to right and top to bottom within columns of mediators ie from gen der to major importantly the direct association between gender and major was no longer significant when taking individual differences ie empathizing systematizing and combativeness and life goals ie pri oritization of family money and status into account such factors ac count for significant variance in the relationship between gender and major and help explain gendered feelings of belonging and subsequent gender gaps in psychology and philosophy when all factors were taken into account gender was no longer uniquely associated with empathizing and systematizing still em pathizing plays an important role the more people report being em pathizing the more they prioritize family and the more they feel they belong in psychology which predicts majoring in psychology over phi losophy empathizing and systematizing were inversely related such that the more empathizing people were the less systematizing they were systematizing contributed negatively to prioritization of family and of money and therefore indirectly to lower feelings of belonging in psy chology systematizing also directly negatively predicted feelings of be longing in psychology and positively predicted feelings of belonging in philosophy which were in turn associated with higher likelihood of ma joring in philosophy over psychology these were the two strongest ef fects in our model put another way people who perceive themselves as highly systematizing do not highly value having a family or mak ing a lot of money in a career and strongly feel that they belong in philosophy but not in psychology consequently they tend to major in philosophy not psychology although gender did not directly predict systematizing such that men or women were more systematizing when taking all other factors into account gender interacted with systematiz ing to predict belonging in psychology specifically men who are high in systematizing feel especially low belonging in psychology compared to women who are equally high in systematizing this suggests that for women high in systematizing aspects of the culture of psychology are still welcoming to them but less so for men systematizing is moderately related to intellectual combativeness intellectual combativeness differs by gender such that men appear more combative than women and positively predicts the prioritization of status from ones career as well as higher feelings of belonging in philosophy although higher prioritization of status is associated with stronger feelings of belonging in psychology and subsequently majoring in psychology the higher feelings of belonging in philosophy associated with high combativeness predicts majoring in philosophy prioritization of family in choosing a career is related to more em pathizing less systematizing and more prioritization of money but it is not predicted or moderated by gender and does not predict feelings of belonging this is to say men and women do not differ in the extent to which they want to have a family and the desire to have a family friendly career does not directly feed into feelings of belonging in one field over the other instead our model suggests that the desire to have a family contributes to the desire to make a lot of money in ones career which in turn contributes to feelings of belonging in and therefore ma joring in psychology over philosophy prioritization of money also inter acted with gender to predict belonging in psychology namely whereas women and men who highly prioritize money feel equal belonging in psychology women who deprioritize money feel they belong in psy chology more than do men who deprioritize money from their careers without a desire to have wealth which is more easily accomplished via 7 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 psychology than philosophy men do not experience the same strong feelings of belonging that women experience again suggesting that the culture of psychology may be friendlier to women prioritization of money was also associated with prioritization of status which positively predicted feelings of belonging in psychology which predict majoring in psychology over philosophy notably priori tization of status interacted with gender also as with the prioritization of money women are more likely than men to feel they belong in psy chology when they do not value garnering status via their career but also probing this another way for men but not women the more they desire status the more men feel they belong in psychology high feelings of belonging in psychology in turn predict majoring in that subject feelings of belonging in psychology and feelings of belonging in philosophy were inversely related such that being high in one tended to predict low levels of the other and of course feel ings of belonging in psychology predicted majoring in that field and feelings of belonging in philosophy predicted majoring in that field summary of findings our model highlights individual difference fac tors and life goals that account for feelings of belonging in academic psychology and philosophy in turn influencing the likelihood of ma joring in either field students are likely to feel they belong and major in psychology to the extent that they perceive themselves as highly em pathizing less systematizing and highly prioritizing family money and status in their career these students tend to be women students tend to feel they belong and major in philosophy to the extent that they per ceive themselves as highly systematizing and intellectually combative these students tend to be men adding some nuance the interaction effects appear to reflect what we might think of as buffers against the gendered culture of psychology for men men who were highly oriented toward systematizing relatively unconcerned about making money in their careers or did not value gaining status via their careers felt that they do not belong in psychology to the same extent that women with comparable traits and priorities did general discussion women and men are disproportionately represented across academic fields psychology and philosophy share history and topical interests yet feature inverse gender gaps more women than men study psychology and more men than women study philosophy data from our own in stitution confirm that pattern and further finds that the pipelines leak early in introductory undergraduate courses it is important to under stand contributors to the feelings of belonging in these fields which are likely a proximate and powerful driver of peoples decisions about what to study given the importance of perceived fit between person and field eg cheryan and plaut 2010 good et al 2012 holland and gottfredson 1976 morganson et al 2010 pler and hell 2012 prediger 1982 walton  cohen 2011 the present research exam ined factors that likely play a role in the maintaining the gender dis parities across psychology vs philosophyie hypotheses that women are more empathizing and prioritizing of family and that men are more systematizing intellectually combative and prioritizing of money and statusand importantly how these factors contribute to feelings of be longing we examined whether and how gender predicts major through per ceptions of ones individual difference traits empathizing systematiz ing and combativeness life goals prioritization of family money and status and in turn feelings of belonging in psychology and philosophy in undergraduate students across north america critically when these factors are considered gender no longer directly predicts major suggest ing that individual differences and life goals that vary with gender may be the proximal cause of the gender gap in psychology and philosophy individual differences and life goal profiles for psychology and philosophy in choosing what career to pursue people compare their personal characteristics to those that appear to dominate the field to assess fit past work casts peoples perceptions of psychology as an empathetic communal field eg holland and gottfedson 1976 and as we find in the present work supportive of having a family wealth and social status philosophy in contrast is seen as the intellectually combative field and as our results suggest unhelpful in pursuing the goals of fam ily wealth and social status we empirically examine the individual difference and life goal profiles of students who feel they belong in and therefore major in psychology versus philosophy the picture is complex although perceptions of their own levels of empathizing systematizing and intellectual combativeness played important roles in feelings of belonging and major they were not merely gendered differences as past work might suggest eg baron cohen et al 2003 when evaluated in light of other factors like com bativeness and life goals men and women did not differ in empathizing and systematizing still empathizing and systematizing play important though not sim ply gendered roles in explaining the gender gap people who view themselves as high in empathizing feel they fit in psychology consis tent with theory and also more strongly desire a career that facili tates having a family prioritization of family in a career is likewise not gendered this takes concerns about familywork balance off the table as a direct explanation of the gender gaps in psychology and phi losophy instead prioritization of family might motivate prioritization of money which more directly contributes to feelings of belonging in psychology systematizing does a lot of work in explaining students feelings of belonging and majoring in psychology vs philosophy ie it is statisti cally associated with many of the contributing factors and these associ ations are of relatively large magnitude students who perceive them selves as highly systematizing feel particularly low belonging in psy chology and particularly high belonging in philosophy perceiving one self as highly systematizing does have different implications for men and womens feelings of fit though compared to highly systemizing women highly systemizing men felt alienated ie lower feelings of belonging in psychology that is it is not that men tend to be more systematizing than women when other factors are considered rather women who are highly systematizing may feel buffered against the incompatibility between their nature and that of psychology ie perceived as empathiz ing compared to men who are highly systematizing it is unclear what that buffer is it could be gender on its own providing the feeling psy chology is for people like them given women feel more comfortable in environments where women are represented eg dasgupta et al 2015 crucially we saw gender differences in intellectual combativeness with men more than women perceiving themselves as highly motivated and willing to engage in intellectual debate even at the cost of social coherence and comfort combativeness predicted strong feelings of be longing and in turn majoring in philosophy an important theoretical insight arises when considering that intellectual combativeness is associ ated with high levels of systematizing which is related to more belong ing in philosophy and inversely related to empathizing which predicts belonging in psychology if combativeness both a drives the gender divide in psychology vs philosophy and b covaries with high system atizing  low empathizing which drive the gender divide in philosophy vs psychology then without operationalizing combativeness we might find that systematizing and empathizing are partially responsible for the gender gaps which prior work might suggest by virtue of limited mea surement put another way intellectual combativeness appears to be an integral piece to the puzzle in understanding why men are attracted to and women are deterred from philosophy compared to psychology 8 hm maranges m iannuccilli k nieswandt et al current research in behavioral sciences 4 2023 100097 our model is also consistent with the possibility that people who perceive themselves as more combative and therefore systematizing feel less belonging in psychology because they also prioritize having a fam ily and money which fosters feelings of fit with psychology notably prioritization of money and status contribute to feelings of belonging in psychology but are unrelated to feelings of belonging in philosophy both of which contribute to major choice this pattern suggests that peo ple who desire a career that provides them with wealth and status feel more at home in psychology but such concerns may not be weighed in considering philosophy as an academic home and major it may be that philosophy does not attract people who desire wealth and social status and that may be inherent to the sort of work philosophers do which we have argued entails abstract work and building knowledge for its own sake maranges et al 2021 however we also found that intel lectual combativeness fed into prioritization of status which suggests that those who feel they belong in philosophy also desire to be known for their work perhaps this is a different sort of status than what psy chology is perceived to provide all in all our findings suggest that the picture is complex but they also add clarity implications for interventions our findings can inform interventions aimed at closing the gen der gaps by increasing womens feelings of belonging in philosophy and mens feelings of belonging in psychology intellectual combative ness is one of the most powerful predictors of feelings of belonging in our model both directly and through associations with systematizing and empathizing accordingly it will be essential to target a expecta tions that men should be but women should not be intellectually com bative and b the combative style of intellectual debate in academic philosophy successful interventions on womens beliefs that they can not grow their intelligence and are not brilliant eg blackwell et al 2007which may be perceived a requisite to engagement in intellec tual combativeness as its link with systematizing could suggestcan be adapted to also address stereotypes about combativeness for example women may benefit from learning that just as many women lawyers en gage in intense styles of debate as men eg burton et al 1991 or about successful debaters who are women as role models in particular con texts help women feel they belong in those contexts eg young et al 2013 addressing this at its core though entails that the stereotype that combativeness indicates intellectual prowess should itself be bro ken especially for men additionally philosophy departments organi zations and conferences should encourage a respectful discussion and discourage confrontational debate styles this can begin in undergradu ate classrooms with instructors explicitly discussing this issue but also this culture change should be at the level of faculty and graduate stu dents who interact with more junior philosophy students recall that people who highly prioritized money who tended to be women and status felt more belonging in psychology whereas those priorities were unrelated to philosophy hence attracting more diverse students into philosophy may be accomplished by highlighting how philosophers often also engage in and inform discussions such as about political or social issues in the public spotlight moreover our results suggest that the prioritization of money may in part emerge from peo ples desire to have a family which can be expensive accordingly mak ing philosophy a more familyfriendly environment such as by allowing flexible hours work from home and funding of childcare may trickle down to undergraduates perceptions finally the gender imbalances themselves likely selfperpetuate stu dents feel less belonging in an academic group where they are in the gender minority eg inzlicht and benzeev 2003 sekaquaptewa and thompson 2002 although broader interventions are necessary to in crease the number of women in philosophy for example pairing the women who are already studying philosophy to work together may buffer their feelings of belonging indeed research in stem finds that when they work in groups with other women vs men dasgupta et al 2015 or receive tutoring from or with other women vs men for re view see robinson et al 2005 women experience heightened feelings of belonging and are more likely to persevere men may also benefit from working with other men in psychology limitations and future directions notwithstanding its strength in terms of relevant sample and out come preregistration and simultaneous modeling of important factors this work is limited in a few ways worth mentioning first although 35 of our sample is nonwhite we did not have enough participants of color to examine other problematic gaps lack of diversity or is sues of intersectionality for example black students are underrepre sented in philosophy ie black phd students and graduates combined make up 132 of philosophers but 134 of the population in the united states botts et al 2014 us census 2021 with a pipeline that also leaks during undergraduate training bright et al forthcoming schwitzgebel et al 2021a it is likely that gender identity interacts with ethnicity  race in creating unique aggregating challenges to ones feelings of belonging in psychology and philosophy ie intersectional ity crenshaw 20181989 we also did not collect detailed data that allowed us to test our hypotheses for students from other underrepre sented groups including lgbq working class and low socioeconomic status and people with disabilities moreover in building on past work and being able to make sufficiently powered generalizable conclusions about people who report their gender as man or woman only we were unable to systematically examine the perceptions and priorities of non binary gendered students in psychology and philosophy future work should aim for more inclusivity in research on gender gaps second and relatedly our sample was limited in diversity insofar as student participants attend universities in weird places ie united states and canada where the hegemonic culture is weird ie west ern educated industrial rich democratic henrich et al 2010 fu ture research should aim to understand which and how perceptions of personality and priorities combine and contribute to feelings of belong ing and majoring in psychology and philosophy for students from mi noritized groups and nonweird samples finally the correlational na ture of our data and analyses means that we cannot make claims about causality rather our findings are consistent with a mechanistic view of particular factors linking gender and major conclusion more women than men study psychology and more men than women study philosophy this is striking given the two fields share his tory and overlapping subject matter feelings of fitting in are essential to understanding undergraduate major choice and therefore these gender gaps in academia we empirically delineate the nuanced perceived per sonality and priority profiles of undergraduate students who feel they belong in and chose to major in psychology vs philosophy women were less intellectually combative than men and prioritized money more peo ple who perceive themselves to be very empathizing but not systematiz ing and who prioritize money and status from their career feel they belong in psychology which predicts majoring in that field people who perceive themselves to be highly systematizing and intellectually com bative feel they belong in philosophy these findings not only advance theory but also inform development of efficacious interventions by un derscoring the essential role perceptions especially individual differ ences in combative approaches to dialog and priorities especially with respect to garnering wealth from a career play in explaining this aca demic gender gap', 'abstract verbal praise is often used to improve prospective memory performance in daily life according to the motivation cognitive model the promotional efect of verbal praise on prospective memory may depend largely on redeploying attentional resources so its promotional efect is likely to be infuenced by attention two groups of college students n128 n117 participated in two experimental studies that examined this hypothesis experiment 1 manipulated attention load by changing the difculty of the ongoing tasks to focus on the efect of verbal praise on prospective memory under diferent attention load conditions the results showed that verbal praise promoted prospective memory performance under both attentional load conditions low high but verbal praise mainly promoted the prospective component when the attentional load was low meanwhile verbal praise mainly promoted the retrospective component when the attention load was high experiment 2 altered the dependence of prospective memory tasks on attentional resources by manipulating the cue focality further exploring the promotional efect of verbal praise on prospective memory with diferent types of cues under the low attention load condition the results showed that verbal praise only promoted prospective memory when nonfocal cues were used the results of this study partially verifed the motivation cognitive model introduction in daily life many things require planning in advance and remember to perform in the right situation this is a typical prospective memory task einstein  mcdaniel 1990 and an example is remembering to print documents when you pass the printing shop from the perspective of composition structure prospective memory contains two components prospective component and retrospective component among them the prospective component refers to remembering something to do in the appropriate situation mainly pointing to the recognition of cues and is most closely related to cue monitoring by comparison the retrospective component refers to the specifc content of prospective memory intention which mainly points to intention extraction and is related to retrospective memory smith  bayen 2004 the preparatory attention processing and memory processing pam theory suggests that the processing of prospective memory includes preparatory attention processing and memory processing among them the prospective component in prospective memory is controlled by preparatory attention processing while the retrospective component is controlled by memory processing smith  bayen 2006 preparatory attention processing continuously occupies individualsattention resources after the intention is formed and even if the target does not appear they also continue to invest attention resources for cue monitoring due to the existence of preparatory attention pam theory holds that the processing of prospective memory always involves attention consumption smith  bayen 2004 2006 the multiprocess theory suggests that in most cases the processing of prospective memory does indeed require a signifcant amount of selfinitiated attentional resources but in a few cases such as focal cues the cues of prospective memory may be spontaneously extracted and therefore the processing of prospective memory may not involve preparatory attentional processing einstein et al 2005 the implementation of a prospective memory task requires the successful recognition of cues and the successful extraction of intention content so insufcient processing in any component may lead to the failure of prospective memory  yongxin li liyongxinhenueducn yunfei guo gyfhenu126com jiaqun gan 932048493qqcom 1 faculty of education henan university jinming road longting district kaifeng china psychological research to ensure the successful execution of prospective memory tasks reward is a method that people often use and several studies have shown that material rewards can signifcantly improve the prospective memory performance of children sheppard et al 2015 and adults jeong  cranney 2009 however in everyday life nonmaterial rewards are also often used to improve prospective memory performance for example verbal praise verbal praise is a common type of nonmaterial reward that efectively improves cognitive performance droe et al 2013 ross et al 2020 to date most studies have focused on the promotional efect of verbal praise from the perspective of motivation deci et al 1999 believes that the main reason for the efectiveness of verbal praise lies in an individuals selfperception whether the content of verbal praise afrms the process eg you were very focused in doing things or the result eg you did a good job on the prospective memory task the information generated afrms the individuals abilities which increases their feelings of competence and selfesteem to demonstrate their ability individuals will make greater eforts to achieve target tasks albrecht et al 2014 further found that the verbal praise group had higher activation in the anterior striatum and midbrain indicating that verbal reward improved individuals subjective evaluation of the task itself in addition they found no signifcant decline in cognitive performance after the withdrawal of verbal praise suggesting that the efects of verbal praise are longlasting the efect of verbal praise on prospective memory may be closely related to metacognition motivation can alter an individuals evaluation of the importance of prospective memory tasks leading them to develop more detailed plans and adopt more efective strategies such as setting reminders in advance during the encoding stage and to engage in more active retelling and strategic monitoring during the retention and retrieval stages all of which may improve prospective memory performance kuhlmann 2019 rummel  meiser 2013 in addition verbal praise as a nonmaterial motivation with positive feedback can also lead individuals to have better expectations of their own prospective memory performance which can also encourage them to strengthen attentional monitoring of prospective memory tasks kuhlmann 2019 rummel  meiser 2013 schnitzspahn et al 2011 the motivational cognitive model explains in detail how motivation improves prospective memory performance the motivational cognitive model suggests that prospective memory goals associated with individuals are perceived as more important and this importance may afect prospective memory tasks in two ways first when individuals perceive the importance of prospective memory tasks they will assign more attention resources to prospective memory tasks which is conducive to adequate processing of prospective memory tasks thereby improving prospective memory performance which is consistent with the views of pam theory second motivation may prompt individuals to adopt some strategies to further ensure the successful execution of prospective memory tasks penningroth  scott 2013a 2015 but the use of strategies typically requires individuals to plan ahead during the coding phase therefore the adoption of efective strategies is also the result of individuals making eforts in advance the performance of prospective memory tasks improved signifcantly when participants received higher importance ratings kliegel et al 2004 and when more attention resources were assigned moyes et al 2019 in short the core view of the motivational cognitive model is that individuals will make more eforts to process prospective memory tasks more fully under motivational conditions so as to improve prospective memory performance since the strategies employed by individuals cannot be measured by objective methods this study focused on the changes in individuals attention to determine whether they put in more efort under the condition of verbal praise the promotional efect of verbal praise on prospective memory is likely to be afected by attention load verbal praise can result in a person evaluating a task as being more important albrecht et al 2014 while importance itself can prompt individuals to allocate more attention resources to ensure the completion of prospective memory tasks penningroth  scott 2013a prospective memory studies in the laboratory usually adopt a dualtask paradigm in which prospective memory tasks are inserted into ongoing tasks and performed simultaneously bisiacchi et al 2009 in the case of moderate task difculty individuals can indeed allocate more attention resources to the prospective memory task when motivated to do so while individuals attention to an ongoing task is poor when the task is very difcult at the same time individuals who received motivation also faced difculties in attention deployment under the high attention load condition which hindered their ability to transfer attention resources from ongoing tasks to prospective memory tasks guo et al 2023 therefore we speculated that verbal praise was likely to promote prospective memory only under the condition of low attentional load even when attention resources are adequate the efect of verbal praise on prospective memory may vary based on how much prospective memory depends on selfinitiated attention resources varying cue focality can alter the attention demand in the prospective memory task itself there is an overlap between the recognition of focal cues in prospective memory and the processing of ongoing tasks for example both the prospective memory cue and the ongoing task are color judgments and the individual can identify the prospective memory cue while processing the ongoing tasks therefore the processing of prospective memory does not consume additional attention resources scullin et al 2007 2010 by comparison the recognition of nonfocal cues psychological research does not overlap with the processing of ongoing tasks for example the ongoing task is color judgment while the prospective memory tasks are word recognition and individuals need to pay extra attention resources to the processing of prospective memory cues after processing the ongoing tasks the prospective memory of focal cues is not highly dependent on cue monitoring and it is difcult to improve the performance of prospective memory even if given additional attention resources by comparison nonfocal prospective memory tasks are highly dependent on cue monitoring and improvement in performance is generally closely related to preparatory attention kliegel et al 2008 therefore in the verbal praise condition more attention costs do not promote prospective memory performance for focal cues while performance should be better in the presence of nonfocal cues whether verbal praise mainly promotes prospective memory through the prospective component or retrospective component deserves further attention the prospective component mainly involves cue recognition while the retrospective component mainly involves intention extraction lajeunesse et al 2019 identifying which component of prospective memory is enhanced by verbal praise will help clarify the mechanism that underpins this efect the motivational cognitive model also states that the motivational induction may be promoted in two ways the frst is to encourage individuals to pay more attention to prospective memory tasks the second is to promote the spontaneous extraction of a prospective memory intention through various methods eg strategies penningroth  scott 2013b penningroth et al 2011 of them the attention allocated by the former is mainly used for cue retrieval which is mainly related to the prospective component guo et al 2023 harrison  einstein 2010 hering et al 2013 while the extraction intention by the latter refects the retrospective component smith  bayen 2004 therefore if the prospective component and the retrospective component can be separated the applicability of the motivational cognitive model in this study can be verifed previous studies have used mathematical calculation cohen et al 2001 and task separation meier  zimmermann 2015 to separate prospective and retrospective components without considering individual guessing multinomial processing tree mpt model is more advantageous because it compensates for this shortcoming mpt model is a simple and efective modeling method formed by the integration of cognitive psychology and statistics and other related disciplines smith  bayen 2004 this model is based on the pam theory the mpt model holds that the prospective component corresponds to the preparatory attention processing while the retrospective component corresponds to the memory processing the model requires that the ongoing task must be a task with two alternative responses for example if the ongoing task is a color judgment task the computer screen frst shows four colored boxes followed by a colored word individuals are asked to judge whether the color of the word appeared in the previous four boxes if the answer is yes the individual should press the j key on the keyboard otherwise heshe should press the f key the prospective memory task involves the individual pressing the space key when encountering specifc words there are four types of words in total 1 a pm task with the same color 2 a pm task with diferent color 3 an ongoing task with the same color and 4 an ongoing task with a diferent color each word appears with three responses the j key the f key and the space bar as shown in fig 1 meanwhile seven parameters need to be introduced to refect the relationship between the four types of words and the three responses c1 c2 p m1 m2 g and c c1 is the probability of identifying the same color c2 is the probability of identifying a diferent color p is preparatory attentional processing m1 is the probability of identifying a prospective memory target m2 is the probability of identifying the nonprospective memory target g is the probability of guessing a prospective memory target and c is the probability of guessing the same color schnitzspahn et al 2012 smith  bayen 2004 in short c1 and c2 refect the probability that individuals actually respond correctly to ongoing tasks p refects the probability that individuals successfully monitor the prospective memory cue and m1 and m2 refect the probability that individuals correctly recall the prospective memory intention content however it is possible for individuals to make correct prospective memory responses and ongoing tasks responses through guessing in some trials to better explain the role of guessing it is necessary to introduce the two parameters c and g the mpt model contains seven free parameters and the maximum likelihood estimation method is used to estimate the parameters since there are seven parameters to be estimated in the model in order to make the parameters measurable erdfelder et al 2009 pointed out that it is necessary to make assumptions about the relationship between each parameter to reduce the number of parameters there are two ways to do this the frst is to set two or more parameters equal bayen et al 1996 and the second is to assign specifc values to parameters erdfelder  buchner 1998 smith and bayen 2004 determined the values of c and g by changing c and g to observe whether p and m change the results showed that when the value of g was between 007 and 017 and the value of c was between 007 and 085 the values of p and m remained stable to make g and c close to the average values and considering the convenience of calculation they made the following assignments c05 g01 according to the standard assumptions of the multinomial models of source monitoring bayes et al 1996 the probability of individuals identifying targets and identifying nontargets should be equal and smith and bayen set m1 m2 these psychological research fig 1 multinomial process  ing tree model of eventbased prospective memory smith  bayen 2004 pm represents prospective memory psychological research settings results in a new fourparameter model c1 c2 p m the estimated values of these four parameters can be obtained by inputting the response category frequencies of all participants for each trial type in the multitree software moshagen 2010 the specifc response frequency required by the mpt model in the software can be referred to the table 2 of this study in this case p represents the prospective component and m represents the retrospective component finally by comparing the parameters we can determine whether there are diferences in the prospective and retrospective components between the verbal praise group and the control group to determine which component is responsible for generating the promotional efect of verbal praise on prospective memory currently many studies have validated the efectiveness of mpt in separating prospective and retrospective components in prospective memory smith  bayen 2006 smith et al 2010 2014 however the mpt model is based on the pam theory and its premise is that individuals must adopt preparatory attention processing to monitor prospective memory cues but under the condition of focal cues the cue monitoring of prospective memory overlaps with ongoing task processing to a high extent which cannot guarantee that the monitoring of focal cues necessarily requires preparatory attention processing einstein et al 2005 therefore the mpt model is currently unable to separate the prospective and retrospective components of prospective memory with focal cues the present study is the frst to focus primarily on the efect of verbal praise on prospective memory from the attentional perspective experiment 1 explored the efect of verbal praise on prospective memory under diferent attentional loads by manipulating the difculty of the ongoing task and varying the attention resources available to an individual we hypothesized that verbal praise promotes prospective memory only under the condition of low attentional load experiment 2 further manipulated the degree of dependence of the prospective memory task on cue monitoring by cue focality under the low attentional load condition and further focused on the infuence of verbal praise on prospective memory under diferent cue types we hypothesized that verbal praise promotes prospective memory only under the nonfocalcue condition this study has theoretical as well as practical signifcance from a theoretical perspective this study examined the changes in diferent prospective memory components in the verbal praise group the results of this study provide a preliminary explanation of the promotional efect of verbal praise on prospective memory and verify the applicability of the motivational cognitive model in explaining this efect in terms of applicability the results of this study confrm the boundary conditions under which verbal praise has a promotional efect and provides suggestions regarding how verbal praise can be used to improve prospective memory performance in daily life experiment 1 participants a total of 128 college students aged 1825 were recruited of whom 43 336 were male they were randomly assigned to four groups 32 in the lowload task and control group 32 in the lowload task and experimental group 32 in the highload task and control group and 32 in the highload task and experimental group all participants had normal vision no red and green color blindness were familiar with the basic operation of the computer and had not participated in a similar experiment recently all participants were tested independently and they received 20 cny about 3 usd as compensation after the experiment all participants signed a written informed consent form the studies were reviewed and approved by the institutional review board of henan provincial key laboratory of psychology and behavior no 20220715011 experimental design we adopted a 2 group verbal praise group control group2 attention load lowload highload betweensubjects design procedure the experimental procedure involved two stages the practice stage and the formal experimental stage at the beginning of the experiment the experimenter explained the experiment to participants and then questioned them about the task requirements to make sure that they had understood the task correctly the practice stage began with the appearance of a  250 ms on the center of the computer screen followed by four low attentional load condition or six high attentional load condition squares with diferent colors 800 ms and fnally a colored word 4000 ms appeared participants were asked to judge the color of the words if the word color was the same as the color of any of the previous squares they were to press the j key otherwise they were to press the f key the prospective memory task involved the participant pressing the space bar when the word name or atmosphere was encountered during the practice phase two prospective memory cues appeared once each after the practice stage the formal experiment began participants in the verbal praise group were praised by the experimenter after they completed the practice stage with the praise meeting three criteria first the praise was based on how well the participant performed in the practice phase second the praise focused on the prospective task telling the participant that heshe had done well on the prospective task and that he or she was at the top of the list of psychological research participants finally the participant was told to keep going for example you did a good job on the prospective task the correct rate is very high and you are at the top of the group the control group was told something irrelevant at the end of the practice stage after the participants understood the instructions they commenced the formal experiment all the participants needed to have a 5min delay before executing the formal experimental program and the flling tasks during the delay period were some simple arithmetic questions the formal experiment consisted of 120 ongoing tasks into which four prospective memory tasks were randomly inserted figs 2 3 results results of task responses prospective memory accuracy the results of an analysis of variance anova showed a signifcant main efect of attentional load f1124568 p005 p 2004 with accuracy being greater in the lowload than in the highload condition the main efect of group was also signifcant with the verbal praise group being more accurate than the control group f11241027 p001 p 2008 the interaction between group and attentional load however was not signifcant p005 prospective memory reaction time the results of an anova showed no signifcant main efect of attentional load or group and no signifcant interaction between the two ps005 ongoing task accuracy the results of an anova showed a signifcant main efect of attentional load f1 12410371 p0001 p 2045 with accuracy higher in the lowload than in the highload condition the main efect of group and the interaction between group and attentional load were not signifcant ps005 ongoing task response time the results of an anova showed that the main efect of attentional load was not signifcant p005 however the main efect of group was signifcant f1 124555 p005 p 2004 with the verbal praise group responding more slowly than the control group the interaction between attentional load and group was not signifcant p005 table 1 the results of multinomial process tree mpt modeling model ft test model ft is the degree of ft between the observed data and the value estimated by the model expressed by the statistic g2 similar to the chisquare distribution which is mainly used to explain whether the observed data is suitable for mpt modeling analysis in this study multitree v047 software was used to analyze all participants responses as shown in table 2 data from the control and experimental groups in both the low attention load condition g2 4296 p005 g2 4100 p005 and high attention load condition g2 4203 fig 2 schematic diagram of the ongoing task under the lowload condition fig 3 schematic diagram of ongoing task under high the attention load condition psychological research p005 g2 4227 p005 were tested for the ft degree of model and the results showed that the ft degree was good parameter estimation and analysis we focused on the differences in the prospective component p and retrospective component m between the control group and the verbal praise group under low and high attention load conditions respectively the results showed that the prospective component of the verbal praise group p069 was higher than the control groups p056 when the attentional load was low g2 1464 p005 by comparison the prospective component of the verbal praise group p054 did not differ from the control groups p044 when the attentional load was high g2 1181 p005 in addition there was no diference between the verbal praise group m096 and the control group m 097 in the retrospective component when the attentional low was low g2 1088 p005 finally the retrospective component of the verbal praise group m 097 was signifcantly higher than the control group m090 when the attentional load was high g2 1674 p001 discussion experiment 1 mainly manipulated individuals available attention resources through the difculty of the ongoing task to explore the infuence of verbal praise on prospective memory under diferent attention loads we found that the prospective memory performance of the verbal praise group was better than the control groups under both low and high attentional load conditions indicating that the promotional efect of verbal praise on prospective memory was not limited by attention load which was inconsistent with our prediction the results also showed that the verbal praise groups response speed on the ongoing task was slower than the control groups indicating that the verbal praise group assigned more attention resources to the prospective memory task this confrms the ideas of previous studies deci et al 1999 and the predictions of the motivation cognitive model penningroth  scott 2013a although verbal praise facilitated prospective memory in both high and low attentional load conditions verbal praise had varying efects on diferent prospective memory components under diferent attentional load conditions when attention load was low the verbal praise group was higher than the control group on the prospective component only indicating that verbal praise can improve prospective memory performance by promoting the prospective component under the sufcient attention condition however when attention load was high the verbal praise group was better than the control group on the retrospective component only which indicates that verbal praise can promote prospective memory performance mainly through improving the retrospective component under the insufcient attention condition experiment 1 mainly focused on nonfocal prospective memory which consumed additional attention resources for cue retrieval scullin et al 2010 verbal praise can be facilitated by allocating more attention resources to prospective memory tasks we also found that verbal praise acted as a facilitator by allocating more attention resources to cue retrieval the prospective component under the adequate attention condition however the processing of focal cues table 1 participants prospective memory performance and ongoing task performance lowcontrol control group under the low attention load condition lowpraise praise group under the low attention load condition highcontrol control group under the high attention load condition highpraise praise group under the high attention load condition acc accuracy rt response time prospective memory ongoing task acc rt ms acc rt ms lowcontrol 049 029 1102 357 078 008 1144 279 lowpraise 060 024 1157 272 078 007 1212 295 highcontrol 034 027 1157 321 065 005 1127 191 highpraise 053 026 1260 309 067 006 1282 292 table 2 response category frequencies of all participants for each trial type lowcontrol control group under the low attention load condition lowpraise praise group under the low attention load condition highcontrol control group under the high attention load condition highpraise praise group under the high attention load condition pm prospective memory ot ongoing task trial type response type f j space lowcontrol pm same 9 21 35 pm diferent 24 10 40 ot same 480 1477 3 ot diferent 1367 377 6 highcontrol pm same 6 15 48 pm diferent 17 7 45 ot same 434 1439 3 ot diferent 1393 462 4 lowpraise pm same 11 26 29 pm diferent 32 13 26 ot same 638 1191 8 ot diferent 1205 654 9 highpraise pm same 10 26 39 pm diferent 20 9 33 ot same 718 1133 3 ot diferent 1197 647 4 psychological research does not require additional attention consumption and is less dependent on attention monitoring harrison  einstein 2010 even with sufcient attention for example under the low attention load condition the verbal praise group can assign sufcient attention to the prospective memory task but the prospective memory task itself is not highly dependent on attention monitoring their prospective memory performance may be less likely to beneft from additional attention costs experiment 2 further examined whether the promotional efect of verbal praise on prospective memory was afected by cue focality under low attention loads experiment 2 participants one hundred and seventeen college students aged 1825 years old including 53 males 453 were randomly recruited the participants were randomly assigned into four groups 29 in the focal cue and control group 30 in the focal cue and experimental group 30 in the nonfocal cue and control group and 28 in the nonfocal cue and experimental group other aspects were consistent with experiment 1 experimental design a betweensubject experiment design of two groups verbal praise group control group2 cue focality focal cue nonfocal cue was adopted experimental procedure the procedure in the nonfocal cue condition was exactly the same as in experiment 1 under the focalcue condition the prospective memory task involved pressing the space bar when yellowcolored words were encountered and the rest is consistent with experiment 1 results the mpt model is based on the pam theory which posits that the successful execution of prospective memory tasks always requires attention resources smith 2003 some studies have found no evidence that the processing of prospective memory tasks is monitored with focal cues brewer et al 2010 harrison  einstein 2010 hence we did not analyze the diferent components of prospective memory in experiment 2 prospective memory accuracy the results of an anova showed that the main efect of cue focality was signifcant f1 113732 p001 p 2006 with accuracy being greater with the focal cue than the nonfocal cue the main efect of group was also signifcant f1 113490 p005 p 2004 with accuracy being higher in the verbal praise group than in the control group the interaction between cue focality and group was also signifcant f1 113544 p005 p 2005 with accuracy being higher in the verbal praise group than in the control group under the nonfocal cue condition p005 while there was no diference between the two groups in the focal cue condition p005 prospective memory response time the results of an anova showed a signifcant main efect of cue focality f1 113988 p001 p 2007 with faster response speed in the focal cue than in the nonfocal cue condition however the main effect of group was not significant p005 and the interaction between cue focality and group was also not signifcant p005 ongoing task accuracy the results of an anova showed that neither the main efects of cue focality or group nor the interaction between the two were signifcant ps005 ongoing task response time the results of an anova showed a significant main effect of cue focality f1 113621 p005 p 2005 with faster ongoing task response speed in the focal cue than in the nonfocal cue condition the main efect of group was also signifcant f1 113431 p005 p 2004 with the verbal praise group responding more slowly than the control group none of the interactions between group and cue focality were signifcant ps005 table 3 discussion experiment 1 revealed that verbal praise promoted prospective memory processing by allocating more attention under the highload condition experiment 2 further focused on whether more attention resources allocated to prospective memory tasks by verbal praise would promote prospective table 3 participants performance on prospective memory tasks and ongoing tasks in focal and nonfocal conditions focalcontrol control group under the focal cue condition focalpraise praise group under the focal cue condition nonfocalcontrol control group under the nonfocal cue condition nonfocalpraise praise group under the nonfocal cue condition acc accuracy rt response time prospective memory ongoing task acc rt ms acc rt ms focalcontrol 071 027 937 201 076 007 941 195 focalpraise 070 035 954 224 080 008 1037 246 nonfocalcontrol 041 036 1040 300 077 007 1089 288 nonfocalpraise 068 024 1152 395 078 006 1183 246 psychological research memory performance only when nonfocal cues were used the results showed that the response speed of the ongoing task was faster with a focal cue than with a nonfocal cue which indicated that the prospective memory task required less attention monitoring with a focal cue compared to a nonfocal cue the verbal praise groups response speed was slower than the control group indicating that the verbal praise group reserved more sustained attention to process prospective memory tasks however we found that the verbal praise groups prospective memory performance was better than the control group in the nonfocal cue condition which is consistent with the fndings of experiment 1 however there was no diference in prospective memory performance between the two groups in the focal cue condition suggesting that prospective memory under the focal cue condition did not beneft from verbal praise this is inconsistent with the view of the motivation cognitive model penningroth  scott 2013a in summary the results of experiment 2 showed that the prospective memory task involving a focal cue was less dependent on attention monitoring individuals in the verbal praise group did not signifcantly improve focalcue prospective memory performance even though they assigned more attention resources to the focal prospective memory task the pam theory suggests that prospective memory involves two processing processes preparatory attention processing and memory processing due to the existence of preparatory attention processing individuals will inevitably consume attention resources for cue monitoring smith  bayen 2004 2006 however the multiprocess theory suggests that the processing of prospective memory requires attentional costs in most cases but in a few cases such as under the focalcue condition prospective memory cues can be spontaneously extracted and the dependence of prospective memory with focal cues on attentional monitoring is not strong einstein et al 2005 therefore although verbal praise encourages individuals to allocate more attentional resources to prospective memory tasks the extraction of focal cues does not heavily rely on cue monitoring so verbal praise does not improve prospective memory performance for focal cues general discussion verbal praise has an efective efect on improving cognitive performance albrecht et al 2014 as a typical social cognitive ability prospective memory is very common in our life and greatly impacts our quality of life therefore it is important to know whether verbal praise can promote prospective memory however to date no studies have directly examined this issue and this study is the frst to explore the efect of verbal praise on prospective memory changes in the difculty of ongoing tasks can manipulate the attention resources that individuals allocate to prospective memory tasks guo et al 2022 while highly difcult ongoing tasks limit the motivation to fexibly allocate attention resources to prospective memory tasks guo et al 2023 therefore the promotional efect of verbal praise on prospective memory may be limited by attention load experiment 1 showed that verbal praise improved prospective memory performance under both low and high attention load conditions which verifed our hypothesis under the low attention load condition only how does verbal praise improve prospective memory performance mpt suggests that motivation may promote task importance in two ways the frst is to encourage individuals to pay more attention to prospective memory tasks and the second is to promote the spontaneous extraction of prospective memory intention contents through various pathways penningroth  scott 2013b penningroth et al 2011 results of the present study showed that verbal praise improved prospective memory performance under both low and high attentional load conditions and the results of the ongoing task showed that the verbal praise group assigned more attention to prospective memory tasks confrming the frst pathway of mpt however we also found that verbal praise promoted the prospective component only when attentional load was low indicating that individuals assigned more attention to cue retrieval of prospective memory under the sufcient attention condition which is consistent with the mpt view in the case of insufcient attention although individuals also allocated more attention it was not used to extract prospective memory cues which meant that the prospective component did not beneft from more attention costs under the high attention load condition at the same time we found that verbal praise only improved the retrospective component performance under the high attention load condition this suggests that verbal praise promotes the extraction of prospective memory intention in the case of insufcient attention and validates the second pathway of mpt there are two possible reasons why verbal praise did not work through the prospective component but through the retrospective component in the high attention load condition first the ongoing task was more difcult while the verbal praise group reserved more time for task processing heathcote et al 2015 the highly difcult ongoing task hindered the transfer of attention zhang 2017 therefore highdifculty ongoing tasks will hinder the transfer of attention from ongoing tasks to prospective memory tasks which is not conducive to the retrieval of prospective memory cues second high attentional load leads to a decline in retrospective memory which makes prospective memory intentions difcult to extract marsh  hicks 1998 however motivation encourages individuals to adopt more strategies to process the association between prospective memory cues and intention content in the coding stage so that the intention psychological research content is in a state of spontaneous extraction penningroth  scott 2013b the higher the degree of automation of cognitive processing the less afect attention load has on task performance zhang et al 2011 therefore the retrospective component of the verbal praise group remained at a high automatic level of processing even under the condition of high attentional load improving the performance of the retrospective component although verbal praise can result in more attention being allocated to facilitate the search for nonfocal cues when attention monitoring is adequate prospective memory tasks for focal cues do not beneft from more attention the results of this study showed that verbal praise continued to prompt individuals to reserve more attention resources for the processing of focalcue prospective memory but verbal praise did not improve the prospective memory performance of focal cues which is inconsistent with the motivational cognitive model this is mainly because the processing of focal cues is consistent with the processing of ongoing tasks and individuals have fully retrieved and recognized prospective memory cues while processing ongoing tasks cona et al 2014 at the same time the recognition of focal cues is mainly a bottomup process zuber et al 2019 while verbal praise mainly afects the topdown attention distribution process therefore the prospective memory of focal cues is less likely to beneft from verbal praise together these results indicate that verbal praise can improve prospective memory performance under both low attentional load and high attentional load conditions when attention load is low verbal praise mainly improves prospective memory performance by allocating more attention to cue monitoring when attention load is high however verbal praise improves prospective memory performance by promoting the extraction of the intended content in addition when attentional load was low the attention resources reserved for prospective memory in the verbal praise group was benefcial for the prospective memory performance of nonfocal cues but not focal cues the results of this study demonstrate that verbal praise does not promote prospective memory through both pathways of the motivational cognitive model but through selective choosing one or the other this study has some limitations first this study only revealed the processing mechanism by which verbal praise promotes prospective memory using simple behavioral responses which are relatively simple indicators eyetracking technology can refect the attentional changes more comprehensively and has more advantages in explaining the processing mechanism underpinning prospective memory for example pupil diameter can refect attention load moyes et al 2019 and fxation time can refect processing degree wedel  pieters 2017 multiple indicators of eyetracking technology can identify what when and how individuals attend to a stimulus second the delay time of the prospective memory task in this study was relatively short under a long delay condition individuals allocate less attention resources to prospective memory tasks conte  mcbride 2018 and forget more intended content barrouillet et al 2012 enhanced attention monitoring induced by verbal praise and the spontaneous extraction of the intended content should improve prospective memory performance to a greater extent third this study adopted the betweensubjects design which cannot exclude the infuence of individual diferences on the experimental efect greenwald 1976 therefore this study found that the promoting efect of verbal praise on prospective memory may also be mixed with the interference of individual diferences this study systematically investigated the efects of verbal praise on prospective memory and its diferent components under different attentional load and cue focality conditions experiment 1 manipulated attention load by changing the difculty of the ongoing tasks to focus on the efect of verbal praise on prospective memory under diferent attention load conditions the results showed that verbal praise promoted prospective memory performance under both attentional load conditions but verbal praise mainly promoted the prospective component when the attentional load was low meanwhile verbal praise mainly promoted the retrospective component when the attentional load was high experiment 2 altered the dependence of prospective memory tasks on attention monitoring by manipulating the cue focality further exploring the promotional efect of verbal praise on prospective memory with diferent types of cues under the low attention load condition the results showed that verbal praise only promoted prospective memory when nonfocal cues were used author contributions yg jg and yl contributed to the study conception and design material preparation and data collection were performed by yg and jg data analysis was performed by yg and fg the frst draft of the manuscript was written by yg jg and yl commented on previous versions of the manuscript all authors read and approved the fnal manuscript the authors wish it to be known that in their opinion the frst two authors should be regarded as joint frst authors funding this research did not receive any specifc grant from funding agencies in the public commercial or notforproft sectors data availability the data that support the fndings of this study are available from the corresponding author on request declarations competing interests the authors declare no competing interests conflict of interest the authors declare no confict of interest ethical approval all procedures performed in studies involving human participants were in accordance with the ethical standards of the insti psychological research tutional andor national research committee and with the 1964 helsinki declaration and its later amendments or comparable ethical standards informed consent informed consent was obtained from all individual participants included in the study', 'abstract background current diagnostic systems for mental disorders rely upon presenting signs and symptoms with the result that current definitions do not adequately reflect relevant neurobiological and behavioral systems  impeding not only research on etiology and pathophysiology but also the development of new treatments discussion the national institute of mental health began the research domain criteria rdoc project in 2009 to develop a research classification system for mental disorders based upon dimensions of neurobiology and observable behavior rdoc supports research to explicate fundamental biobehavioral dimensions that cut across current heterogeneous disorder categories we summarize the rationale status and longterm goals of rdoc outline challenges in developing a research classification system such as construct validity and a suitable process for updating the framework and discuss seven distinct differences in conception and emphasis from current psychiatric nosologies summary future diagnostic systems cannot reflect ongoing advances in genetics neuroscience and cognitive science until a literature organized around these disciplines is available to inform the revision efforts the goal of the rdoc project is to provide a framework for research to transform the approach to the nosology of mental disorders keywords diagnosis dsm icd psychiatric diagnosis psychopathology rdoc research domain criteria background as of this writing there are three versions of diagnostic systems for psychiatry in development by far the most notoriety has been attached to the revision of the diagnostic and statistical manual of mental disorders dsm published by the american psychiatric association which has been under revision sufficiently long enough to receive a name change from dsmv to dsm5 this attention is not surprising given the prominence of the dsm for clinical diagnosis both in the us and internationally its simultaneous role in research and the number of controversial issues that have been involved in the revision process  such as the debates over autism spectrum disorder 1 bereavement and depression 2 and personality disorders 34 to name just a few the dsm revisions have also prompted an extensive revisiting of important issues regarding the nature of mental disorders and how they should be considered scientifically an excellent summary and analysis of these topics is represented by the series of papers that appeared recently in philosophy ethics and humanities in medicine and bmc medicine 57 see also 8 the second major revision is that of the mental and behavioural disorders section of the international classification of diseases icd11 being developed by the world health organization this revision effort is being accomplished by an international group of experts including some intentional overlap with members from the dsm committees it is worth noting that the icd represents the official diagnostic standard in the us as in the rest of the world although both the dsm and icd emphasize clinical utility the scope of the clinical settings where the icd is employed tends to be yet more varied and extensive than that of the dsm the latter is intended largely for use by highly trained mental health professionals though it is employed by many professional groups by contrast the icd is necessarily designed for health  correspondence bcuthbermailnihgov 1 national institute of mental health national institutes of health 6001 executive boulevard msc 9632 room 7121 bethesda md 208929632 usa 3 expressparcel delivery rockville md 20852 usa full list of author information is available at the end of the article  2013 cuthbert and insel licensee biomed central ltd this is an open access article distributed under the terms of the creative commons attribution license httpcreativecommonsorglicensesby20 which permits unrestricted use distribution and reproduction in any medium provided the original work is properly cited cuthbert and insel bmc medicine 2013 11126 httpwwwbiomedcentralcom1741701511126 settings around the world to be used not only by practitioners with widely divergent levels of expertise but also in cultural settings where assumptions about the etiology and nature of disorders may be highly dissimilar from the western milieu of the dsm accordingly the icd places stronger emphasis on public health applications than the dsm and one reflection of this emphasis is the use of definitions that emphasize short text descriptions of each disorder rather than the polythetic symptom lists of the dsm finally the national institute of mental health nimh instituted the research domain criteria rdoc project in early 2009 given its status as a research classification system rather than one intended for routine clinical use this initiative diverges markedly from the others in multiple respects the seven major differences between rdoc and the established systems are delineated in the sections that follow as its share of this forum one caveat is in order at the outset to provide an appropriate context for the remarks that follow the dictionary reminds us that the first sense of the noun debate is a discussion  involving opposing viewpoints as appropriate for its latin root that means to beat 9 however discussions among the framers of the dsm5 the icd11 revisions and the nimh rdoc have from their inception been cordial and marked by general agreement about the relative emphasis of each respective system and also about their shared interests thus  unfortunately from the perspective of sparking a sharp exchange among divergent views  the debate in this case must proceed more along the lines of the terms more elaborated definition a deliberation or consideration in this more congenial sense there is indeed much to consider discussion a diagnostic system can have many purposes for instance a major reason for the creation of the icd was to establish a comprehensive manual for determining causes of mortality thus enhancing efforts at improving public health however perhaps the preeminent role of diagnosis in medicine is to determine the exact nature of a patients disease in order to administer the optimal treatment yet very little discussion of this aspect can be found either in published papers or in the extensive blogosphere that has sprung up around the dsm5 the revisions have renewed debates about the definition and nature of mental disorders the various positions in the philosophy of science that might represent how to think about mental illness realist essentialist and so on categorical versus dimensional approaches to disorders and the role of reductionism and phenomenology 58 any discussion about the ramifications of these various considerations in actually making a difference on how we treat our patients however has been conspicuously lacking this lack is likely due in no small part to the current nature of treatments for mental disorders on the one hand effective treatments exist treatments for major classes of disorders such as depression anxiety disorders schizophrenia and bipolar disorders are available and effective for large numbers of patients further a number of effective treatment modalities  pharmaceutical interventions psychosocial or behavioral treatments medical devices  have been established on the other hand treatments are not particularly precise and tend to affect broad classes of disorders antidepressant medications such as selective serotonin reuptake inhibitors are used to treat not only depression but a wide variety of anxiety mood and other disorders antipsychotic agents are used not only with schizophrenia but in bipolar disorder and sometimes for personality and other severe disorders anxiolytics such as valium are prescribed widely across the anxiety and mood spectrum a similar situation prevails for behavioral treatments for instance the use of cognitivebehavioral therapy albeit with many variants has expanded beyond the internalizing disorders spectrum for which it was originally developed to the treatment of virtually all mental disorders for example see 10 although decent treatments for mental disorders are thus plentiful it is instructive to contrast the changes in disease burden for other diseases over the past several decades with that for mental disorders for instance the impact of research  both clinically and in public health arenas  has been dramatic for heart disease death due to heart disease climbed steadily from 1950 through 1968 at a rate that projected almost 18 million deaths in 2007 instead because of the rapid progress of research the actual mortality due to heart disease was only about one quarter of that number approximately 11 million deaths in 2007 alone were averted according to the predicted peak rate 11 similarly survival rates for children with acute lymphoblastic leukemia have improved over the last several decades from less than 10 to over 90 12 by contrast mortality has not decreased for any mental illness prevalence rates are similarly unchanged 13 there are no clinical tests for diagnosis detection of disorders is delayed well beyond generally accepted onset of pathology and there are no welldeveloped preventive interventions there are many reasons for this lack of progress in mental disorders the brain is the most complex organ in the body and it is wellaccepted that mental illnesses involve highly complex interactions of genetic factors and experience the brain cannot be studied directly with the facility we have for more accessible organs limiting progress based on pathology however the diagnostic system for psychiatry has also been increasingly noted as an impediment to progress the problems have been extensively documented for example 1418 and do not need to be cuthbert and insel bmc medicine 2013 11126 page 2 of 8 httpwwwbiomedcentralcom1741701511126 elaborated here but include excessive comorbidity of disorders marked heterogeneity of mechanisms and reification of disorders in particular the underlying validity of the disease entities has been questioned in that the dsm and icd categories do not map well onto emerging findings from genetics systems neuroscience and behavioral science for example 1920 as a result it becomes very difficult to translate research from basic studies either in animal models or in humans to a systematic understanding of pathology or to systematic treatments directed at mechanisms nevertheless the dsm and icd system the two nosologies are largely overlapping in terms of the actual listing of disorders has become the standard to obtain research grants regarding etiology and pathophysiology to conduct drug trials at all phases and to obtain regulatory approvals for pharmaceutical treatments in behavioral research as well the need to establish evidencebased treatments has led researchers to copy the lead of drug trials and conduct trials in terms of dsm and icd diagnoses thus issues with the current nosology markedly affect the treatment development arena this point is well illustrated in a quotation from a recent paper by several pharmaceutical industry scientists regarding problems in drug development using the current system on average a marketed psychiatric drug is efficacious in approximately half of the patients who take it one reason for this low response rate is the artificial grouping of heterogeneous syndromes with different pathophysiological mechanisms into one disorder  by increasing the mechanistic understanding of disease and matching the right treatments to the right patients one could move from onesizefitsall to targeted therapy and increase the benefitrisk ratio for patients these scientists conclude that a pedestrian trialanderror search of multitarget agents must be hazarded  until clinical trial design and patient segmentation can improve to the point of matching disease phenotype to circuitbased deficits 21 p 1276 this problem is no doubt a not insignificant reason why so many pharmaceutical companies have withdrawn from active development research in mental disorders 2223 and the reliance on biologically heterogeneous categories as the gold standard for diagnosis has clearly precluded the identification or validation of biomarkers although one could imagine revising the diagnostic categories to align with biological discoveries our field has essentially excluded biological findings that do not map on to the current heterogeneous categories of symptom clusters in other areas of medicine trends have increasingly moved in the direction of ever more precise specification of the genetic molecular and cellular aspects of disease in specialty after specialty there has been a realization that disease entities that appear to be a single disorder actually have distinct genetic precursors and pathophysiology for instance for many forms of cancer diagnosis is no longer defined by the involved organ or even the pathologists report but rather by analysis of genetic variants that can predict exactly what treatment will be optimal for example 24 in another domain perhaps the most striking example of this trend involves a new drug ivacaftor kalydeco approved by the food and drug administration after an expedited review the drug is effective in treating patients with cystic fibrosis who have a form of the syndrome with a specific mutation of the cystic fibrosis transmembrane regulator gene only 4 of patients with cystic fibrosis have this genetic mutation but for these patients the compound is highly effective in correcting the action of the malfunctioning protein 25 these new approaches toward individualized treatment are now generally called precision medicine and represent the forefront of medical science in november 2011 the us national academy of sciences published a major report on precision medicine outlining the significance of this development and calling for new knowledge networks that can harness the power of promising technologies to identify and correct specific pathophysiologies that result from genetic and environmental causes 26 as yet the field of mental disorders research lags badly behind the rest of medicine in moving toward precision medicine yet knowledge of the central nervous system has exploded over the last two decades and new technologies are rapidly eclipsing such wellknown methods as positron emission tomography scans and magnetic resonance imaging how can these rapid developments in basic science be harnessed in the service of precision medicine for mental disorders research domain criteria as a national health ministry nimh is committed to reducing the burden of suffering due to mental illness through research decades of research have increasingly revealed that neural circuits and systems are a critical factor in how the brain is organized and functions and how genetics and epigenetics exert their influence however this knowledge cannot be implemented in clinical studies as readily as might be hoped any one mechanism such as fear circuits or working memory is implicated in multiple disorders as currently defined it is difficult to know which diagnostic category to select first to explore any promising leads and a positive result immediately raises the question of whether the demonstration of efficacy must be extended to all similar disorders a timeconsuming and expensive proposition contrariwise a syndrome such as major depression clearly involves multiple mechanisms  dysfunction in the hypothalamic pituitary axis in brain rewardseeking activities in emotion regulation circuits in modulatory neurotransmitter systems in cognitive systems and in epigenetic marks thus it is not surprising that studies to establish cuthbert and insel bmc medicine 2013 11126 page 3 of 8 httpwwwbiomedcentralcom1741701511126 the cause of major depression are equivocal and difficult to replicate nor that new treatments directed toward a particular mechanism are often only marginally effective and cannot be replicated in response to this situation nimh established in its strategic plan of 2008 the following goal to develop for research purposes new ways of classifying mental disorders based on dimensions of observable behavior and neurobiological measures the instantiation of this goal is the rdoc project and is nimhs effort to develop a precision medicine approach for mental disorders 27 rdoc represents a real paradigm shift by considering mental disorders from a translational point of view rdoc does not take as a starting point the traditional view of disorders as symptom complexes based largely on clinical descriptions rather the approach proceeds in two steps the first step is to inventory the fundamental primary behavioral functions that the brain has evolved to carry out and to specify the neural systems that are primarily responsible for implementing these functions for instance much is now known about circuits for fear and defense 28 for various aspects of appetitive behavior such as learning to predict reward and moving toward reward 29 and cognitive functions such as working memory 30 the second step then involves a consideration of psychopathology in terms of dysfunction of various kinds and degrees in particular systems as studied from an integrative multisystems point of view the four aims of the rdoc project are listed in table 1 beneath the statement of goal 14 the project began with deliberations among members of an internal nimh working group which served to define the overall shape of the effort as well as the specific process to be followed the workgroup determined that the optimal approach was to establish a hierarchical scheme with the specific dimensions nested within five major domains of functioning see table 2 for a listing of the rdoc matrix as of june 2012 at the end of the initial conference series the project moved forward rapidly once this organizational matrix was established as called for aim 1 of table 1 the rdoc process involved a series of workshops with experts in the field to determine the fundamental behavioral components to be included in the system the five major domains conceived on empirical grounds from such diverse research areas as temperament behavior genetics and structural models of mental disorders also served as a convenient way to organize the workshops in that one workshop was conducted for each of the five domains approximately 30 to 40 experts convened for each workshop their charge was to determine which dimensions should be included within the domain provide a definition for each dimension and provide a list of the elements for each dimension that could be used to measure it at each of several units of analysis as specified in aim 4 of table 1 an important consideration is that the dimensions as behavioral entities tied to neural systems are always dependent upon the march of research to continually refine and evolve a scientific understanding of their function and of their implementing circuits in this sense the dimensions represent constructs as classically defined in psychological research 31 and this term was adopted for rdoc to emphasize that they will and should always be subject to further validation and revision the rdoc matrix thus consists of a series of rows with the constructs nested within their superordinate domains and the columns representing the units of analysis the reader is encouraged to consult the rdoc website httpwwwnimhnihgovresearchfundingrdocindex shtml which contains the completed matrices from all of the rdoc workshops the seven pillars the distinctions between rdoc and the dsm and icd systems can be captured by seven major points that include both conceptual and practical differences first the approach incorporates a strong translational research perspective rather than starting with symptombased definitions of disorders and working toward their pathophysiology rdoc inverts this process basic science  in genetics other areas of neuroscience and behavioral science serves as the starting point and disorders are considered in terms of disruptions of the normalrange operation of these systems with an emphasis on the mechanisms that serve to result in dysfunctions of varying degrees second rdoc incorporates an explicitly dimensional approach to psychopathology as called for in many recent analyses of psychopathology 3233 however in contrast table 1 national institute of mental health strategic goal 14 develop for research purposes new ways of classifying mental disorders based on dimensions of observable behavior and neurobiological measures aim  task 1 initiate a process for bringing together experts in clinical and basic sciences to jointly identify the fundamental behavioral components that may span multiple disorders eg executive functioning affect regulation person perception and that are more amenable to neuroscience approaches 2 determine the full range of variation from normal to abnormal among the fundamental components to improve understanding of what is typical versus pathological 3 develop reliable and valid measures of these fundamental components of mental disorders for use in basic studies and in more clinical settings 4 integrate the fundamental genetic neurobiological behavioral environmental and experiential components that comprise these mental disorders cuthbert and insel bmc medicine 2013 11126 page 4 of 8 httpwwwbiomedcentralcom1741701511126 to views that emphasize dimensionality mostly as a function of symptom severity rdoc is committed to studying the full range of variation from normal to abnormal in some cases only one end of a dimension may involve problem behavior for instance one is seldom likely to complain of an outstanding memory or keen vision but often both extremes of a dimension may be considered as abnormal  for example a complete lack of fear may be associated with aggressive or psychopathic behavior and the opposite end of diminished rewardseeking may be mania an important consideration regarding dimensionality is that the relationship between increasing disruptions in functional mechanisms and the severity of symptoms may be markedly nonlinear with tipping points that mark a transition to more severe pathology a critical area of research is to determine the exact location of such points and how they are affected in each individual by various risk or resilience factors the third distinction follows directly from the second aim 3 in table 1 includes a call to develop reliable and valid measures of these fundamental components one of the drawbacks of a pathogen model of illness is that most scales developed over the past decades have either been designed to study normal traits such as personality or else clinical symptoms of disorder and thus lack sensitivity at one end or the other of a putative dimension in particular zones of very mild or transient psychopathology with their potential for understanding proximate etiology and for indicated prevention receive short shrift thus scale development represents a high priority for rdoc research applications in fact wellvalidated and psychometrically optimized measures based upon cognitive neuroscience research are beginning to appear 34 consistent with contemporary measurement science new scales would and should almost invariably incorporate interval or ratio scaling to improve quantification of the phenomena of interest as such assessments muster it becomes feasible to determine cutpoints along the distribution for varying types of interventions essentially similar to practices in other areas of medicine where continuous measures are available such as hypertension or hypercholesterolemia a further advantage of this approach is that ongoing research studies about relative risk at various points along the dimension can inform decisions about changing the cutpoints at which interventions are indicated  as has happened repeatedly such as in hypertension research 35 the fourth distinction concerns the types of designs and sampling strategies that rdoc studies must necessarily follow in the traditional clinical study the independent variable is almost always one or more usually one dsm or icd groups often versus controls it is relatively straightforward to diagnose the patients according to the symptombased criteria excluding those who fail to meet criteria for the diagnosis under study the resultant groups form the independent grouping variable an important public health issue concerns the unknown number of such patients whose conditions are essentially invisible to research by virtue of failing to meet criteria although it is well known that for some disorders such as eating disorders not otherwise specified is the modal diagnosis rdoc by contrast involves a twostep procedure the investigator must first establish the sampling frame that is what group of individuals will be entered into the study because this will not be identical to a dsm or icd diagnosis other criteria will have to be applied in some cases this might simply comprise all patients presenting at a certain type of clinic such as for anxiety disorders or serious mental illness however such a sampling frame might fail to meet the goal of studying the full range and so a control group might also be needed  with a wider range of inclusion however rather than the typical supernormal control group with no psychiatric history then the second step is to specify the independent variable in the study to permit investigators freedom in pursuing their hypotheses the independent variable may be chosen from any unit of analysis thus performance on a working memory task might be the independent variable table 2 research domain criteria october 2012 constructs are listed within each domain negative valence domain positive valence systems cognitive systems systems for social processes arousalmodulatory systems acute threat fear approach motivation attention affiliation and attachment arousal potential threat anxiety initial responsiveness to reward perception social communication biological rhythms sustained threat sustained responsiveness to reward working memory perception and understanding of self sleepwake loss reward learning declarative memory perception and understanding of others frustrative nonreward habit language behavior cognitive effortful control cuthbert and insel bmc medicine 2013 11126 page 5 of 8 httpwwwbiomedcentralcom1741701511126 for a study of working memory in serious mental illness dependent variables might comprise neuroimaging of specified brain areas relevant assessments of realworld dysfunction and an exploration of relevant candidate genes for a study of anxiety disorders fearpotentiated startle might be the independent variable stratified by a relevant genetic polymorphism and the dependent variables could be overall symptom severity and distress plus performance on a behavioral fearavoidance test thus while more interesting research designs can be created the investigator will need to be more thoughtful about crafting the design of the study to answer the particular experimental question fifth and critically important the system is intended to provide a structure that places equal weight on behavioral functions and upon neural circuits and their constituent elements  that is to be an integrative model rather than one based primarily on either behavior or neuroscience this integrative approach can be seen in the way in which goal 14 is stated the criterion for including a construct in the matrix during the workshops reflects this same priority participants were instructed that there were two requirements for adding a construct to the matrix first there must be strong evidence for the validity of the suggested construct itself as a behavioral function second there must be strong evidence that the suggested construct maps onto a specific biological system such as a brain circuit this rule was carefully followed over the course of the workshop series there were several instances where a nominated construct was not included either because a nominated function could not be paired with an implementing neural system or because a consensus could not be reached regarding the function of a nominated circuit the nimh working groups shorthand expression for this idea was behavioral science studies what the brain evolved to do and neuroscience studies how the brain implements it thus claims that the rdoc system simply involves biomarkers or endophenotypes are oversimplified at best following from this consideration a sixth distinction is that the rdoc project is intended at its inception in particular to concentrate on constructs for which there is solid evidence to serve as a platform for ongoing research there is no claim to include all of the psychopathology that is listed in the various categories of the dsm and icd nosologies this reflects a deliberate decision by nimh to constrain the initial scope of the project to elements for which there is considerable data so as to provide a solid foundation on which to gain experience and indicate how more provisional constructs may be studied profitably in the future finally a researchoriented scheme like rdoc faces both a luxury and a risk in not being tied to fixed definitions of disorders as many commentators have pointed out any changes to dsm or icd criteria prompt considerable upheaval throughout the mental health system  in officially reported prevalence rates in possible insurance reimbursement changes in legal proceedings and declarations of disability in regulatory practice as an experimental classification rdoc does not face these liabilities in fact a strong goal of a research system ought to be its flexibility in dynamically accommodating those research advances that it tries to foster provision must be made to delete constructs that have been superseded by new thinking to add constructs to split one construct into two and so on the nimh rdoc workgroup has actively considered the optimal process for considering such changes which will be disseminated in the near future as this consideration implies and in contrast to clinical nosologies the constructs appearing in the rdoc matrix table 2 are not the only ones that can be studied a new construct can be added to the matrix only when replicated data are furnished to provide evidence that it meets the two criteria indicated above a validated construct and a specifiable neural circuit it follows that such studies could not be conducted if only those constructs listed in the rdoc matrix were permitted for study thus a critical component of rdoc is to permit research involving welljustified experiments seeking to validate constructs that are not currently part of the rdoc matrix or to modify in various ways the extant constructs summary psychiatry lags behind other areas of medicine in building avenues toward a precision medicine approach to diagnosis and will not catch up until a system is available that reflects recent progress in genetics other areas of neuroscience and behavioral science however such a system cannot be implemented until a database is available that can inform its development this is the essential rationale for the rdoc project it is difficult to estimate how long such a project may take already promising developments are being forged by investigators who have probed the circuits from both basic and clinical directions and have related these findings to wellvalidated tasks that measure functioning however the integrative approach that rdoc calls for is so new that unforeseen obstacles surely await the pioneers in this area this is only to be expected in the long run there seems to be a growing consensus in the field that a more empirically based approach must be developed and the inherent qualities of the research process itself should serve to shape midcourse corrections as the project moves forward it should be reiterated however that the rdoc framework is explicitly intended to be a moving target and that the framework should grow and change with the pace of new research findings thus the cuthbert and insel bmc medicine 2013 11126 page 6 of 8 httpwwwbiomedcentralcom1741701511126 challenge is not to design an optimal list of relatively permanent elements but rather to construct a platform that can both accommodate and foster continual developments in research knowledge and methods it will be quite apparent to the reader that rdoc is neither designed nor intended to be used for practical clinical purposes at this early stage the nearterm goal of rdoc rather is to build a new framework of research that can produce pioneering new findings and approaches to inform future versions of psychiatric nosologies in particular the goal is to lay the groundwork for specifying how diagnosticians can accomplish the goal of precision medicine for mental disorders  pinpointing with increasing accuracy the precise genetic neural circuit and behavioral data that can generate tailored recommendations for interventions that can manage cure and prevent mental disorders in the largest possible number of individuals in this sense although the immediate thrust of the rdoc project sets it apart from the established structures of the dsm and icd the longterm aspirations for all three systems converge on reducing the burden of suffering for those with mental disorders', 'abstract human infants are fascinated by other people they bring to this fascination a constellation of rich and flexible expectations about the intentions motivating peoples actions here we test 11monthold infants and stateof theart learningdriven neuralnetwork models on the baby intuitions benchmark bib a suite of tasks challenging both infants and machines to make highlevel predictions about the underlying causes of agents actions infants expected agents actions to be directed towards objects not locations and infants demonstrated default expectations about agents rationally efficient actions towards goals the neuralnetwork models failed to capture infants knowledge our work provides a comprehensive framework in which to characterize infants commonsense psychology and takes the first step in testing whether human knowledge and humanlike artificial intelligence can be built from the foundations cognitive and developmental theories postulate the earlydeveloping ease with which infants know about people gergely n  adasdy csibra  bro  1995 woodward 1998 objects spelke 1990 stahl  feigenson 2015 and places hermer  spelke 1994 is impressive especially compared with the difficulties machines have had in achieving these simple human competencies lake ullman tenenbaum  gershman 2017 marcus  davis 2019 such differ ences between human and artificial intelligence ai are critical to address if we aim to create commonsense ai leading to ai that we better understand and that better understands us one of the general challenges of building commonsense ai is deciding what knowledge to start with a human infants foundational knowledge is limited abstract and reflects our evolutionary inheri tance yet it can accommodate any context or culture in which that in fant might develop spelke 2022 spelke  kinzler 2007 if an aim of ai is to build the flexible commonsense thinker that human adults become then machines might need to start like adults do from the same core abilities as infants whether achieved through learningdriven or engineered approaches botvinick et al 2017 over the past several decades foundational research on infants commonsense psychology ie infants understanding of the intentions goals preferences and rationality underlying agents actions has sug gested that infants attribute goals to agents and expect agents to pursue goals in rationally efficient ways baillargeon scott  bian 2016 gergely et al 1995 spelke 2022 woodward 1998 the predictions that support infants commonsense psychology are foundational to human social intelligence banaji  gelman 2013 jaraettinger gweon schulz  tenenbaum 2016 and could thus inform better commonsense ai but these predictions are typically missing from machinelearning algorithms which instead predict actions directly e g churn clicks likes etc griffiths 2015 and therefore lack flexibility to new contexts and situations nevertheless research on infants commonsense psychology has not yet been evaluated in a framework that could be directly tested against machineslet alone built into thembecause of nonscalable stimuli varied task demands isolated questions and mixed results for example experiments on infants commonsense psychology have exemplified agents and their actions using various displays from live human actors reaching for everyday objects woodward 1998 to live puppets with or without animate features like eyes or fur johnson slaughter  carey 1998 to highly minimal animations of simple shapes navigating in 2d or 3d worlds csibra bro  koos   gergely 2003 csibra gergely bro  koos   brockbank 1999 these experiments have also typically focused on individual questions of eg goal woodward 1998 or ra tionality gergely et al 1995 attribution although some work has probed for example how infants inferences about goals and rationality might combine to support notions of consistency cost or value liu ullman tenenbaum  spelke 2017 scott  baillargeon 2013 different accounts of infants knowledge about agents have  corresponding author email address moiradillonnyuedu mr dillon contents lists available at sciencedirect cognition journal homepage wwwelseviercomlocatecognit httpsdoiorg101016jcognition2023105406 received 8 september 2022 received in revised form 8 february 2023 accepted 9 february 2023 cognition 235 2023 105406 2 suggested that this knowledge coheres as a unified set of abstract con cepts of causal efficacy efficiency goaldirectedness and perceptual access spelke 2022 reflects infants intuitive understanding of agents mental states which direct their efficient actions consistent with their mental states baillargeon et al 2015 baillargeon et al 2016 or emerges from individual achievements rooted in infants own action experience woodward 2009 woodward sommerville  guajardo 2001 from this rich experimental and theoretical tradition thus arises the need for a comprehensive framework in which to characterize in fants knowledge of agents with results on one task comparable with those on another and with results on the suite of tasks comparable across infants and machines such a framework can inform both theories of infants knowledge and the future of humanlike ai here we take a critical step in addressing this need we provide a comprehensive framework for testing infants commonsense psychology by assessing infants performance on the baby intuitions benchmark bib a suite of six tasks probing commonsense psychology bib was designed expressly to allow for testing both infant and machine intelli gence alike gandhi stojnic lake  dillon 2021 and fulfilling that intention here we also directly compare the performance of infants and machines providing an empirical foundation for building humanlike ai fig 1 schematic of bibs six tasks used in experiments 1  2 see also fig s1 for each task observers first see eight familiarization trial videos in which an agent acts consistently in terms of its goals rationality or instrumentality the exact makeup of the grid world and the movement of the agent may vary across trials as described in the main text and si one example still image per task from a familiarization trial video is shown here observers then see expected and unexpected test trial videos with the order of these trials varying for infants example still images of both test trial videos per task are shown here all of the videos are available at httpsosfior98je g stojni c et al cognition 235 2023 105406 3 1 general methods 11 materials bibs tasks include short silent animated videos with simple visuals heider  simmel 1944 like basic shapes without eyes or limbs un dertaking basic movements in a grid world figs 1 and s1 this design allowed for the stimulis scalable procedural generation which is required for testing machinelearning algorithms and emphasized the highlevel properties of agents csibra et al 1999 gao mccarthy  scholl 2010 johnson  gilmore 2003 meltzoff 1995 which chal lenges the limits and abstraction of an observers inferential capacity kominsky lucca thomas frank  hamlin 2022 this design also presented a novel overhead navigational context which required an assumption of agents full observability of the grid world and its con tents baker jaraettinger saxe  tenenbaum 2017 luo  baillar geon 2007 luo  johnson 2009 rabinowitz perbet song zhang  botvinick 2018 importantly all of bibs tasks are presentationally consistent allowing for comparisons across tasks without concerns of attributing null effects to varying visual memory or other task demands instead of focusing on one principle of commonsense psychology moreover bibs tasks focus on three possible attributions to agents actions that an observer could makegoal attribution rationality attribution and instrumentality attributionthereby addressing whether and how such principles of commonsense psychology might cohere using bibs environment gandhi et al 2021 we procedurally generated the video stimuli to test infants and computational models and chose the clearest examples of the particular principles of commonsense psychology targeted by each task figs 1 and s1 the first three tasks focus on an observers attribution of goals to agents actions the goaldirected task captures the idea that agents goals are directed towards objects not locations observers watch an agent repeatedly move to the same one of two objects in approximately the same location in an unchanging grid world during familiarization at test observers may be more surprised when the agent moves to a new object in that grid world after the locations of the two objects switch woodward 1998 the multiagent task asks whether goals are specific to agents observers watch an agent move to the same one of two objects during familiarization in a changing grid world with both objects appearing in varying locations at test observers may be more surprised when the original agent versus a new agent moves to a new object buresh  woodward 2007 repacholi  gopnik 1997 the inacces siblegoal task asks whether agents might form new goals when their existing goals become unattainable observers watch an agent move to the same one of two objects during familiarization in a changing grid world with both objects appearing in varying locations at test the grid world changes again such that the agents goal object becomes physi cally inaccessible observers may be more surprised when the agent moves to a new object when its prior goal object is accessible versus inaccessible luo  baillargeon 2007 scott  baillargeon 2013 the next two tasks focus on an observers attribution of rationality to agents actions the efficientagent task captures the idea that agents act rationally to achieve goals observers watch an agent move to an object efficiently around obstacles in an unchanging grid world during famil iarization at test the object appears in a location that it had appeared during familiarization but the grid world has changed such that the obstacles that blocked the object are gone or have been replaced with different obstacles gergely et al 1995 liu  spelke 2017 observers may be more surprised when the agent moves along a familiar but now inefficient path to the object the inefficientagent task asks what ex pectations observers have about agents who initially move inefficiently in a changing grid world during familiarization observers watch an agent move along the same paths to an object as the agent in the effi cientagent task but this time there are no obstacles in the agents way so the agents movements to the object are inefficient at test the environment changes as in the efficient agent task observers may either be more surprised when the agent continues to move inefficiently to the object liu  spelke 2017 or may have no expectations about whether that agent will move efficiently or inefficiently to the object gergely et al 1995 the last task focuses on an observers attribution of instrumentality to agents actions the instrumentalaction task captures the idea that agents should only take instrumental actions when necessary during familiarization observers watch an agent move first to a key which it uses to remove a barrier around an object in varying locations and then to that object at test observers may be more surprised when the agent continues to move to the key instead of directly to the object when the barrier is no longer blocking the object sommerville  woodward 2005 woodward  sommerville 2000 all of the stimuli videos are available at httpsosfior98je and additional details about each task are included in the si bibs task structure adopts the violationofexpectation looking time paradigm often used to test infants spelke 1985 t eglas  et al 2011 observers see a series of familiarization trials that serve to set up an expectation followed by an expected outcome that is perceptually dissimilar to the familiarization but is conceptually consistent and an unexpected outcome that is perceptually similar to the familiarization but is conceptually surprising this task structure has been used in recent machinelearning benchmarks focusing on common sense piloto weinstein battaglia  botvinick 2022 shu et al 2021 smith et al 2019 and is advantageous because it both protects against lowlevel heuristicbased solutions spelke 1985 and allows for an algorithms quantitative measure of surprise to be compared with a wellestablished psychological measure of surprise piloto et al 2022 stahl  kibbe 2022 2 infant methods 21 infant design and analyses in experiment 1 we collected infants responses to two of bibs six tasks the goaldirected task and the efficientagent task mixedmodel linear regressions with raw looking time as the dependent variable outcome expected versus unexpected as a fixed effect and participant as a randomeffects intercept evaluated infants performance on each task and an additional regression examined infants overall perfor mance across both tasks to obtain pvalues we ran type 3 wald tests on the results of each regression experiment 1 focused on these two tasks because the common sense they measured has had consistent findings in the prior literature on infants action understanding baillargeon et al 2016 gergely  csibra 2003 spelke 2022 woodward 2009 experiment 1 thus aimed to provide initial evidence of infants commonsense psychology as elicited by bibs highly minimal displays in bibs fully observable overhead navigational context and with bibs multiple tasks presented to infants online experiment 2 followed a preregistered design and analysis plan httpsosfiop6kba with replications of the two tasks in experiment 1 with several improvements including automated trial progression balancing of the side of the goal object across participants in the goal directed task and matching of the testtrial lengths within participants in the efficientagent task infants were tested on these two tasks as well as on bibs other four tasks outlined above that were not included in experiment 1 following experiment 1 experiment 2 evaluated infants perfor mance on each task with planned mixedmodel linear regressions and type 3 wald tests with raw looking time as the dependent variable outcome expected versus unexpected as a fixed effect and participant as a randomeffects intercept additional planned regressions examined infants overall performance across all six tasks and directly compared their performance on the two tasks focused on agents rational actions g stojni c et al cognition 235 2023 105406 4 22 infant participants in experiment 1 typically developing 11monthold infants n  26 mage  1113 months range  1042 months  1183 months 12 girls born at 37 weeks gestational age were included they completed the goaldirected task the efficientagent task or both with half of the infants receiving each task first totaling n  48 individual testing ses sions and n  24 sessions per task an additional four sessions were excluded because infants did not complete the session in experiment 2 typically developing 11monthold infants n  58 mage  1106 months range  1050 months  1150 months 31 girls born at 37 weeks gestational age were included each infant completed at least one of bibs tasks totaling n  288 individual testing sessions following our preregistration data collection stopped when 32 infants mage  1109 months range  1050 months  1150 months 17 girls completed all six of bibs tasks tasks were presented in a semi randomized order using 32 fixed orders that averaged to each task being presented 533 times in each ordinal position range 47 times all included sessions for each task contributed to the analyses reported here the final sample sizes for each task were goaldirected task n  48 multiagent task n  49 inaccessiblegoal task n  47 efficient agent task n  47 inefficientagent task n  49 instrumentalaction task n  48 the results from the 32 infants who completed all six of bibs tasks were consistent with the results reported here and so are reported in the si an additional 37 sessions were excluded because of preregistered exclusion criteria including looking time  15 s to least one test trial andor two familiarization trials with or without the infant completing the session 16 poor video quality andor technical failure 18 and caretaker interference 3 an additional two sessions were excluded post hoc for extreme values  40 s to one test outcome which could artificially inflate the calculation of the samples variance these extreme values were identified through examination of a histogram of the raw looking times across all of the sessions and across all of the tasks by two researchers masked to the task and outcome represented by each value exclusions were consistent across tasks goaldirected task 5 multiagent task 6 inaccessiblegoal task 9 efficientagent task 7 inefficientagent task 5 instrumentalgoal task 7 the total exclusion rate was 119 participating families received a 5 amazon gift card after each testing session and received a bonus gift card of 30 if they completed all six sessions prior to participation in session one we obtained informed consent from the infants legal guardian and we confirmed consent before each subsequent session the use of human participants for this study was approved by the institutional review board on the use of human subjects at our university 23 infant procedure infants were tested online on zoom in the first ten minutes of the first testing session the experimenter explained to caretakers the in structions for setting up their device and for positioning the infant in front of the screen we asked caretakers to close their eyes and not communicate with the infant during the stimuli presentation the experimenter masked to what trial was being presented and the order of the test trials coded infants looking to the stimuli live from the start of each video and controlled the progression of stimuli using pyhab kominsky 2019 and slidescom each trial video was preceded by a 5 s attention grabber a swirling blob accompanied by a chiming sound centered on the screen to focus the infants attention to the screen and each video froze after the agent reached an object the last frame of the video remained on the screen until infants looked away for 2 s consec utively or for a maximum of 60 s testing sessions were recorded through the zoom recording function capturing both the infants face and the screen presenting the stimuli following our preregistration a different researcher masked to the study outcome what trial was being presented and the order of the test trials recoded 48 randomly chosen sessions 25 from the 32 infants who completed all six tasks the reliability between the first and second coder was very high icc  098 3 infant results infants performance on experiment 1s two tasks is displayed in fig 2 infants looking time varied by task with longer looking to the efficientagent versus goaldirected task f1 71  934 p  003 reflecting the longer testtrial lengths in the efficientagent task see si overall infants looked longer to the unexpected versus expected out comes f1 66  1134 p  001 and there was no task by outcome interaction f1 66  030 p  585 infants were surprised looked longer when an agent moved to a new object in the goaldirected task f 1 23  473 p  040 and they were surprised when an efficient agent later took an inefficient path to an object in the efficientagent task f1 23  260 p  016 infants performance on experiment 2s six tasks is also displayed in fig 2 infants looking time varied by task f5 341  278 p  018 reflecting the different testtrial lengths of the different tasks see si overall infants did not look longer to unexpected versus expected outcomes f1 341  227 p  133 but a task by outcome interaction suggested that different tasks elicited different patterns of infants looking f5 341  223 p  051 we first considered infants performance on experiment 2s three tasks that focused on goal attribution the goaldirected multiagent and inaccessiblegoal tasks first consistent with the results in experiment 1 infants were surprised when an agent moved to a new object in the goal directed task f1 47  409 p  049 infants presented with a new agent in the multiagent task however did not show a difference in surprise when that agent versus the original agent moved to a new object f1 48  341 p  071 with longer looking times to the expected outcome infants in the inaccessiblegoal task also did not show a dif ference in surprise when an agent moved to a new object when its goal object was accessible versus inaccessible f1 46  002 p  891 we next considered infants performance on the two tasks that focused on rationality attribution the efficientagent and inefficient agent tasks first consistent with the results in experiment 1 infants were surprised when an efficient agent later took an inefficient path to an object in the efficientagent task f1 46  772 p  008 infants in the inefficientagent task did not show a difference in surprise when an inefficient agent continued to move inefficiently to an object at test f1 48  251 p  119 but when comparing infants performance in the efficientagent and inefficientagent tasks directly there was no signifi cant task by outcome interaction f1 132  049 p  484 we did not find evidence that infants surprise at the inefficient agents later inefficient action was different from their surprise at the efficient agents later inefficient action finally we considered infants instrumentality attribution through their performance on the instrumentalaction task infants did not show a difference in surprise when the agent moved to the tool as opposed to its goal object when the tool was no longer needed to achieve the goal f1 47  003 p  853 4 infant discussion infants successful performance in the goaldirected and efficient agent tasks in both experiments 1 and 2 suggest that they expect agents actions to be goal directed towards objects not locations and that they expect agents goaldirected actions to be rationally efficient these results also show that infants common sense about the underlying causes of agents actions are accessible when testing infants online and are highly abstract infants expectations are elicited by bibs minimal displays and are generalizable to bibs novel overhead navigational context g stojni c et al cognition 235 2023 105406 5 fig 2 infants raw looking times to the two outcomes in each of bibs tasks in experiments 1  2 gray lines connect the individual looking times represented by blue and yellow dots of each infant to each outcome red dots connected by red lines indicate the mean looking times to each outcome for each task beta coefficients are effects sizes in terms of standard deviations and statistical analyses are reported in the main text p  05 p  01 for interpretation of the references to colour in this figure legend please see the on line version g stojni c et al cognition 235 2023 105406 6 this latter suggestion is especially striking given infants success on the efficientagent task since obstacles in the grid world blocked an agents direct access to the goal object given infants sensitivity to and use of agents perceptual access to objects when making inferences about agents actions luo  baillargeon 2007 luo  johnson 2009 infants evidently appreciated bibs blocking obstacles as only physical not perceptual with bibs context providing no information that these obstacles limit an agents perceptual access infants may have inter preted the obstacles as something that agents could see over or see through future studies could explore how infants appreciate the geo metric physical and perceptual affordances of such overhead naviga tional environments infants pattern of performance on bib thus enriches our under standing of their commonsense psychology and raises new questions about the abstract principles that might be inherent to that common sense building on questions of infants sensitivity to agents physical and perceptual access to objects future versions of the goaldirected task could reveal how having an agent move around obstacles to a goal object instead of taking only straight pathsactions providing addi tional cues to agency johnson shimizu  ok 2007 luo  baillargeon 2005might bolster infants goal attribution in that task introducing significant changes to the arrangement of obstacles across the famil iarization and test environments in the goaldirected task moreover could explore the effects of context changes on goal attribution liu  spelke 2017 sommerville  crane 2009 these latter results might also shed light on infants failures in some of bibs other tasks for example infants may have failed in the inaccessiblegoal task because the arrangement of obstacles changed from familiarization to test including in a way that affected one objects physical accessibility in fants may have found a change in the objects accessibility itself sur prising or they may not have generalized the agents goal to this new test environment with significantly different physical affordances because they interpreted this change as indicating two different places in which the agent was acting sommerville  crane 2009 the multi agent task similarly changed the arrangement of obstacles from famil iarization to test although infants may have failed in this task simply because of heightened attention to the new agent who appeared for the first and only time in the expected outcome prior studies showing agentspecific goal attribution had presented the new agent in both test outcomes buresh  woodward 2007 changes to the affordances of the environment from familiarization to test may also explain the pattern of findings in the inefficientagent task which did not differ from the patterns of findings in the efficient agent task in particular previous literature suggests both that infants do not expect an agent who had previously moved inefficiently to later move efficiently when an obstacle present during familiarization is removed from the test environment gergely et al 1995 skerry carey  spelke 2013 and that infants do expect a previously inefficient agent to later move efficiently if the test environment introduces a new obstacle liu  spelke 2017 the changes in the number and location of the obstacles across the inefficientagent tasks familiarization and test environments may have weakly elicited or elicited in only some infants this latter default prediction about rationally efficient goaldirected actions for inefficient agents in the inefficientagent task liu  spelke 2017 future versions of the inefficientagent task could thus focus specifically on the effects of different kinds of changes in the context and in the environments affordances on infants rationality attribution finally given infants successes in previous tasks probing their un derstanding of instrumental actions infants may have failed in bibs instrumentalaction task because they could not understand the tool objects causal efficacy sommerville hildebrand  crane 2008 or the agents ultimate goal specifically prior findings suggesting that infants recognize agents instrumental actions eg the use of a tool relied on tools whose causal efficacy was familiar to infants eg pulling a cloth to bring a toy within reach piaget 1953 sommerville  woodward 2005 or on novel tools with which infants were first given direct experience sommerville et al 2008 the tool infants saw in the instrumentalaction task was both novel and not something they were given experience with future versions of the instrumentalaction task might thus introduce statechanges such as colour changes to the contacted tools and objects which in previous studies have made the causal efficacy of otherwise novel and inscrutable actions appreciable to young infants liu brooks  spelke 2019 skerry et al 2013 5 model methods 51 model design and analyses to examine whether infants intelligence about agents might be re flected in stateoftheart machine intelligence we compared infants performance on bib in experiment 2 to the performance of three learningdriven neuralnetwork models following prior work gandhi et al 2021 rabinowitz et al 2018 the models formed predictions about an agents actions at test based on its actions during familiariza tion to obtain a continuous measure of surprise as a correlate of infants looking time we calculated the models prediction error for each frame of each outcome and considered the frame with the maximum error to compare model and infant performance we then calculated the zscored mean surprisal score to each outcome for each model and the zscored mean looking time to each outcome for infants zscores were calculated within task for an unplanned quantitative comparison of the overall similarity between the infants and each models performance we evaluated the root mean squared error rmse across bibs six tasks using the mean zscore to the unexpected outcome we also included a comparison between infants performance and a baseline which we gave a surprisal score of 0 for all tasks finally to confirm that the models performance on the specific trials presented to infants was representative of their performance more generally and not due to any idiosyncrasies of the particular videos shown to infants we also evaluated the models accuracy on bibs full dataset gandhi et al 2021 because those results were consistent with the models performance on the infant videos and with prior work gandhi et al 2021 they are reported in the si 52 model specifications learningdriven neural network models have accelerated recent advances in ai lecun bengio  hinton 2015 rabinowitz et al 2018 and so we chose to compare such models performance on bib to in fants approaches like reinforcement learning sutton  barto 2018 and inverse reinforcement learning ng  russel 2000 for example have succeeded in learning to control agents and in understanding the actions of agents but these approaches cannot be used with bib because they require privileged information including the ability to actively control agents in the test environment and in the case of reinforcement learning receive a reward infants engage with stimuli like bibs through passive observation and so we based our modeling on the theory of mind net tomnet architecture from rabinowitz et al 2018 which is a neural network designed specifically for passive observation that has been shown to make inferences about an agents underlying mental states from its behavior with this architecture we tested three models from two classes behavioral cloning bc and video modeling gandhi et al 2021 the models schematized architectures are presented in figs 3 and s2 two bc models predicted how an agent would act using the background training as examples of state and action pairs see model training below to predict the agents next action in a test trial bc combined infor mation from the learned features from the previous frame of a testtrial video along with the learned features in the set of familiarizationtrial videos video modeling used a similar strategy architecture and training procedure but it aimed to predict the entire next frame of the g stojni c et al cognition 235 2023 105406 7 testtrial video rather than just the agents next action the two bc models differed in their encoding of the familiarization trials one bc model relied on a simple multilayer perceptron mlp to encode pairs of states and actions independently fig s2 and the other bc model relied on a more complex bidirectional recurrent neural network rnn to sequentially encode pairs of states and actions fig 3 the states were encoded with a convolutional neural network cnn which was pretrained using augmented temporal contrast atc stooke lee abbeel  laskin 2020 table s1 provides the cnn specifications and the atc data augmentation details for both the mlp and rnn encoders the model obtained a characteristic embedding rabinowitz et al 2018 of an agent by first aggregating the embed dings across frames using the average for the mlp and the last step for the rnn for each familiarization trial and by second averaging across familiarization trials when aggregating frames the videos were randomly subsampled to use up to 30 frames to predict the future actions of the agent defined as the continuous change in position based on the video at 3 frames per second the models combined the char acteristic embedding with the current state of the environment also encoded with the cnn see table s2 for the specifications of the bc models the one video model sequentially encoded each familiarization trial by passing up to 30 frames through a cnn and then combining them with a bidirectional rnn the model obtained a characteristic embedding of an agent by averaging the rnn embeddings the model combined the characteristic embedding with the current state of the environment specified by the current frame of the video to predict the next frame of the video at 3 frames per second using a unet archi tecture ronneberger et al 2015 53 model training prior to being tested the models were trained on thousands of background examples provided by the bib dataset gandhi et al 2021 of biblike agents exhibiting simple behaviors in a grid world while the training set included individual components of the test set eg agents movement to objects agents consistent object goals barriers tools etc see below success on the test set required models to flexibly combine representations across the different training tasks moreover since training included only expected outcomes training with labeled videos was not possible the training otherwise used the same familiarization test task design as the test set in one training task an agent moved to one object in varying loca tions in the grid world in a second training task two objects were presented in varying locations in the grid world but always very close to the agent the agent consistently moved to one of the two objects in a third training task the agent moved to one object in varying locations in the grid world at varying points during the familiarization that agent was substituted by another agent finally in a fourth training task a green barrier surrounded an agent and a key the agent retrieved the key to let itself out of the blocked area to move to an object we included five runs of each model type with the runs initialized randomly and trained until they converged on the background training the bc models were trained to minimize mean squared error and the video model was trained to minimize mean squared error in pixel space twenty percent of the background training trials were left out as a validation set and the models were successful at the validation set in predicting agents actions on all of the background training tasks with low prediction errors for example the mse error for the bc models on lstm lstm averaged characteristic characteristic unet prediction mse loss lstm lstm lstm lstm characteristic 8 familiarization trials test frame characteristic 1 characteristic 8 8 familiarization trials tiled to 64 x 64 policy mlp 02 08 prediction mse loss bcrnn model video model fig 3 architecture of the video and bc rnn models gandhi et al 2021 rabinowitz et al 2018 an agentcharacteristic embedding was inferred from the familiarization trials using a recurrent net this embedding with a frame from the test trial was used to predict the next action of the agent in case of the bc model and the next frame of the video using a unet ronneberger fischer  brox 2015 in the case of the video model g stojni c et al cognition 235 2023 105406 8 the validation set was about 003 which is 08 of the maximum possible prediction error 400 the only exception was that the bc rnn model performed an order of magnitude less well compared to the bc mlp model on the training task in which two objects were presented very close to the agent and the agent consistently moved to just one see si 6 model results fig 4 displays the zscored means of the models surprisal scores to the expected and unexpected outcomes for each task see si for addi tional details the zscored means of infants looking times in the tasks of experiment 2 are also displayed model performance shows little resemblance to infant performance first to evaluate machines goal attribution relative to infants we compared infants and models on the goalattribution task unlike in fants who attributed to agents goal objects not goal locations the models either attributed to agents goal locations bc mlp or neither goal objects nor goal locations bc rnn video model next to evaluate machines rationality attribution relative to infants we compared in fants and models on the efficientagent and inefficientagent tasks while models attributed rational action to agents in the efficientagent task to an even greater degree than did infants models did not attribute rational action to previously inefficient agents who act in new envi ronments in the inefficientagent task here the models performance was nearly orthogonal to infants who did attribute rational action to previously inefficient agents who act in new environments the comparisons between machine and infant performance on bibs other three tasks revealed no instances in which the models demon strated positive predictions about agents actions missing from infants predictions in particular while infants may have been relatively more surprised at the appearance of the new agent in the expected outcome of the multiagent task as described above the models did not show a difference in surprise across the two outcomes in the inaccessiblegoal task the video model did appear to be more surprised when the agent moved to a new object when its goal object was accessible unlike the infants but given this models failure on the goaldirected and multi agent tasks its performance is unlikely to reflect an understanding of agents goaldirected actions towards objects for example the model may have learned that the obstacles in the grid world block objects and that agents move to objects this would lead to a lower surprisal score when an agent moved to the one accessible object compared with when it moved to either one of the accessible objects similarly in the instrumentalaction task the models seemed to have succeeded where the infants did not showing greater surprise when the agent moved to the key when it was unnecessary to do so but closer investigation of the models performance shows that this apparent success is limited to test trials in which the green barrier was absent versus present and incon sequential see si a true understanding of instrumental actions would generalize across the presence or absence of the green barrier at test the models thus did not understand agents instrumental actions finally the rmse analysis revealed high values for all infant and model comparisons bc rnn 0319 bc mlp 0492 video model 0297 suggesting little similarity between infant and model perfor mance indeed these rmse values were higher than the one obtained by comparing infants performance to baseline surprisal scores of 0 for all tasks 0143 7 model discussion bib was expressly designed to allow for testing both infant and ma chine intelligence alike gandhi et al 2021 providing an empirical foundation for building humanlike ai while the performance of the models tested here has not previously been compared with human performance let alone with infant performance and while and models like these are limited in their capacity for flexible generalization to out ofdistribution novel test displays compared with the displays used for their training a generalization bib requires and infants excel at such models have nevertheless accelerated recent advances in ai lecun et al 2015 rabinowitz et al 2018 our comparison reveals that the stateoftheart machine theory of mind captured in such models is indeed missing key principles of commonsense psychology that infants possess in particular while infants expect agents goaldirected actions to be towards objects not locations models either have no expectations or expect those actions to be towards locations not objects and while infants expect both previously efficient and inefficient agents to exhibit rational and efficient goaldirected actions towards objects in new en vironments models only expect previously efficient agents to act effi ciently in new environments finally where we were unable to find any predictions that infants might have about the goals of new agents about agents goal objects in new environments or about novel instrumental actions models show no additional commonsense psychology our approach of directly comparing infant and machine intelligence allows us to specify what principles of commonsense psychology are present in infants yet missing in machines thereby inspiring new di rections in engineering ai for example alternative models based on bayesian inverse planning have been applied successfully to tasks like bib by making more explicit abstract inferences about mental states baker et al 2017 baker saxe  tenenbaum 2009 shu et al 2021 nevertheless extending the bayesian approach to bib in particular and to videos in general is not straightforward a video format does not by itself provide the identification of the agents or objects present in the scene let alone any relations among them recent approaches based on inverse reinforcement learning sim  xu 2019 yu yu finn  ermon 2019 could also be promising but as reviewed above they require online active sampling from the testing environment and bibs envi ronment like much of infants experience involves passive viewing it thus remains an open challenge for learningdriven systems to acquire sufficiently rich abstract structure from bibs training to match infant commonsense intelligence nevertheless setting infant common sense as a benchmark for machine common sense promises to give ai the foundations of human intelligence 8 general discussion bib includes six highly minimal but presentationally consistent tasks focusing on three highlevel principles of commonsense psychology goal attribution rationality attribution and instrumentality attribution infants successes on bib suggest they have a highly abstract notion of agents actions as goaldirected towards objects and a principle of ra tionality that leads to default expectations of agents efficient actions towards goals these results are consistent with the rich literature on infants commonsense psychology baillargeon et al 2015 baillargeon et al 2016 spelke 2022 woodward 2009 woodward et al 2001 and synthesize the literatures findings in a unified framework that can be directly compared withand perhaps built intomachine intelli gence in addition bib uniquely reveals that infants appreciate agents actions in a novel overhead navigational context here recognizing obstacles as physical but not perceptual barriers to action infants failures on bib suggest that changes to the contexts in which goals are first demonstrated may have significant impacts on infants goal and rationality attribution liu  spelke 2017 sommerville  crane 2009 for example infants may not generalize an agents goal to a test environment with even minimal or inconsequential changes relative to the environment in which the goal was initially demonstrated if those changes suggest that agents are acting in a new place regardless of how infants might come to understand the geometry of bibs envi ronment their sensitivity to and use of where an agent is for goal and rationality attribution is apparent future studies might thus investigate infants use of such geometry for recognizing places based on their shape or navigability even before infants can navigate on their own deen g stojni c et al cognition 235 2023 105406 9 fig 4 zscored means of the models surprisal scores each model is shown with a different shape and in a different shade of gray and the z scored means of infants looking times shown in red to the expected and unexpected outcomes in each of bibs six tasks in experiment 2 models differ from infants in terms of infants successful goal and ra tionality attribution a and models show no additional commonsense psychology missing from infants performance b for interpretation of the references to colour in this figure legend please see the online version g stojni c et al cognition 235 2023 105406 10 et al 2017 kosakowski et al 2021 future work exploring infants knowledge about the world could extend our general approach to investigate other aspects of infant commonsense psychology because bibs tasks are procedurally gener ated and presentationally consistent for example new tasks could easily be incorporated into bibs dataset future studies might explore ex pectations of agents notions of cost and value jaraettinger et al 2016 liu et al 2017 or recognition of agents actions that might signal potential social partnerships meltzoff 2007 powell  spelke 2013 schachner  carey 2013 tomasello 2018 while we show that learningdriven neuralnetwork approaches already fall short of infants common sense on bibs existing tasks such expectations will never theless become increasingly important for ai too as it becomes further embedded in realworld multiagent settings that demand common sense extending our approach can ultimately inform comprehensive accounts of infants knowledge not only about agents but also about objects lin stavans  baillargeon 2022 spelke 1990 stahl  fei genson 2015 and places hermer  spelke 1994 allowing us to more fully describe the origins and development of human common sense and provide an avenue for building the future of humanlike ai bib called for an interanimating research program between devel opmental cognitive science and artificial intelligence the present work demonstrates that such a program is both possible and generative for both fields our work provides a first step in this productive dialogue between the cognitive and computational sciences to test whether knowledge can be built in human or machine from the foundations that cognitive and developmental theories postulate', 'happiness is a valuable experience and societies want their citizens to be happy although this societal commitment seems laudable overly emphasizing positivity versus negativity may create an unattainable emotion norm that ironically compromises individual wellbeing in this multinational study 40 countries 7443 participants we investigate how societal pressure to be happy and not sad predicts emotional cognitive and clinical indicators of wellbeing around the world and examine how these relations difer as a function of countries national happiness levels collected from the world happiness report although detrimental wellbeing associations manifest for an average country the strength of these relations varies across countries peoples felt societal pressure to be happy and not sad is particularly linked to poor wellbeing in countries with a higher world happiness index although the crosssectional nature of our work prohibits causal conclusions our fndings highlight the correlational link between social emotion valuation and individual wellbeing and suggest that high national happiness levels may have downsides for some humans value happiness around the world individuals share a similar aspiration to lead a satisfying and happy life1  yet there is also an emerging recognition that this personal quest in itself may have wellbeing consequences placing a premium on the value of positive emotion is known to paradoxically undermine our wellbeing not only as a function of how we value happiness ourselves24  but also as a function of how the society we live in emphasizes the importance of being happy57  preliminary work suggests that the deleterious efects of pursuing happiness may vary across nations89  here we aim to provide a robust crossnational test of this predictive efect investigating whether the experienced social value placed on happiness ironically relates to poorer wellbeing across a large sample of nations we also examine a previously unexplored source of variance between countries their global levels of selfreported happiness assessed with an established metric for societal wellbeing the world happiness index whi10 by examining the link between social valuation of emotion and wellbeing across the globe we aim to provide further insight into the important link between culture and individual emotional functioning1112 the societal pressure to be happy and subjective wellbeing happiness or in scientifc terms high subjective wellbeing13 is advantageous and desired not in the least because it signals accomplishment and optimal functioning14 for individuals high subjective wellbeing is associated with personal thriving in various life domains eg work social relations physical health1517 but also for nations more broadly social indicators research consistently illustrates that happy inhabitants indicate societal fourishing on economic social and political fronts18 together these favorable outcomes explain peoples natural tendency to value happiness both for themselves and their fellow man although this social engagement with happiness appears admirable recent research also highlights the risks of overly promoting positive emotion which can result in a felt social pressure to be happy1923 today the message that happiness is an important life goal is expressed at many diferent levels in modern societies and socialemotion research shows that people readily internalize these salient emotion standards2426 on a macro level for example the prominence of happiness is evidenced explicitly by the numerous happiness coaches campaigns and selfhelp books that provide us with tips and tricks to cultivate the most positive mindset27 but also more implicitly by the seemingly perfect lives of infuencers on social media28 and the ubiquity of smiling faces and happiness allusions in primetime commercials and magazines2930 on a microlevel people may feel pressured by their friends family or colleagues to present themselves in an overly positive way because these close social contacts directly or indirectly encourage them to feel happy3132 at last this subjective experience may even exist in the absence of concrete objective antecedents33 regardless of the specifc mechanism this onesided social emphasis on happiness also risks simultaneously cultivating the perception that there is little room for negativity9  indeed in many modern societies or social groups the natural experience of negative emotion is easily stigmatized34 regarded as maladaptive for our mental wellbeing35 and as something trou blesome that instantly needs cure3637 also in this case social others may shape the internal expectation that negativity is undesired2425 nevertheless occasional feelings of stress sadness or anxiety are an inevitable reality for every human being making it virtually impossible to constantly comply with the apparent stringent norm to be happy5  because this unattainable standard readily reveals discrepancies between our actual emotional life and the emotions society apparently approves of the perceived failure to meet social expectations is known to trigger negative metaemotions pessimistic selfattitudes and ruminative responding7203839 with the resulting ironic aggravation of these undesired emotional states4041 eventually the chronic failure to adhere to these unrealistic emotion standards may compromise peoples wellbeing as demonstrated by a large body of correlational and experimental research with various indicators of subjective wellbeing13 emotionally the experimental induction to value happiness eg via happiness extolling mock articles or verbal communication paradoxically elicits blunted positive emotional responding to enjoyable events3  increased rumination over negative emotion7  and stronger feelings of loneliness22 on the fip side experiencing societal pressure to avoid negativity eg induced via mock articles that emphasize the social cost of negative emotion instigates increased negative emotion both in terms of intensity and duration9  and equally triggers loneliness19 cognitively the societal valuation of positive emotions and the perceived devalu ation of negative ones relates to lower life satisfaction judgments for people who occasionally feel negative921 finally in the clinical realm excessively valuing positivity has been linked to more depressive symptoms in both adolescent23 and adult samples42 and compared to healthy controls depressed patients hold stronger beliefs that they should feel more positive and less negative43 within individuals perceiving social pressure not to feel negative paradoxically predicts increases in depressive symptomatology over time5  3 vol0123456789 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports an important question that currently remains unanswered is to what extent the detrimental link between the felt social pressure to be happy and individual wellbeing is universal versus culturespecifc1112 te vast majority of cited studies are typically confned to singlenation western samples eg 51920 in the few cases where cultural variation is central to their investigation researchers mainly relied on a small sample of diferent geographical regions eg united states germany russia east asia8  or their study was limited to diferent nationalities living in the same country eg australian and east asian students living in australia9  terefore a comprehensive and crossnational evaluation of the tendency to place a social premium on happiness and the associated wellbeing problems with doing so together with the examination of potential countrylevel moderators is a crucial next step in understanding the link between social emotion valuation and individual adaptive functioning21 countrylevel happiness the world happiness index although there are many avenues through which the social value placed on happiness may be inadvertently com municated and reinforced it is possible that the happiness seen in other members of society may aggravate the ironic and negative predictive wellbeing efects of the felt social pressure to achieve personal happiness4446 signs of human happiness can manifest in a multitude of ways47 and are not limited to the explicit expression of overt joyful behavior alone eg smiling facial expressions positive verbal communication etc happiness is also evident in other more subtle implicit overt cues eg having more social contact engaging in pleasurable activities etc and fnally also includes truly covert experiences of joy and related behaviors eg feeling happy providing a high happiness rating in a wellbeing survey etc if the happiness that is displayed by other citizens adds to the personal pressure to be happy or amplifes its predictive wellbeing efects then national levels of selfreported happiness within a given society could pick up on this process one of the most prominent and established barometers to evaluate national levels of self reported happiness is the whi an annual metric published by the sustainable development solution network commissioned by the united nations10 based on the subjective happiness ratings of a largescale and nationally representative sample collected by the gallup world poll48 this initiative aims to present a global ranking of the most happy and unhappy nations in the world at its core the whi is thought to summarize how happy the average person within a country typically feels10 however it is equally possible that in countries in which citizens report higher levels of happiness people on average also experience more social pressure to be happy and not sad because social norms prescribing the value of happiness are elevated within these countries if this hypothesis is correct we should expect a meaning ful countrylevel relation between the average perceived social pressure to be happy within a country and its national whi score second the possibility exists that in countries with higher national happiness levels peoples own personal failure to at times live up to societys prescribed standard to be happy may be accentuated by other peoples actual happiness based on an integration of the previously cited body of sociocultural68 and metaemo tional3843 research it is possible that for some individuals the happiness seen in others may set up a forced social comparison context4446 in which discrepancies between ones own emotional life and societys perceived expectations are more painfully apparent because others seemingly comply with the prevailing standard to be happy with little trouble in this regard social network research shows that happiness is distributed unequally within societies4950 and this imbalance in happiness could create the detrimental basis for social comparison in a population indeed for people who regularly experience negative emotion being confronted with happy people inevitably highlights the fact that their feelings are out of step with the emotional lives of others46 and this selfother incongruity could aggravate the negative predictive wellbeing efects of the felt social pressure to strive for happiness72045 if this rationale is correct we should expect that the negative relation between peoples perceived social pressure to pursue happiness and their wellbeing is ironically stronger in high whi countries the current study to determine how the perceived social pressure to pursue happiness relates to peoples subjective wellbeing around the world we conducted a largescale crossnational study 40 countries 7443 participants in a frst step we examined whether the detrimental wellbeing associations of this felt pressure replicated across a wide array of countries we surveyed for both participants perceived social pressure to be happy51 as well as not to be depressed or anxious20 regarding their subjective wellbeing we acknowledged the multicomponential structure of this construct1352 in line with established conventions on how to survey subjective wellbeing53 we considered both emotional ie the frequency and intensity of positive pa and negative afect na cogni tive ie life satisfaction and clinical indicators ie depressive anxiety and stressrelated symptomatology in a second step we examined the role of nations global happiness levels as a potential source of between country variance explaining the negative wellbeing associations of the felt social pressure to feel positive and not negative to this end we obtained a global whi score for each participating country from the world hap piness report10 tis score is based on the average life evaluation of a nationally representative sample48 using the cantril ladder54 respondents are asked to evaluate the quality of their current life on a 11rung ladder that ranges from worst possible life zero to best possible life ten consequently the whi is more an indication of the average life satisfaction displayed by the inhabitants of a particular country rather than their global subjec tive wellbeing10 first we explored whether higher national whi scores are associated with stronger felt social pressure to be happy and not anxious or depressed second we examined the moderating role of countries whi score on the relation between this felt social pressure and peoples wellbeing we hypothesized that the perceived societal pressure to feel happy and not anxious or depressed ironically shows stronger detrimental relations with peoples 4 vol1234567890 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports subjective wellbeing in high whi nations in these happy contexts the painful observation that other people are seemingly emotionally able to live up to societys expectations when you yourself are unable to do so likely makes personal deviations from the desired emotion standard more salient2444 in this sense the negative predictive wellbeing efects of the felt social pressure to strive for happiness and avoid sadness are likely to be reinforced in high whi countries to evaluate how the perceived societal pressure to feel a positive and b not negative is linked to poor sub jective wellbeing we performed two separate series of multilevel models participants nested within countries with the diferent subjective wellbeing indicators as outcomes of interest although previous work established a causal and unidirectional efect of experiencing pressure to be happy and not sad on subjective wellbeing eg 79  we acknowledge that our selection of outcomes and predictors in the current crosssectional multilevel con text is somewhat arbitrary and that we are ultimately restricted to correlational claims but see si 6 where we show that this arbitrary decision does not impact our conclusions for each model we examined the random efect distributions of these types of pressure in the prediction of wellbeing to see if detrimental links manifest globally or whether nationspecifc relations appear next to explore the role of countries whi score we exam ined its countrylevel relation with the average felt social pressure within a country and evaluated its crosslevel interaction with peoples felt social pressure to see whether national happiness levels moderated the within country relations with wellbeing see methods for more detailed information about our statistical analyses results descriptive statistics before answering the research questions central to this investigation diferent ele ments in table 1 deserve special consideration first within nations the interrelation between the social pres sure to feel positive and not negative is moderately positive r054 p0001 tis suggests a common factor in the perceived social pressure to pursue positivity and to avoid negativity but also underscores the unique ness of both constructs second in line with previous research269192123 both types of social pressure show the expected pattern of associations with all wellbeing indicators feeling social pressure to be happy and not sad is associated with reduced life satisfaction experiencing less frequent and intense positive but more frequent and intense negative emotions and more symptoms of depression anxiety and stress rs005 ps0050 finally between countries national whi scores are only signifcantly related to countries average life satisfaction levels r037 p0017 but not with emotional or clinical markers of wellbeing tis confrms the convergent and discriminant validity of this countrylevel metric because the whi only assesses countries average life satisfac tion and not global subjective wellbeing universal versus nationspecifc subjective wellbeing efects te fxed efects in table 2 indicate how the perceived social pressure to be happy and not anxious or depressed separately relate to all subjective wellbeing markers for the average country in our sample tese results are fully in line with the average within nation correlations in table 1 within the average country the social pressure to be happy and not sad is linked to lower life satisfaction judgements s005 ps0024 emotionally experiencing these types of social pres sure relate to less frequent and intense positive but more frequent and intense negative emotions s009 table 1 summary statistics and correlations among all measures iccintraclass correlation ratio of betweencountry variance to total variance within and betweencountry multilevel internal consistencies  were calculated following79 correlations below the diagonal represent the average withincountry correlation between peoples personal scores correlations above the diagonal represent the betweencountry correlations between country means ie national scores whiworld happiness index papositive afect nanegative afect sehssocial expectancies to be happy scale sedassocial expectancies not to feel depressed or anxious scale p05 p01 p001 variables descriptive statistics correlations mean sd icc  within  between 1 2 3 4 5 6 7 8 9 10 whi cognitive subjective wellbeing 1 life satisfaction 430 129 06 80 89 19  05 33  04  12  10 16 04 30 37  emotional subjective wellbeing 2 pa frequency 560 154 03 74 83 47   35  85   51   37   09  42   05  27  21 3 na frequency 474 185 08 75 92  38   43   29 93  64  58  64  54 44  23 4 pa intensity 560 166 03 77 83 39  76   35   32   40   14  35  05  02  21 5 na intensity 480 196 06 77 89  34   41  82   22  57  49  63  48 48  29 clinical subjective wellbeing 6 depression 175 065 12 83 95  44   48  63   39  58  90 88  22 26 06 7 anxiety 169 060 11 78 95  23   33  55   25  52  63  79  32  21  06 8 stress 200 061 12 80 95  25   40  61   31  58  65  70  23 41  23 perceived emotion norm 9 sehs 601 134 11 75 94  05   10  27   08  24  21  22  24  63  23 10 sedas 556 118 11 72 95  21   23  31   18  29  27  23  26  54  26 5 vol0123456789 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports ps0003 rs100 finally in the clinical realm feeling pressured to be happy and not sad predicts stronger symptoms of depression anxiety and general distress s009 ps0001 te predictive wellbeing efects of the felt social pressure not to feel negative are typically stronger than to feel positive however when examining the variability in random efects we observe considerable betweencountry hetero geneity for some wellbeing indicators tis points towards a potential moderating impact of critical nationlevel variables that exacerbate or alleviate the debilitating link between the felt social pressure to be happy and not sad and personal wellbeing in particular countryspecifc patterns appear for positive markers of subjective wellbeing ie life satisfaction pa frequency and intensity for these indicators the average negative relation with both types of social pressure signifcantly switches sign in 3 to 13 of the countries in our sample and is nonsignifcant in 10 to 28 countries in contrast for negative markers of subjective wellbeing ie clinical symptoms na frequency and intensity the average positive association with these types of pressure only turns negative in maximum 1 country and is nonsignifcant in 4 to 8 countries suggesting a more universal detrimental association the moderating role of national happiness levels to explain this betweencountry variability in predictive wellbeing efects we examine the role of nations global levels of selfreported happiness on the per ceived social pressure to be happy and not sad in two ways first inspecting the correlations above the diagonal in table 1 national whi scores are not signifcantly related to the average felt societal pressure within a country rs023 ps0112 tus contrary to what we predicted the perceived societal norms that prescribe people to feel happy and not anxious or depressed are not particularly elevated in countries with a high whi score second however when exploring the moderating impact of national happiness levels we observe how the withincountry association between almost all subjective wellbeing indicators and the felt societal pressure to be happy and not sad changes as a function of a countrys whi score see fig 1 for the perceived social pressure to be happy signifcant crosslevel interactions with nations whi score indicate that these efects are stronger in countries that report higher levels of national happiness in line with our hypothesis perceiving social pres sure to be happy is linked to poorer subjective wellbeing in high whi countries both emotionally s007 ps0016 rs87 cognitively 008 p0003 and clinically s003 ps0006 indeed as can be seen from panel a in fig 2 comparing the countries in our sample with a lower  1 sd versus higher  1 sd whi score the link between peoples perceived social pressure to be happy and their subjective wellbeing is substantially stronger in the latter in terms of absolute magnitude diferences the abso lute explanatory efect of the perceived social pressure to be happy in peoples wellbeing is almost always small to nonexistent in low whi nations s008 except for the prediction of the frequency and intensity of na feelings in contrast in high whi nations the absolute predictive efect of this pressure ranges from 012 to 044 in terms of relative magnitude the absolute diference in predictive efects between low and high whi countries is the smallest for the clinical indicators diference in s006 and the largest for the emotional indicators of psychological wellbeing diference in s012 finally the graphical visualization of these signifcant crosslevel interactions in fig 3 panel a further unfolds the moderating impact of national whi scores independent of whi status the perceived social pres sure to be happy predicts poorer subjective wellbeing in all indicators however in high whi countries this prediction is always stronger generally producing larger diferences in wellbeing in happier nations between people who experience little versus a great deal of social pressure to be happy table 2 exploring the universality of the detrimental wellbeing efects of the perceived social pressure to be happy and not to be depressed or anxious each fxed efect represents the observed relation for the average country in our sample te standard deviation of the random efects distribution describes the observed variability around that average association for each wellbeing variable we report the number of signifcant positive signifcant negative and nullassociations across countries n40 for the perceived social pressure to be happy n39 for the perceived social pressure not to be depressed or anxious due to an irreversible coding error for poland both types of pressure were withincountry centered te number of associations that mirror the fxed efect are bolded papositive afect nanegative afect p05 p01 p001 perceived social pressure to be happy perceived social pressure not to be depressed or anxious fixed efect sd random efects  positive  negative  null fixed efect sd random efects  positive  negative  null cognitive subjective wellbeing life satisfaction  005 011 5 10 25  023 010 5 24 10 emotional subjective wellbeing pa frequency  011 014 4 14 22  027 015 13 18 8 na frequency 036 009 36 0 4 046 008 35 0 4 pa intensity  009 013 3 10 27  024 013 3 28 8 na intensity 036 010 32 0 8 046 009 34 0 5 clinical subjective wellbeing depression 010 005 32 0 8 014 004 33 0 6 anxiety 009 003 33 1 6 011 003 31 0 8 stress 010 004 34 0 6 012 004 32 1 6 6 vol1234567890 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports for the felt societal pressure not to be depressed or anxious we observe a pattern of results that is highly similar here signifcant crosslevel interactions with countries whi score indicate that most negative predic tive wellbeing efects of participants perceived societal pressure to not feel negative are stronger in countries that report higher levels of national happiness emotionally in countries with a high whi score the perceived societal pressure to avoid negative emotion shows stronger ties to a reduced experience of pa both in terms of frequency and intensity s009 ps0005 rs93 and an increased experience of na in terms of intensity 007 p0017 r67 but not frequency 004 p0146 r80 cognitively this societal pressure predicts poor life satisfaction particularly in happy nations 007 p0005 clinically in high whi countries feeling socially pressured not to feel depressed or anxious paradoxically predicts more symptoms of depression and general distress s003 ps0003 but not anxiety 002 p0057 an explicit comparison of lower  1 sd versus higher  1 sd whi nations in fig 2 panel b further elucidates the diferences in the strength of these withincountry associations for all wellbeing indicators the link with peoples perceived social pressure not to feel anxious or depressed is substantially weaker in low whi countries except for the frequency of na and anxiety symptoms regarding absolute magnitude diferences the absolute explanatory efect of the perceived social pressure not to feel negative in peoples wellbeing never exceeds 019 in countries with a lower whi score except for the frequency and intensity of na in contrast in high whi countries the absolute signifcant predictive efect of this pressure is almost always higher ranging from 015 to 053 in terms of relative magnitude the absolute diference in explanatory efects between low and high whi nations is smallest for anxiety symptoms diference in 003 and strongest for the frequency in pa diference in 017 finally panel b in fig 3 again illustrates that for all wellbeing indicators people who experience social pres sure not to feel negative always report poorer subjective wellbeing irrespective of their countrys whi score however in high whi nations this link is usually stronger except for na frequency and anxiety symptoms figure 1 exploring the diferential role of the perceived social pressure a to be happy and b not to be depressed or anxious in various wellbeing indicators as function of countries world happiness index whi dots represent the magnitude of the fxed efects in each multilevel model redsignifcant graynon signifcant with 05 error bars refer to the 95 confdence interval intercepts are not presented for optimal visibility but were always signifcant with ps001 original values standard errors test statistics and pvalues can be found in supplementary tables 2 and 3 papositive afect nanegative afect lslife satisfaction sehssocial expectancies to be happy scale sedassocial expectancies not to feel depressed or anxious scale 7 vol0123456789 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports which leads diferences in personal wellbeing in happier countries to be more extreme when comparing citizens who perceive considerable versus little social pressure not to feel negative discussion te present crossnational study set out to explore how the perceived social emotion norms to pursue positivity and avert negativity play a role in peoples subjective wellbeing around the world breaking down wellbeing into its diferent constituents1352 we robustly demonstrated how the perceived societal premium on happiness and aversion of sadness in most countries paradoxically relates to fewer and less intense experiences of posi tive emotions with an opposite pattern for negative emotions lower life satisfaction evaluations and more symptomatic complaints related to depression anxiety and general distress however we also demonstrated that these negative predictive wellbeing efects are not entirely universal corroborating the preliminary fndings of earlier studies89  particularly for positive markers of subjective well being we observed how the predictive efects of the felt social pressure to be happy and not sad are subject to substantial national diferences tat is in a great number of countries feeling socially pressured to be happy and not sad was actually unrelated to positive wellbeing and in a small minority an opposite association even emerged here the perceived social premium on happiness was related to higher life satisfaction evaluations and figure 2 te predictive efect of peoples perceived social pressure a to be happy and b not to be depressed or anxious for all wellbeing indicators in high and low whi countries  11 sd te magnitude and transparency of the edges corresponds with the strength of the association green lines represent positive relations red lines negative relations gray lines indicate that the crosslevel interaction was nonsignifcant meaning that the personlevel relation between the perceived emotion norm and subjective wellbeing did not meaningfully difer in low versus high whi countries also denoted with an  whiworld happiness index sehssocial expectancies to be happy scale sedassocial expectancies not to feel depressed or anxious scale papositive afect nanegative afect ffrequency iintensity lslife satisfaction depdepressive symptoms anxanxiety symptoms strstress symptoms 8 vol1234567890 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports a more frequent and intense experience of positive emotion in contrast for negative indicators of subjective well being the predictive efects of the felt social pressure to be happy and not sad were more universally negative in almost all countries experiencing pressure to be happy and not sad was related to more and stronger negative feelings and stronger symptoms of depression anxiety and stress tis diference suggests that the presence of negative wellbeing eg frequent and intense negative emotions or psychopathological symptoms likely dis closes discrepancies with the prevailing societal emotion standard that are more salient compared to the absence of positive wellbeing eg little to low positive emotions or poor life satisfaction tis fnding coincides with figure 3 unfolding all crosslevel interactions between countries whi score and participants perceived social pressure a to be happy and b not to be depressed or anxious in the prediction of individual subjective well being to distinguish between low  1 sd and high 1 sd values we adopted the average withincountry sd for personlevel predictors and the betweencountry sd for countries whi score gray plots indicate that the crosslevel interaction was not signifcant also denoted with an  whiworld happiness index sehssocial expectancies to be happy scale sedassocial expectancies not to feel depressed or anxious scale papositive afect nanegative afect lslife satisfaction 9 vol0123456789 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports the observation that experiencing social pressure to avoid negativity overall typically shows poorer wellbeing associations than the pressure to pursue positivity exploring the factors that drive this countrylevel variability in predictive wellbeing efects we found that feeling pressured to be happy and not sad is particularly associated with poor wellbeing in countries with high national happiness levels although these perceived happiness norms in themselves are on average not more elevated among citizens of highranked whi nations the personal belief that social others view the experience of positive emotions as a key indicator of success in life and devalue negative emotions is found to hold an especially negative relation to peoples wellbeing in happier countries regarding actual causal process explanations the crosssectional nature of our data prevents us to uncover the exact explanatory mechanisms underlying the current moderation but the idea that the happiness seen in others in high whi countries could ironically amplify the negative relation between the perceived social pressure to be happy and personal wellbeing opens up multiple avenues for future research for example it is possible that citizens of high whi nations are typically more expressive of their happiness as previous crosscultural studies established national diferences in emotional display rules55 not only would this explain a higher whi ranking for these countries more overt signs of happiness would also produce a stronger detrimental basis for negative social comparison in unhappy people746 second it could be that high whi countries also sufer from more happiness inequality4950 nationlevel aggregates do not reveal how happiness is distributed within a country53 but a stronger imbalance in happiness would explain why unhappy individuals feel compelled to align their feel ings with those of the majority group if true this perceived social pressure would not only further impair these outcasts personal wellbeing it would also again lead to more happiness inequality ultimately contributing to a selfsustaining feedback mechanism regardless of the specifc mechanism our fndings emphasize that an exclusive focus or overreliance on national aggregates may be misleading to inform wellbeing policy a concern that has been repeatedly expressed by the social indicator movement in the past5356 although the whi is meaningfully related to countries average life satisfaction levels underscoring the construct validity of this index on a national level ie this metric does not echo countrylevel averages in emotional or clinical wellbeing10 the present results suggest that the whi may be less equipped to provide insight into the subjective wellbeing of specifc citizens indeed we found that a higher whi ranking does not necessarily indicate higher subjective wellbeing for everyone within that country as the normative emotion processes in high whi societies may paradoxically work against individual wellbeing for some overall these fndings further illustrate the concern that considerable betweenperson heterogeneity may undermine the unifying quality of countrylevel metrics5657 and highlight the importance of additionally considering the withinnation processes that may explain this variability finally with respect to the potential societal implications of our fndings nationwide psychoeducational campaigns that put the pressing need to be happy in perspective while also acknowledging the valuable role of negative emotion particularly in high whi nations could have benefcial efects for peoples psychological wellbeing in the long run25  in this way the outdated yet dominant societal discourse that promotes a onesided embrace of ones emotions can make way for an updated version in which people learn to appreciate the full scope of their emotional lives both positive and negative limitations te current fndings should be considered in the light of some limitations first in addition to the fact that our claims about the specifc process mechanisms underlying our results remain speculative on the basis of correlational data alone we acknowledge that countrylevel whi scores may not provide the optimal window of analysis to fttingly establish negative social comparison although the whi clearly captures ele ments of this referential process at a macrolevel future studies with a more fnegrained resolution are needed to complement the current work with fndings from microlevel contexts ie social comparison of happiness as a result of immediate social interactions previous work on the prevalence of suicide in happy places has shown that both perspectives do not always converge in their results5859 and other paradoxical patterns described in the happiness literature eg the easterlin paradox60 have highlighted the critical importance of explicitly clarifying the level of analysis when interpreting results in this regard future studies could also beneft from explicitly dis tinguishing between diferent potential sources that shape peoples perceived social pressure to be happy and not sad eg macro versus microlevel implicit versus explicit objective antecedents versus subjective appraisals etc although this pressure is likely multidetermined the instruments currently available do not diferentiate between these diferent factors second to assess the average life satisfaction in our own sample we did not include the original whi cantril ladder1054 but rather relied on dieners traditional satisfaction with life rating scale61 although both instru ments are known to correlate highly62 we cannot simply generalize our fndings to other types of wellbeing assessments similarly compared to the diverse and nationally representative samples in whi research10 this multinational study mainly comprised a student population limiting the generalizability of the found associa tions to other subsamples within a country for example with respect to diferent age groups the factors that contribute to a happy life are known to change remarkably across the life span63 and adolescent students are gen erally more susceptible to the expectations of social others or the infuence of peers compared to adults eg6465 surveying a more balanced research sample and exploring the moderating role of other theoretically relevant demographic covariates both on an individual and societal level will further elucidate the comprehensiveness of the established wellbeing associations finally as is the case in all crossnational studies expecting full language and translation equivalence across countries is difcult eg some emotion words may be interpreted slightly diferent around the world66 nev ertheless national inequivalence would likely introduce more measurement noise to the data acting against establishing meaningful associations 10 vol1234567890 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports materials and methods participants te present research project was part of a larger crossnational study investigating how indi vidual and cultural values infuence emotional wellbeing and moral attitudes around the world te initiating sites were based in australia and belgium and the associated researchers contacted potential collaborators via email in which they outlined the aims and nature of the study and provided an initial copy of the survey mate rials in english upon agreeing to participate all collaborating sites arranged the requisite ethical approval for data collection at their host institution and translated the questionnaires into their native language see si 1 for more information about this process te original study was approved by the psychological sciences human ethics advisory group in australia 16474652 and the socialsocietal ethical committee ku leuven in bel gium g2017 10 954 each collaborating site was asked to enroll a minimum sample of 100 university students that originated from the nation of testing eg no international or exchange students in the end we collected data from 40 diferent countries 42 sites adequately covering all populated con tinents in the world ie europe n  17 asia n  10 africa n  4 south america n  4 north america n  3 oceania n2 a world map with all participating countries can be found in si 2 together with the fnal sample size for each site on average each country collected 186 participants sd129 with a total sample of 7443 participants taking part in the study mage2181 sdage560 te balance of gender identifcation consisted of 32 men 612 women 03 other and 65 unspecifed and the majority of participants 876 were enrolled in a psychology course at the time of the study all participants provided informed consent procedure and materials in each country we adopted a standardized survey battery that was locally translated into participants native language and backtranslated by some but not all host institutions see si 1 for more information to evaluate their subjective wellbeing alongside their perception of the predominant emotion norms in their country participants were only sampled a single time next accessing the public data of the 2019 world happiness report we obtained a global whi score for each participating country10 summary statistics and correlations among all measures can be found in table 1 emotional wellbeing components natural positive and negative afect to evaluate the emotional components in subjective wellbeing the distinct and global experience of positive pa and negative afect na67 we com piled a list of four positive happy joyful relaxed calm and four negative sad depressed stressed anxious emotion items respectively te selection of these emotions was based on the circumplex model of afect68 to ensure an adequate representation of diferent arousal levels we invited participants to rate their everyday emotional experience both in terms of frequency how ofen have you experienced the following emotion during the last month and intensity how intense was your experience of the following emotion as both dimensions are known to relate diferently to subjective wellbeing69 for each emotion item participants provided their response on a 9point likert scale that ranged from none of the time one to all of the time nine for frequency and from very mild one to very intense nine for intensity we averaged samevalenced emotion ratings for each dimension to create a score for pa and na frequency and pa and na intensity cognitive wellbeing component satisfaction with life we assessed life satisfaction with the satisfaction with life scale61 tis 5item questionnaire is designed to capture a broad and integrative evaluation of peoples life eg te conditions of my life are excellent and concerns the cognitivejudgmental component in subjective wellbeing13 participants rated each item on a 7point likert scale ranging from strongly disagree one to strongly agree seven and we averaged across items to get a global life satisfaction score clinical wellbeing components mood complaints to determine the presence of moodrelated symptoma tology experiential factors that usually undermine high subjective wellbeing52 participants had to complete the depression anxiety and stress scale70 tis 21item survey is based on the tripartite model of anxiety and depression71 and consists of three 7item subscales that aim to diferentiate between prototypical symptoms of depression eg i felt downhearted and blue anxiety eg i felt scared without any good reason and general distress eg i tended to overreact to situations70 participants indicated how frequently they experienced each item over the last week on a 4point scale that ranged from not at all zero to most of the time three and we averaged responses per subscale to get an indication of each symptom type severity perceived emotion norms we assessed participants perceived societal expectancies to feel positive with the social expectancies about happiness sehs51 and not to feel negative with the social expectancies about depression and anxiety scale sedas20 te sehs is a 9item survey that evaluates peoples global idea about how they think their society expects people to pursue positivity eg i think that society places a great deal of pressure on people to feel happy or people in my society view people who feel happy as more valuable see si 3 for the full item list conversely the sedas is a 13item instrument that reveals peoples general beliefs about how they think their society disapproves of negative emotional states such as depression or anxiety eg i think soci ety tends to place a lot of pressure on people not to feel depressed or anxious or i think society accepts people who feel depressed or anxious as normal reversed for both scales participants rated each statement on a 9point likert scale that ranged from strongly disagree one to strongly agree nine we averaged across all items afer rescoring the reversed items so that higher sehs and sedas scores indicated stronger individual beliefs that society pressures people to be happy and disapproves of negative emotion respectively due to an irreversible coding error the sedas scores for poland are missing 11 vol0123456789 scientifc reports  2022 121514  httpsdoiorg101038s4159802104262z wwwnaturecomscientificreports world happiness index to get a robust indication of the countrylevel happiness reported within a particular society we evaluated countries whi score a countrys whi score is based on the average life evaluation of a nationally representative sample10 using the cantril ladder54 in this singleitem survey respondents are asked to evaluate the quality of their current life on a 11rung ladder that ranges from worst possible life zero to best possible life ten as such the whi is more an indication of the average life satisfaction displayed by the inhab itants of a particular country rather than their global subjective wellbeing10 cantril ladder evaluations and traditional selfreport measures for life satisfaction eg61 are known to correlate very high62 because data collection took place in 2019 we adopted the whi scores for that year freely accessible online httpsworldhappinessreported2019 te countries that took part in our study representatively covered the global ranking m611 sd086 with the netherlands being the highest ranked country in our sample 749 position 5 and uganda the lowest 419 position 136 out of 156 for the participating sites in england scotland wales and northern ireland we imputed the whi score of the united kingdom in all analyses we used countries actual whi score not their corresponding ranking statistical analyses all analyses in this article were conducted in r version 40072 to reproduce our results and fgures researchers can consult the data code and materials at the open science framework https osfio3aut4 all methods were carried out in accordance with relevant guidelines and regulations multilevel analysis to account for the hierarchical structure of the data we performed our analyses in a multilevel framework using the lme4 rpackage73 specifcally we ran various twolevel models with persons n7443 nested within countries n40 in all models slopes and intercept were allowed to vary randomly across countries to account for possible national diferences in the found efects for an intuitive interpretation of the model parameter estimates we groupmean centered all personlevel predictors countrylevel whi scores were grandmean centered in this way we efectively separated within and betweencountry efects74 all sta tistical tests were twosided to evaluate how the perception of the societal emotion standard in a country diferently relates to subjective wellbeing as a function of nations global happiness level we ran a series of multilevel models with the various wellbeing indicators as the outcome of interest ie cognitive emotional and clinical wellbeing markers at the personlevel we either entered participants perceived societal pressure to feel positive sehs or not to feel negative sedas as the focal predictor separately at the countrylevel we introduced the national whi scores and evaluated the crosslevel interactions with the global intercept and personlevel predictor a generic overview of all model formulae can be found in si 4 we emphasize that our multilevel approach inevitably introduces an asymmetry in the specifed relation between outcome and predictor75 because the selection of an outcome and predictor is always somewhat arbi trary with crosssectional data we additionally ran all reversed models together with a third statistical approach in which all variables were withincountry standardized to remove the asymmetry in a multilevel context76 results can be found in si 6 and illustrate that this arbitrary decision did not impact our conclusions robustness analysis with respect to the emotional wellbeing components we acknowledge that every item operationalization of a pa and na composite score is somewhat arbitrary because there is little theoretical con sensus on how researchers should exactly construct these afective aggregates77 we performed a leaveoneout multiverse analysis for our pa and na constructs eg78 for each of the multilevel models that involved pa or na frequency or intensity as a predictor we evaluated the robustness of each model parameter under diferent pa and na operationalizations because we evaluated four specifc emotion items for each afective construct this yielded 15 alternative pa and na operationalizations each based on a unique combination of emotion items we entered each unique afective aggregate as a predictor in the previously outlined models and evaluated the proportion of models for which the signifcance test of each estimate with 005 yielded identical conclu sions as the model in which the pa and na composites were based on all emotion items of which the results are presented here a higher robustness percentage r indicates that the model parameter is less driven by particular pa and na operationalizations', 'abstract in the last year new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understand ing tasks the glue benchmark introduced a little over one year ago offers a singlenumber metric that summarizes progress on a diverse set of such tasks but performance on the benchmark has recently surpassed the level of nonexpert humans suggesting limited headroom for further research in this paper we present superglue a new benchmark styled after glue with a new set of more diffi cult language understanding tasks a software toolkit and a public leaderboard superglue is available at supergluebenchmarkcom 1 introduction in the past year there has been notable progress across many natural language processing nlp tasks led by methods such as elmo peters et al 2018 openai gpt radford et al 2018 and bert devlin et al 2019 the common thread connecting these methods is that they couple selfsupervised learning from massive unlabelled text corpora with a recipe for effectively adapting the resulting model to target tasks the tasks that have proven amenable to this general approach include question answering sentiment analysis textual entailment and parsing among many others devlin et al 2019 kitaev and klein 2018 ia in this context the glue benchmark wang et al 2019a has become a prominent evaluation framework for research towards generalpurpose language understanding technologies glue is a collection of nine language understanding tasks built on existing public datasets together with private test data an evaluation server a singlenumber target metric and an accompanying expert constructed diagnostic set glue was designed to provide a generalpurpose evaluation of language understanding that covers a range of training data volumes task genres and task formulations we believe it was these aspects that made glue particularly appropriate for exhibiting the transfer learning potential of approaches like openai gpt and bert the progress of the last twelve months has eroded headroom on the glue benchmark dramatically while some tasks figure 1 and some linguistic phenomena figure 2 in appendix b measured in glue remain difficult the current state of the art glue score as of early july 2019 884 from yang et al 2019 surpasses human performance 871 from nangia and bowman 2019 by 13 equal contribution correspondence gluebenchmarkadmingooglegroupscom 33rd conference on neural information processing systems neurips 2019 vancouver canada bilstmelmoattn openai gpt bert  singletask adapters bert large bert on stilts bert  bam sembert snorkel metal alice large mtdnn ensemble xlnetlarge ensemble 05 06 07 08 09 10 11 12 glue score human performance cola sst2 mrpc stsb qqp mnli qnli rte wnli figure 1 glue benchmark performance for submitted systems rescaled to set human performance to 10 shown as a single number score and broken down into the nine constituent task performances for tasks with multiple metrics we use an average of the metrics more information on the tasks included in glue can be found in wang et al 2019a and in warstadt et al 2018 cola socher et al 2013 sst2 dolan and brockett 2005 mrpc cer et al 2017 stsb and williams et al 2018 mnli and rajpurkar et al 2016 the original data source for qnli points and in fact exceeds this human performance estimate on four tasks consequently while there remains substantial scope for improvement towards glues highlevel goals the original version of the benchmark is no longer a suitable metric for quantifying such progress in response we introduce superglue a new benchmark designed to pose a more rigorous test of language understanding superglue has the same highlevel motivation as glue to provide a simple hardtogame measure of progress toward generalpurpose language understanding technolo gies for english we anticipate that significant progress on superglue should require substantive innovations in a number of core areas of machine learning including sampleefficient transfer multitask and unsupervised or selfsupervised learning superglue follows the basic design of glue it consists of a public leaderboard built around eight language understanding tasks drawing on existing data accompanied by a singlenumber performance metric and an analysis toolkit however it improves upon glue in several ways  more challenging tasks superglue retains the two hardest tasks in glue the remain ing tasks were identified from those submitted to an open call for task proposals and were selected based on difficulty for current nlp approaches  more diverse task formats the task formats in glue are limited to sentence and sentencepair classification we expand the set of task formats in superglue to include coreference resolution and question answering qa  comprehensive human baselines we include human performance estimates for all bench mark tasks which verify that substantial headroom exists between a strong bertbased baseline and human performance  improved code support superglue is distributed with a new modular toolkit for work on pretraining multitask learning and transfer learning in nlp built around standard tools including pytorch paszke et al 2017 and allennlp gardner et al 2017  refined usage rules the conditions for inclusion on the superglue leaderboard have been revamped to ensure fair competition an informative leaderboard and full credit assignment to data and task creators the superglue leaderboard data and software tools are available at supergluebenchmarkcom 2 2 related work much work prior to glue demonstrated that training neural models with large amounts of available supervision can produce representations that effectively transfer to a broad range of nlp tasks collobert and weston 2008 dai and le 2015 kiros et al 2015 hill et al 2016 conneau and kiela 2018 mccann et al 2017 peters et al 2018 glue was presented as a formal challenge affording straightforward comparison between such taskagnostic transfer learning techniques other similarlymotivated benchmarks include senteval conneau and kiela 2018 which specifically evaluates fixedsize sentence embeddings and decanlp mccann et al 2018 which recasts a set of target tasks into a general questionanswering format and prohibits taskspecific parameters in contrast glue provides a lightweight classification api and no restrictions on model architecture or parameter sharing which seems to have been wellsuited to recent work in this area since its release glue has been used as a testbed and showcase by the developers of several influential models including gpt radford et al 2018 and bert devlin et al 2019 as shown in figure 1 progress on glue since its release has been striking on glue gpt and bert achieved scores of 728 and 802 respectively relative to 665 for an elmobased model peters et al 2018 and 637 for the strongest baseline with no multitask learning or pretraining above the word level recent models liu et al 2019d yang et al 2019 have clearly surpassed estimates of nonexpert human performance on glue nangia and bowman 2019 the success of these models on glue has been driven by everincreasing model capacity compute power and data quantity as well as innovations in model expressivity from recurrent to bidirectional recurrent to multiheaded transformer encoders and degree of contextualization from learning representation of words in isolation to using unidirectional contexts and ultimately to leveraging bidirectional contexts in parallel to work scaling up pretrained models several studies have focused on complementary methods for augmenting performance of pretrained models phang et al 2018 show that bert can be improved using twostage pretraining ie finetuning the pretrained model on an intermediate datarich supervised task before finetuning it again on a datapoor target task liu et al 2019dc and bach et al 2018 get further improvements respectively via multitask finetuning and using massive amounts of weak supervision anonymous 2018 demonstrate that knowledge distillation hinton et al 2015 furlanello et al 2018 can lead to student networks that outperform their teachers overall the quantity and quality of research contributions aimed at the challenges posed by glue underline the utility of this style of benchmark for machine learning researchers looking to evaluate new applicationagnostic methods on language understanding limits to current approaches are also apparent via the glue suite performance on the glue diagnostic entailment dataset at 042 r3 falls far below the average human performance of 080 r3 reported in the original glue publication with models performing near or even below chance on some linguistic phenomena figure 2 appendix b while some initially difficult categories saw gains from advances on glue eg double negation others remain hard restrictivity or even adversarial disjunction downward monotonicity this suggests that even as unsupervised pretraining produces everbetter statistical summaries of text it remains difficult to extract many details crucial to semantics without the right kind of supervision much recent work has made similar observations about the limitations of existing pretrained models jia and liang 2017 naik et al 2018 mccoy and linzen 2019 mccoy et al 2019 liu et al 2019ab 3 superglue overview 31 design process the goal of superglue is to provide a simple robust evaluation metric of any method capable of being applied to a broad range of language understanding tasks to that end in designing superglue we identify the following desiderata of tasks in the benchmark  task substance tasks should test a systems ability to understand and reason about texts written in english  task difficulty tasks should be beyond the scope of current stateoftheart systems but solvable by most collegeeducated english speakers we exclude tasks that require domainspecific knowledge eg medical notes or scientific papers 3 table 1 the tasks included in superglue wsd stands for word sense disambiguation nli is natural language inference coref is coreference resolution and qa is question answering for multirc we list the number of total answers for 45683166 traindevtest questions the metrics for multirc are binary f1 on all answeroptions and exact match corpus train dev test task metrics text sources boolq 9427 3270 3245 qa acc google queries wikipedia cb 250 57 250 nli accf1 various copa 400 100 500 qa acc blogs photography encyclopedia multirc 5100 953 1800 qa f1aem various record 101k 10k 10k qa f1em news cnn daily mail rte 2500 278 300 nli acc news wikipedia wic 6000 638 1400 wsd acc wordnet verbnet wiktionary wsc 554 104 146 coref acc fiction books  evaluability tasks must have an automatic performance metric that corresponds well to human judgments of output quality certain text generation tasks fail to meet this criteria due to issues surrounding automatic metrics like rouge and bleu callisonburch et al 2006 liu et al 2016 ia  public data we require that tasks have existing public training data in order to minimize the risks involved in newlycreated datasets we also prefer tasks for which we have access to or could create a test set with private labels  task format we prefer tasks that had relatively simple input and output formats to avoid incentivizing the users of the benchmark to create complex taskspecific model architectures nevertheless while glue is restricted to tasks involving single sentence or sentence pair inputs for superglue we expand the scope to consider tasks with longer inputs this yields a set of tasks that requires understanding individual tokens in context complete sentences intersentence relations and entire paragraphs  license we require that task data be available under licences that allow use and redistribu tion for research purposes to identify possible tasks for superglue we disseminated a public call for task proposals to the nlp community and received approximately 30 proposals we filtered these proposals according to our criteria many proposals were not suitable due to licensing issues complex formats and insufficient headroom we provide examples of such tasks in appendix d for each of the remaining tasks we ran a bertbased baseline and a human baseline and filtered out tasks which were either too challenging for humans without extensive training or too easy for our machine baselines 32 selected tasks following this process we arrived at eight tasks to use in superglue see tables 1 and 2 for details and specific examples of each task boolq boolean questions clark et al 2019 is a qa task where each example consists of a short passage and a yesno question about the passage the questions are provided anonymously and unsolicited by users of the google search engine and afterwards paired with a paragraph from a wikipedia article containing the answer following the original work we evaluate with accuracy cb commitmentbank de marneffe et al 2019 is a corpus of short texts in which at least one sentence contains an embedded clause each of these embedded clauses is annotated with the degree to which it appears the person who wrote the text is committed to the truth of the clause the resulting task framed as threeclass textual entailment on examples that are drawn from the wall street journal fiction from the british national corpus and switchboard each example consists of a premise containing an embedded clause and the corresponding hypothesis is the extraction of that clause we use a subset of the data that had interannotator agreement above 80 the data is imbalanced relatively fewer neutral examples so we evaluate using accuracy and f1 where for multiclass f1 we compute the unweighted average of the f1 per class 4 table 2 development set examples from the tasks in superglue bold text represents part of the example format for each task text in italics is part of the model input underlined text is specially marked in the input text in a monospaced font represents the expected model output boolq passage barqs  barqs is an american soft drink its brand of root beer is notable for having caffeine barqs created by edward barq and bottled since the turn of the 20th century is owned by the barq family but bottled by the cocacola company it was known as barqs famous olde tyme root beer until 2012 question is barqs root beer a pepsi product answer no cb text b and yet uh i we i hope to see employer based you know helping out you know child uh care centers at the place of employment and things like that that will help out a uhhuh b what do you think do you think we are setting a trend hypothesis they are setting a trend entailment unknown copa premise my body cast a shadow over the grass question whats the cause for this alternative 1 the sun was rising alternative 2 the grass was cut correct alternative 1 multirc paragraph susan wanted to have a birthday party she called all of her friends she has five friends her mom said that susan can invite them all to the party her first friend could not go to the party because she was sick her second friend was going out of town her third friend was not so sure if her parents would let her the fourth friend said maybe the fifth friend could go to the party for sure susan was a little sad on the day of the party all five friends showed up each friend had a present for susan susan was happy and sent each friend a thank you card the next week question did susans sick friend recover candidate answers yes she recovered t no f yes t no she didnt recover f yes she was at susans party t record paragraph cnn puerto rico on sunday overwhelmingly voted for statehood but congress the only body that can approve new states will ultimately decide whether the status of the us commonwealth changes ninetyseven percent of the votes in the nonbinding referendum favored statehood an increase over the results of a 2012 referendum official results from the state electorcal commission show it was the fifth such vote on statehood today we the people of puerto rico are sending a strong and clear message to the us congress  and to the world  claiming our equal rights as american citizens puerto rico gov ricardo rossello said in a news release highlight puerto rico voted sunday in favor of us statehood query for one they can truthfully say dont blame me i didnt vote for them  when discussing the placeholder presidency correct entities us rte text dana reeve the widow of the actor christopher reeve has died of lung cancer at age 44 according to the christopher reeve foundation hypothesis christopher reeve had an accident entailment false wic context 1 room and board context 2 he nailed boards across the windows sense match false wsc text mark told pete many lies about himself which pete included in his book he should have been more truthful coreference false copa choice of plausible alternatives roemmele et al 2011 is a causal reasoning task in which a system is given a premise sentence and must determine either the cause or effect of the premise from two possible choices all examples are handcrafted and focus on topics from blogs and a photographyrelated encyclopedia following the original work we evaluate using accuracy multirc multisentence reading comprehension khashabi et al 2018 is a qa task where each example consists of a context paragraph a question about that paragraph and a list of possible answers the system must predict which answers are true and which are false while many qa tasks exist we use multirc because of a number of desirable properties i each question can have multiple possible correct answers so each questionanswer pair must be evaluated independent of other pairs ii the questions are designed such that answering each question requires drawing facts from multiple context sentences and iii the questionanswer pair format more closely matches the api of other tasks in superglue than the more popular spanextractive qa format does the paragraphs are drawn from seven domains including news fiction and historical text the evaluation metrics are f1 over all answeroptions f1a and exact match of each questions set of answers em 5 record reading comprehension with commonsense reasoning dataset zhang et al 2018 is a multiplechoice qa task each example consists of a news article and a clozestyle question about the article in which one entity is masked out the system must predict the masked out entity from a given list of possible entities in the provided passage where the same entity may be expressed using multiple different surface forms all of which are considered correct articles are drawn from cnn and daily mail following the original work we evaluate with max over all mentions tokenlevel f1 and exact match em rte recognizing textual entailment datasets come from a series of annual competitions on textual entailment2 rte is included in glue and we use the same data and format as glue we merge data from rte1 dagan et al 2006 rte2 bar haim et al 2006 rte3 giampiccolo et al 2007 and rte5 bentivogli et al 20093 all datasets are combined and converted to twoclass classification entailment and not_entailment of all the glue tasks rte is among those that benefits from transfer learning the most with performance jumping from near randomchance 56 at the time of glues launch to 863 accuracy liu et al 2019d yang et al 2019 at the time of writing given the nearly eight point gap with respect to human performance however the task is not yet solved by machines and we expect the remaining gap to be difficult to close wic wordincontext pilehvar and camachocollados 2019 is a word sense disambiguation task cast as binary classification of sentence pairs given two text snippets and a polysemous word that appears in both sentences the task is to determine whether the word is used with the same sense in both sentences sentences are drawn from wordnet miller 1995 verbnet schuler 2005 and wiktionary we follow the original work and evaluate using accuracy wsc winograd schema challenge levesque et al 2012 is a coreference resolution task in which examples consist of a sentence with a pronoun and a list of noun phrases from the sentence the system must determine the correct referrent of the pronoun from among the provided choices winograd schemas are designed to require everyday knowledge and commonsense reasoning to solve glue includes a version of wsc recast as nli known as wnli until very recently no substantial progress had been made on wnli with many submissions opting to submit majority class predic tions4 in the past few months several works kocijan et al 2019 liu et al 2019d have made rapid progress via a hueristic data augmentation scheme raising machine performance to 904 accuracy given estimated human performance of 96 there is still a gap between machine and human performance which we expect will be relatively difficult to close we therefore include a version of wsc cast as binary classification where each example consists of a sentence with a marked pronoun and noun and the task is to determine if the pronoun refers to that noun the training and validation examples are drawn from the original wsc data levesque et al 2012 as well as those distributed by the affiliated organization commonsense reasoning 5 the test examples are derived from fiction books and have been shared with us by the authors of the original dataset we evaluate using accuracy 33 scoring as with glue we seek to give a sense of aggregate system performance over all tasks by averaging scores of all tasks lacking a fair criterion with which to weight the contributions of each task to the overall score we opt for the simple approach of weighing each task equally and for tasks with multiple metrics first averaging those metrics to get a task score 34 tools for model analysis analyzing linguistic and world knowledge in models glue includes an expertconstructed diagnostic dataset that automatically tests models for a broad range of linguistic commonsense and world knowledge each example in this broadcoverage diagnostic is a sentence pair labeled with 2textual entailment is also known as natural language inference or nli 3rte4 is not publicly available while rte6 and rte7 do not conform to the standard nli task 4wnli is especially difficult due to an adversarial traindev split premise sentences that appear in the training set often appear in the development set with a different hypothesis and a flipped label if a system memorizes the training set which was easy due to the small size of the training set it could perform far below chance on the development set we remove this adversarial design in our version of wsc by ensuring that no sentences are shared between the training validation and test sets 5 httpcommonsensereasoningorgdisambiguationhtml 6 a threeway entailment relation entailment neutral or contradiction and tagged with labels that indicate the phenomena that characterize the relationship between the two sentences submissions to the glue leaderboard are required to include predictions from the submissions multinli classifier on the diagnostic dataset and analyses of the results were shown alongside the main leaderboard since this broadcoverage diagnostic task has proved difficult for top models we retain it in superglue however since multinli is not part of superglue we collapse contradiction and neutral into a single not_entailment label and request that submissions include predictions on the resulting set from the model used for the rte task we collect nonexpert annotations to estimate human performance following the same procedure we use for the main benchmark tasks section 52 we estimate an accuracy of 88 and a matthews correlation coefficient mcc the twoclass variant of the r3 metric used in glue of 077 analyzing gender bias in models recent work has identified the presence and amplification of many social biases in datadriven machine learning models lu et al 2018 zhao et al 2018 kiritchenko and mohammad 2018 to promote the detection of such biases we include winogender rudinger et al 2018 as an additional diagnostic dataset winogender is designed to measure gender bias in coreference resolution systems we use the diverse natural language inference collection dnc poliak et al 2018 version that casts winogender as a textual entailment task6 each example consists of a premise sentence with a male or female pronoun and a hypothesis giving a possible antecedent of the pronoun examples occur in minimal pairs where the only difference between an example and its pair is the gender of the pronoun in the premise performance on winogender is measured with both accuracy and the gender parity score the percentage of minimal pairs for which the predictions are the same we note that a system can trivially obtain a perfect gender parity score by guessing the same class for all examples so a high gender parity score is meaningless unless accompanied by high accuracy we collect nonexpert annotations to estimate human performance and observe an accuracy of 997 and a gender parity score of 099 like any diagnostic winogender has limitations it offers only positive predictive value a poor bias score is clear evidence that a model exhibits gender bias but a good score does not mean that the model is unbiased more specifically in the dnc version of the task a low gender parity score means that a models prediction of textual entailment can be changed with a change in pronouns all else equal it is plausible that there are forms of bias that are relevant to target tasks of interest but that do not surface in this setting gonen and goldberg 2019 in addition winogender does not cover all forms of social bias or even all forms of gender for instance the version of the data used here offers no coverage of genderneutral they or nonbinary pronouns despite these limitations we believe that winogenders inclusion is worthwhile in providing a coarse sense of how social biases evolve with model performance and for keeping attention on the social ramifications of nlp models 4 using superglue software tools to facilitate using superglue we release jiant wang et al 2019b7 a modular software toolkit built with pytorch paszke et al 2017 components from allennlp gardner et al 2017 and the pytorchpretrainedbert package8 jiant implements our baselines and supports the evaluation of custom models and training methods on the benchmark tasks the toolkit includes support for existing popular pretrained models such as openai gpt and bert as well as support for multistage and multitask learning of the kind seen in the strongest models on glue eligibility any system or method that can produce predictions for the superglue tasks is eligible for submission to the leaderboard subject to the datause and submission frequency policies stated immediately below there are no restrictions on the type of methods that may be used and there is no requirement that any form of parameter sharing or shared initialization be used across the tasks in the benchmark to limit overfitting to the private test data users are limited to a maximum of two submissions per day and six submissions per month 6we filter out 23 examples where the labels are ambiguous 7 httpsgithubcomnyumlljiant 8 httpsgithubcomhuggingfacepytorchpretrainedbert 7 table 3 baseline performance on the superglue test sets and diagnostics for cb we report accuracy and macroaverage f1 for multirc we report f1 on all answeroptions and exact match of each questions set of correct answers axb is the broadcoverage diagnostic task scored using matthews correlation mcc axg is the winogender diagnostic scored using accuracy and the gender parity score gps all values are scaled by 100 the avg column is the overall benchmark score on nonax tasks the bolded numbers reflect the best machine performance on task multirc has multiple test sets released on a staggered schedule and these results evaluate on an installation of the test set that is a subset of ours model avg boolq cb copa multirc record rte wic wsc axb axg metrics acc f1acc acc f1aem f1em acc acc acc mcc gps acc most frequent 471 623 217484 500 611  03 334325 503 500 651 00 1000 500 cbow 443 621 490712 516 00  04 140136 497 530 651 04 1000 500 bert 690 774 757836 706 700  240 720713 716 695 643 230 978  517 bert 715 790 847904 738 700  241 720713 790 695 643 380 994  514 outside best  804    844 704245 748730 827       human est 898 890 958989 1000 818519 917913 936 800 1000 770 993  997 data data for the tasks are available for download through the superglue site and through a download script included with the software toolkit each task comes with a standardized training set development set and unlabeled test set submitted systems may use any public or private data when developing their systems with a few exceptions systems may only use the supergluedistributed versions of the task datasets as these use different trainvalidationtest splits from other public versions in some cases systems also may not use the unlabeled test data for the tasks in system development in any way may not use the structured source data that was used to collect the wic labels senseannotated example sentences from wordnet verbnet and wiktionary in any way and may not build systems that share information across separate test examples in any way we do not endorse the use of the benchmark data for nonresearch applications due to concerns about socially relevant biases such as ethnicityoccupation associations that may be undesirable or legally problematic in deployed systems because these biases are evident in texts from a wide variety of sources and collection methods eg rudinger et al 2017 and because none of our task datasets directly mitigate them one can reasonably presume that our training sets teach models these biases to some extent and that our evaluation sets similarly reward models that learn these biases to ensure reasonable credit assignment because we build very directly on prior work we ask the authors of submitted systems to directly name and cite the specific datasets that they use including the benchmark datasets we will enforce this as a requirement for papers to be listed on the leaderboard 5 experiments 51 baselines bert our main baselines are built around bert variants of which are among the most successful approach on glue at the time of writing specifically we use the bertlargecased variant following the practice recommended in devlin et al 2019 for each task we use the simplest possible architecture on top of bert we finetune a copy of the pretrained bert model separately for each task and leave the development of multitask learning models to future work for training we use the procedure specified in devlin et al 2019 we use adam kingma and ba 2014 with an initial learning rate of 105 and finetune for a maximum of 10 epochs for classification tasks with sentencepair inputs boolq cb rte wic we concatenate the sentences with a sep token feed the fused input to bert and use a logistic regression classifier that sees the representation corresponding to cls for wic only we also concatenate the representation of the marked word to the cls representation for copa multirc and record for each answer choice we similarly concatenate the context with that answer choice and feed the resulting sequence into bert to produce an answer representation for copa we project these representations into a scalar and take as the answer the choice with the highest associated scalar for multirc because each question can have more than one correct answer we feed each answer representation into 8 a logistic regression classifier for record we also evaluate the probability of each candidate independent of other candidates and take the most likely candidate as the models prediction for wsc which is a spanbased task we use a model inspired by tenney et al 2019 given the bert representation for each word in the original sentence we get span representations of the pronoun and noun phrase via a selfattention spanpooling operator lee et al 2017 before feeding it into a logistic regression classifier bert we also report results using bert with additional training on related datasets before finetuning on the benchmark tasks following the stilts twostage style of transfer learning phang et al 2018 given the productive use of multinli in pretraining and intermediate finetuning of pretrained language models conneau et al 2017 phang et al 2018 ia for cb rte and boolq we use multinli as a transfer task by first using the above procedure on multinli similarly given the similarity of copa to swag zellers et al 2018 we first finetune bert on swag these results are reported as bert for all other tasks we reuse the results of bert finetuned on just that task simple baselines we include a baseline where for each task we simply predict the majority class9 as well as a bagofwords baseline where each input is represented as an average of its tokens glove word vectors the 300d840b release from pennington et al 2014 outside best we list the best known result on each task to date except on tasks which we recast wsc resplit cb or achieve the best known result wic the outside results for copa multirc and rte are from sap et al 2019 trivedi et al 2019 and liu et al 2019d respectively 52 human performance pilehvar and camachocollados 2019 khashabi et al 2018 nangia and bowman 2019 and zhang et al 2018 respectively provide estimates for human performance on wic multirc rte and record for the remaining tasks including the diagnostic set we estimate human performance by hiring crowdworker annotators through amazons mechanical turk platform to reannotate a sample of each test set we follow a two step procedure where a crowd worker completes a short training phase before proceeding to the annotation phase modeled after the method used by nangia and bowman 2019 for glue for both phases and all tasks the average pay rate is 2375hr10 in the training phase workers are provided with instructions on the task linked to an faq page and are asked to annotate up to 30 examples from the development set after answering each example workers are also asked to check their work against the provided ground truth label after the training phase is complete we provide the qualification to work on the annotation phase to all workers who annotated a minimum of five examples ie completed five hits during training and achieved performance at or above the median performance across all workers during training in the annotation phase workers are provided with the same instructions as the training phase and are linked to the same faq page the instructions for all tasks are provided in appendix c for the annotation phase we randomly sample 100 examples from the tasks test set with the exception of wsc where we annotate the full test set for each example we collect annotations from five workers and take a majority vote to estimate human performance for additional details see appendix c3 53 results table 3 shows results for all baselines the simple baselines of predicting the most frequent class and cbow do not perform well overall achieving near chance performance for several of the tasks using bert increases the average superglue score by 25 points attaining significant gains on all of the benchmark tasks particularly multirc record and rte on wsc bert actually performs worse than the simple baselines likely due to the small size of the dataset and the lack of data augmentation using multinli as an additional source of supervision for boolq cb and rte leads to a 25 point improvement on all tasks using swag as a transfer task for copa sees an 8 point improvement 9 for record we predict the entity that has the highest f1 with the other entity options 10this estimate is taken from httpsturkerviewcom 9 our best baselines still lag substantially behind human performance on average there is a nearly 20 point gap between bert and human performance the largest gap is on wsc with a 35 point difference between the best model and human performance the smallest margins are on boolq cb rte and wic with gaps of around 10 points on each of these we believe these gaps will be challenging to close on wsc and copa human performance is perfect on three other tasks it is in the midtohigh 90s on the diagnostics all models continue to lag significantly behind humans though all models obtain near perfect gender parity scores on winogender this is due to the fact that they are obtaining accuracy near that of random guessing 6 conclusion we present superglue a new benchmark for evaluating generalpurpose language understanding systems superglue updates the glue benchmark by identifying a new set of challenging nlu tasks as measured by the difference between human and machine baselines the set of eight tasks in our benchmark emphasizes diverse task formats and lowdata training data tasks with nearly half the tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples we evaluate bertbased baselines and find that they still lag behind humans by nearly 20 points given the difficulty of superglue for bert we expect that further progress in multitask transfer and unsupervisedselfsupervised learning techniques will be necessary to approach humanlevel per formance on the benchmark overall we argue that superglue offers a rich and challenging testbed for work developing new generalpurpose machine learning methods for language understanding', 'abstract an important paradigm of natural language processing consists of largescale pre training on general domain data and adaptation to particular tasks or domains as we pretrain larger models full finetuning which retrains all model parameters becomes less feasible using gpt3 175b as an example  deploying indepen dent instances of finetuned models each with 175b parameters is prohibitively expensive we propose lowrank adaptation or lora which freezes the pre trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture greatly reducing the number of trainable pa rameters for downstream tasks compared to gpt3 175b finetuned with adam lora can reduce the number of trainable parameters by 10000 times and the gpu memory requirement by 3 times lora performs onpar or better than fine tuning in model quality on roberta deberta gpt2 and gpt3 despite hav ing fewer trainable parameters a higher training throughput and unlike adapters no additional inference latency we also provide an empirical investigation into rankdeficiency in language model adaptation which sheds light on the efficacy of lora we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta deberta and gpt2 at httpsgithubcommicrosoftlora 1 introduction pretrained weights w  rdd x h b  0 a  n0  2  d r pretrained weights w  rdd x fx d figure 1 our reparametriza tion we only train a and b many applications in natural language processing rely on adapt ing one largescale pretrained language model to multiple down stream applications such adaptation is usually done via finetuning which updates all the parameters of the pretrained model the ma jor downside of finetuning is that the new model contains as many parameters as in the original model as larger models are trained every few months this changes from a mere inconvenience for gpt2 radford et al b or roberta large liu et al 2019 to a critical deployment challenge for gpt3 brown et al 2020 with 175 billion trainable parameters1 many sought to mitigate this by adapting only some parameters or learning external modules for new tasks this way we only need to store and load a small number of taskspecific parameters in ad dition to the pretrained model for each task greatly boosting the operational efficiency when deployed however existing techniques equal contribution 0compared to v1 this draft includes better baselines experiments on glue and more on adapter latency 1while gpt3 175b achieves nontrivial performance with fewshot learning finetuning boosts its perfor mance significantly as shown in appendix a 1 arxiv210609685v2 cscl 16 oct 2021 often introduce inference latency houlsby et al 2019 rebuffi et al 2017 by extending model depth or reduce the models usable sequence length li  liang 2021 lester et al 2021 ham bardzumyan et al 2020 liu et al 2021 section 3 more importantly these method often fail to match the finetuning baselines posing a tradeoff between efficiency and model quality we take inspiration from li et al 2018a aghajanyan et al 2020 which show that the learned overparametrized models in fact reside on a low intrinsic dimension we hypothesize that the change in weights during model adaptation also has a low intrinsic rank leading to our proposed lowrank adaptation lora approach lora allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers change during adaptation instead while keeping the pretrained weights frozen as shown in figure 1 using gpt3 175b as an example we show that a very low rank ie r in figure 1 can be one or two suffices even when the full rank ie d is as high as 12288 making lora both storage and computeefficient lora possesses several key advantages  a pretrained model can be shared and used to build many small lora modules for dif ferent tasks we can freeze the shared model and efficiently switch tasks by replacing the matrices a and b in figure 1 reducing the storage requirement and taskswitching over head significantly  lora makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters instead we only optimize the injected much smaller lowrank matrices  our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed introducing no inference latency compared to a fully finetuned model by construction  lora is orthogonal to many prior methods and can be combined with many of them such as prefixtuning we provide an example in appendix e terminologies and conventions we make frequent references to the transformer architecture and use the conventional terminologies for its dimensions we call the input and output di mension size of a transformer layer dmodel we use wq wk wv and wo to refer to the querykeyvalueoutput projection matrices in the selfattention module w or w0 refers to a pre trained weight matrix and w its accumulated gradient update during adaptation we use r to denote the rank of a lora module we follow the conventions set out by vaswani et al 2017 brown et al 2020 and use adam loshchilov  hutter 2019 kingma  ba 2017 for model optimization and use a transformer mlp feedforward dimension df fn  4  dmodel 2 problem statement while our proposal is agnostic to training objective we focus on language modeling as our motivat ing use case below is a brief description of the language modeling problem and in particular the maximization of conditional probabilities given a taskspecific prompt suppose we are given a pretrained autoregressive language model pyx parametrized by  for instance pyx can be a generic multitask learner such as gpt radford et al b brown et al 2020 based on the transformer architecture vaswani et al 2017 consider adapting this pretrained model to downstream conditional text generation tasks such as summarization machine reading comprehension mrc and natural language to sql nl2sql each downstream task is represented by a training dataset of contexttarget pairs z  xi  yii1n  where both xi and yi are sequences of tokens for example in nl2sql xi is a natural language query and yi its corresponding sql command for summarization xi is the content of an article and yi its summary 2 during full finetuning the model is initialized to pretrained weights 0 and updated to 0   by repeatedly following the gradient to maximize the conditional language modeling objective max  x xyz x y t1 log pytx yt 1 one of the main drawbacks for full finetuning is that for each downstream task we learn a different set of parameters  whose dimension  equals 0 thus if the pretrained model is large such as gpt3 with 0  175 billion storing and deploying many independent instances of finetuned models can be challenging if at all feasible in this paper we adopt a more parameterefficient approach where the taskspecific parameter increment    is further encoded by a much smallersized set of parameters  with  0 the task of finding  thus becomes optimizing over  max  x xyz x y t1 log p0ytx yt 2 in the subsequent sections we propose to use a lowrank representation to encode  that is both compute and memoryefficient when the pretrained model is gpt3 175b the number of train able parameters  can be as small as 001 of 0 3 arent existing solutions good enough the problem we set out to tackle is by no means new since the inception of transfer learning dozens of works have sought to make model adaptation more parameter and computeefficient see sec tion 6 for a survey of some of the wellknown works using language modeling as an example there are two prominent strategies when it comes to efficient adaptations adding adapter layers houlsby et al 2019 rebuffi et al 2017 pfeiffer et al 2021 ruckl  e et al 2020 or optimizing some forms  of the input layer activations li  liang 2021 lester et al 2021 hambardzumyan et al 2020 liu et al 2021 however both strategies have their limitations especially in a largescale and latencysensitive production scenario adapter layers introduce inference latency there are many variants of adapters we focus on the original design by houlsby et al 2019 which has two adapter layers per transformer block and a more recent one by lin et al 2020 which has only one per block but with an additional layernorm ba et al 2016 while one can reduce the overall latency by pruning layers or exploit ing multitask settings ruckl  e et al 2020 pfeiffer et al 2021 there is no direct ways to bypass  the extra compute in adapter layers this seems like a nonissue since adapter layers are designed to have few parameters sometimes 1 of the original model by having a small bottleneck di mension which limits the flops they can add however large neural networks rely on hardware parallelism to keep the latency low and adapter layers have to be processed sequentially this makes a difference in the online inference setting where the batch size is typically as small as one in a generic scenario without model parallelism such as running inference on gpt2 radford et al b medium on a single gpu we see a noticeable increase in latency when using adapters even with a very small bottleneck dimension table 1 this problem gets worse when we need to shard the model as done in shoeybi et al 2020 lep ikhin et al 2020 because the additional depth requires more synchronous gpu operations such as allreduce and broadcast unless we store the adapter parameters redundantly many times directly optimizing the prompt is hard the other direction as exemplified by prefix tuning li  liang 2021 faces a different challenge we observe that prefix tuning is difficult to optimize and that its performance changes nonmonotonically in trainable parameters confirming similar observations in the original paper more fundamentally reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task which we suspect makes tuning the prompt less performant compared to other methods we defer the study on task performance to section 5 3 batch size 32 16 1 sequence length 512 256 128  05m 11m 11m finetunelora 1449408 338006 19827 adapterl 1482010 22 354805 50 23921 207 adapterh 1492210 30 366305 84 25822 303 table 1 infernece latency of a single forward pass in gpt2 medium measured in milliseconds av eraged over 100 trials we use an nvidia quadro rtx8000  denotes the number of trainable parameters in adapter layers adapterl and adapterh are two variants of adapter tuning which we describe in section 51 the inference latency introduced by adapter layers can be significant in an online shortsequencelength scenario see the full study in appendix b 4 our method we describe the simple design of lora and its practical benefits the principles outlined here apply to any dense layers in deep learning models though we only focus on certain weights in transformer language models in our experiments as the motivating use case 41 lowrankparametrized update matrices a neural network contains many dense layers which perform matrix multiplication the weight matrices in these layers typically have fullrank when adapting to a specific task aghajanyan et al 2020 shows that the pretrained language models have a low instrisic dimension and can still learn efficiently despite a random projection to a smaller subspace inspired by this we hypothe size the updates to the weights also have a low intrinsic rank during adaptation for a pretrained weight matrix w0  r dk  we constrain its update by representing the latter with a lowrank de composition w0  w  w0  ba where b  r dr  a  r rk  and the rank r mind k during training w0 is frozen and does not receive gradient updates while a and b contain trainable parameters note both w0 and w  ba are multiplied with the same input and their respective output vectors are summed coordinatewise for h  w0x our modified forward pass yields h  w0x  w x  w0x  bax 3 we illustrate our reparametrization in figure 1 we use a random gaussian initialization for a and zero for b so w  ba is zero at the beginning of training we then scale w x by  r  where  is a constant in r when optimizing with adam tuning  is roughly the same as tuning the learning rate if we scale the initialization appropriately as a result we simply set  to the first r we try and do not tune it this scaling helps to reduce the need to retune hyperparameters when we vary r yang  hu 2021 a generalization of full finetuning a more general form of finetuning allows the training of a subset of the pretrained parameters lora takes a step further and does not require the accumu lated gradient update to weight matrices to have fullrank during adaptation this means that when applying lora to all weight matrices and training all biases2  we roughly recover the expressive ness of full finetuning by setting the lora rank r to the rank of the pretrained weight matrices in other words as we increase the number of trainable parameters 3  training lora roughly converges to training the original model while adapterbased methods converges to an mlp and prefixbased methods to a model that cannot take long input sequences no additional inference latency when deployed in production we can explicitly compute and store w  w0  ba and perform inference as usual note that both w0 and ba are in r dk  when we need to switch to another downstream task we can recover w0 by subtracting ba and then adding a different b0a0  a quick operation with very little memory overhead critically this 2they represent a negligible number of parameters compared to weights 3an inevitability when adapting to hard tasks 4 guarantees that we do not introduce any additional latency during inference compared to a finetuned model by construction 42 applying lora to transformer in principle we can apply lora to any subset of weight matrices in a neural network to reduce the number of trainable parameters in the transformer architecture there are four weight matrices in the selfattention module wq wk wv wo and two in the mlp module we treat wq or wk wv as a single matrix of dimension dmodel dmodel even though the output dimension is usually sliced into attention heads we limit our study to only adapting the attention weights for downstream tasks and freeze the mlp modules so they are not trained in downstream tasks both for simplicity and parameterefficiencywe further study the effect on adapting different types of attention weight matrices in a transformer in section 71 we leave the empirical investigation of adapting the mlp layers layernorm layers and biases to a future work practical benefits and limitations the most significant benefit comes from the reduction in memory and storage usage for a large transformer trained with adam we reduce that vram usage by up to 23 if r dmodel as we do not need to store the optimizer states for the frozen parameters on gpt3 175b we reduce the vram consumption during training from 12tb to 350gb with r  4 and only the query and value projection matrices being adapted the checkpoint size is reduced by roughly 10000 from 350gb to 35mb4  this allows us to train with signifi cantly fewer gpus and avoid io bottlenecks another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the lora weights as opposed to all the parameters this allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pretrained weights in vram we also observe a 25 speedup during training on gpt3 175b compared to full finetuning5 as we do not need to calculate the gradient for the vast majority of the parameters lora also has its limitations for example it is not straightforward to batch inputs to different tasks with different a and b in a single forward pass if one chooses to absorb a and b into w to eliminate additional inference latency though it is possible to not merge the weights and dynamically choose the lora modules to use for samples in a batch for scenarios where latency is not critical 5 empirical experiments we evaluate the downstream task performance of lora on roberta liu et al 2019 de berta he et al 2021 and gpt2 radford et al b before scaling up to gpt3 175b brown et al 2020 our experiments cover a wide range of tasks from natural language understanding nlu to generation nlg specifically we evaluate on the glue wang et al 2019 benchmark for roberta and deberta we follow the setup of li  liang 2021 on gpt2 for a direct com parison and add wikisql zhong et al 2017 nl to sql queries and samsum gliwa et al 2019 conversation summarization for largescale experiments on gpt3 see appendix c for more details on the datasets we use we use nvidia tesla v100 for all experiments 51 baselines to compare with other baselines broadly we replicate the setups used by prior work and reuse their reported numbers whenever possible this however means that some baselines might only appear in certain experiments finetuning ft is a common approach for adaptation during finetuning the model is initialized to the pretrained weights and biases and all model parameters undergo gradient updatesa simple variant is to update only some layers while freezing others we include one such baseline reported in prior work li  liang 2021 on gpt2 which adapts just the last two layers fttop2 4we still need the 350gb model during deployment however storing 100 adapted models only requires 350gb  35mb  100  354gb as opposed to 100  350gb  35tb 5 for gpt3 175b the training throughput for full finetuning is 325 tokenss per v100 gpu with the same number of weight shards for model parallelism the throughput is 431 tokenss per v100 gpu for lora 5 model  method  trainable parameters mnli sst2 mrpc cola qnli qqp rte stsb avg robbase ft 1250m 876 948 902 636 928 919 787 912 864 robbase bitfit 01m 847 937 927 620 918 840 815 908 852 robbase adptd  03m 8710 9421 88511 6084 9311 9020 71527 8973 844 robbase adptd  09m 8731 9473 8841 6269 9302 9060 75922 9031 854 robbase lora 03m 8753 9512 8977 63412 9333 9081 8667 9152 872 roblarge ft 3550m 902 964 909 680 947 922 866 924 889 roblarge lora 08m 9062 9625 90912 68219 9493 9161 87425 9262 890 roblarge adptp  30m 9023 9613 9027 68310 9482 9191 83829 9217 884 roblarge adptp  08m 9053 9662 89712 67825 9483 9172 80129 9194 879 roblarge adpth  60m 8995 9623 88729 66544 9472 9211 83411 91017 878 roblarge adpth  08m 9033 9635 87717 66320 9472 9151 72929 9155 864 roblarge lora 08m 9062 9625 90210 68219 9483 9162 85211 9235 886 debxxl ft 15000m 918 972 920 720 960 927 939 929 911 debxxl lora 47m 9192 9692 9266 72411 9601 9291 9494 9302 913 table 2 robertabase robertalarge and debertaxxl with different adaptation methods on the glue benchmark we report the overall matched and mismatched accuracy for mnli matthews correlation for cola pearson correlation for stsb and accuracy for other tasks higher is better for all metrics  indicates numbers published in prior works  indicates runs configured in a setup similar to houlsby et al 2019 for a fair comparison biasonly or bitfit is a baseline where we only train the bias vectors while freezing everything else contemporarily this baseline has also been studied by bitfit zaken et al 2021 prefixembedding tuning preembed inserts special tokens among the input tokens these spe cial tokens have trainable word embeddings and are generally not in the models vocabulary where to place such tokens can have an impact on performance we focus on prefixing which prepends such tokens to the prompt and infixing which appends to the prompt both are discussed in li  liang 2021 we use lp resp li denote the number of prefix resp infix tokens the number of trainable parameters is   dmodel  lp  li prefixlayer tuning prelayer is an extension to prefixembedding tuning instead of just learning the word embeddings or equivalently the activations after the embedding layer for some special tokens we learn the activations after every transformer layer the activations computed from pre vious layers are simply replaced by trainable ones the resulting number of trainable parameters is   l  dmodel  lp  li where l is the number of transformer layers adapter tuning as proposed in houlsby et al 2019 inserts adapter layers between the self attention module and the mlp module and the subsequent residual connection there are two fully connected layers with biases in an adapter layer with a nonlinearity in between we call this original design adapterh  recently lin et al 2020 proposed a more efficient design with the adapter layer applied only after the mlp module and after a layernorm we call it adapterl  this is very similar to another deign proposed in pfeiffer et al 2021 which we call adapterp  we also include another baseline call adapterdrop ruckl  e et al 2020 which drops some adapter layers for  greater efficiency adapterd  we cite numbers from prior works whenever possible to maximize the number of baselines we compare with they are in rows with an asterisk  in the first column in all cases we have   ladpt 2dmodel rrdmodel 2lln dmodel where ladpt is the number of adapter layers and lln the number of trainable layernorms eg in adapterl  lora adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices as mentioned in section 42 we only apply lora to wq and wv in most experiments for simplicity the number of trainable parameters is determined by the rank r and the shape of the original weights   2  llora  dmodel  r where llora is the number of weight matrices we apply lora to 6 model  method  trainable e2e nlg challenge parameters bleu nist met rougel cider gpt2 m ft 35492m 682 862 462 710 247 gpt2 m adapterl  037m 663 841 450 698 240 gpt2 m adapterl  1109m 689 871 461 713 247 gpt2 m adapterh  1109m 6736 85007 4602 7072 24401 gpt2 m fttop2 2519m 681 859 460 708 241 gpt2 m prelayer 035m 697 881 461 714 249 gpt2 m lora 035m 7041 88502 4682 7181 25302 gpt2 l ft 77403m 685 878 460 699 245 gpt2 l adapterl  088m 6911 86803 4630 7142 2490 gpt2 l adapterl  2300m 6893 87004 4611 7132 24502 gpt2 l prelayer 077m 703 885 462 717 247 gpt2 l lora 077m 7041 88902 4682 7202 24702 table 3 gpt2 medium m and large l with different adaptation methods on the e2e nlg challenge for all metrics higher is better lora outperforms several baselines with comparable or fewer trainable parameters confidence intervals are shown for experiments we ran  indicates numbers published in prior works 52 roberta baselarge roberta liu et al 2019 optimized the pretraining recipe originally proposed in bert devlin et al 2019a and boosted the latters task performance without introducing many more trainable parameters while roberta has been overtaken by much larger models on nlp leaderboards such as the glue benchmark wang et al 2019 in recent years it remains a competitive and popular pretrained model for its size among practitioners we take the pretrained roberta base 125m and roberta large 355m from the huggingface transformers library wolf et al 2020 and evaluate the performance of different efficient adaptation approaches on tasks from the glue benchmark we also replicate houlsby et al 2019 and pfeiffer et al 2021 according to their setup to ensure a fair comparison we make two crucial changes to how we evaluate lora when comparing with adapters first we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines second we initialize the model to the pretrained model for mrpc rte and stsb not a model already adapted to mnli like the finetuning baseline runs following this more restricted setup from houlsby et al 2019 are labeled with  the result is presented in table 2 top three sections see section d1 for details on the hyperparameters used 53 deberta xxl deberta he et al 2021 is a more recent variant of bert that is trained on a much larger scale and performs very competitively on benchmarks such as glue wang et al 2019 and su perglue wang et al 2020 we evaluate if lora can still match the performance of a fully finetuned deberta xxl 15b on glue the result is presented in table 2 bottom section see section d2 for details on the hyperparameters used 54 gpt2 mediumlarge having shown that lora can be a competitive alternative to full finetuning on nlu we hope to answer if lora still prevails on nlg models such as gpt2 medium and large radford et al b we keep our setup as close as possible to li  liang 2021 for a direct comparison due to space constraint we only present our result on e2e nlg challenge table 3 in this section see section f1 for results on webnlg gardent et al 2017 and dart nan et al 2020 we include a list of the hyperparameters used in section d3 7 modelmethod  trainable wikisql mnlim samsum parameters acc  acc  r1r2rl gpt3 ft 1752558m 738 895 520280445 gpt3 bitfit 142m 713 910 513274435 gpt3 preembed 32m 631 886 483242405 gpt3 prelayer 202m 701 895 508273435 gpt3 adapterh  71m 719 898 530289448 gpt3 adapterh  401m 732 915 532290451 gpt3 lora 47m 734 917 538298459 gpt3 lora 377m 740 916 534292451 table 4 performance of different adaptation methods on gpt3 175b we report the logical form validation accuracy on wikisql validation accuracy on multinlimatched and rouge12l on samsum lora performs better than prior approaches including full finetuning the results on wikisql have a fluctuation around 05 mnlim around 01 and samsum around 020201 for the three metrics 55 scaling up to gpt3 175b as a final stress test for lora we scale up to gpt3 with 175 billion parameters due to the high training cost we only report the typical standard deviation for a given task over random seeds as opposed to providing one for every entry see section d4 for details on the hyperparameters used as shown in table 4 lora matches or exceeds the finetuning baseline on all three datasets note that not all methods benefit monotonically from having more trainable parameters as shown in fig ure 2 we observe a significant performance drop when we use more than 256 special tokens for prefixembedding tuning or more than 32 special tokens for prefixlayer tuning this corroborates similar observations in li  liang 2021 while a thorough investigation into this phenomenon is outofscope for this work we suspect that having more special tokens causes the input distri bution to shift further away from the pretraining data distribution separately we investigate the performance of different adaptation approaches in the lowdata regime in section f3 6 7 8 9 10 11 log10  trainable parameters 055 060 065 070 075 validation accuracy wikisql method finetune prefixembed prefixlayer adapterh lora 6 7 8 9 10 11 log10  trainable parameters 084 086 088 090 092 multinlimatched figure 2 gpt3 175b validation accuracy vs number of trainable parameters of several adaptation methods on wikisql and mnlimatched lora exhibits better scalability and task performance see section f2 for more details on the plotted data points 6 related works transformer language models transformer vaswani et al 2017 is a sequencetosequence architecture that makes heavy use of selfattention radford et al a applied it to autoregressive lan guage modeling by using a stack of transformer decoders since then transformerbased language models have dominated nlp achieving the stateoftheart in many tasks a new paradigm emerged with bert devlin et al 2019b and gpt2 radford et al b  both are large transformer lan 8 guage models trained on a large amount of text  where finetuning on taskspecific data after pre training on general domain data provides a significant performance gain compared to training on taskspecific data directly training larger transformers generally results in better performance and remains an active research direction gpt3 brown et al 2020 is the largest single transformer language model trained todate with 175b parameters prompt engineering and finetuning while gpt3 175b can adapt its behavior with just a few additional training examples the result depends heavily on the input prompt brown et al 2020 this necessitates an empirical art of composing and formatting the prompt to maximize a models performance on a desired task which is known as prompt engineering or prompt hacking finetuning retrains a model pretrained on general domains to a specific task devlin et al 2019b radford et al a variants of it include learning just a subset of the parameters devlin et al 2019b collobert  weston 2008 yet practitioners often retrain all of them to maximize the downstream performance however the enormity of gpt3 175b makes it challenging to perform finetuning in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as pretraining parameterefficient adaptation many have proposed inserting adapter layers between existing layers in a neural network houlsby et al 2019 rebuffi et al 2017 lin et al 2020 our method uses a similar bottleneck structure to impose a lowrank constraint on the weight updates the key functional difference is that our learned weights can be merged with the main weights during inference thus not introducing any latency which is not the case for the adapter layers section 3 a comtenporary extension of adapter is compacter mahabadi et al 2021 which essentially parametrizes the adapter layers using kronecker products with some predetermined weight sharing scheme similarly combining lora with other tensor productbased methods could potentially improve its parameter efficiency which we leave to future work more recently many proposed optimizing the input word embeddings in lieu of finetuning akin to a continuous and differentiable generalization of prompt engineering li  liang 2021 lester et al 2021 hambardzumyan et al 2020 liu et al 2021 we include comparisons with li  liang 2021 in our experiment section however this line of works can only scale up by using more special tokens in the prompt which take up available sequence length for task tokens when positional embeddings are learned lowrank structures in deep learning lowrank structure is very common in machine learn ing a lot of machine learning problems have certain intrinsic lowrank structure li et al 2016 cai et al 2010 li et al 2018b grasedyck et al 2013 moreover it is known that for many deep learning tasks especially those with a heavily overparametrized neural network the learned neural network will enjoy lowrank properties after training oymak et al 2019 some prior works even explicitly impose the lowrank constraint when training the original neural network sainath et al 2013 povey et al 2018 zhang et al 2014 jaderberg et al 2014 zhao et al 2016 kho dak et al 2021 denil et al 2014 however to the best of our knowledge none of these works considers lowrank update to a frozen model for adaptation to downstream tasks in theory liter ature it is known that neural networks outperform other classical learning methods including the corresponding finitewidth neural tangent kernels allenzhu et al 2019 li  liang 2018 when the underlying concept class has certain lowrank structure ghorbani et al 2020 allenzhu  li 2019 allenzhu  li 2020a another theoretical result in allenzhu  li 2020b suggests that lowrank adaptations can be useful for adversarial training in sum we believe that our proposed lowrank adaptation update is wellmotivated by the literature 7 understanding the lowrank updates given the empirical advantage of lora we hope to further explain the properties of the lowrank adaptation learned from downstream tasks note that the lowrank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel but also gives better interpretability of how the update weights are correlated with the pretrained weights we focus our study on gpt3 175b where we achieved the largest reduction of trainable parameters up to 10000 without adversely affecting task performances we perform a sequence of empirical studies to answer the following questions 1 given a parameter budget constraint which subset of weight matrices in a pretrained transformer should we adapt 9 to maximize downstream performance 2 is the optimal adaptation matrix w really rank deficient if so what is a good rank to use in practice 3 what is the connection between w and w does w highly correlate with w how large is w comparing to w we believe that our answers to question 2 and 3 shed light on the fundamental principles of using pretrained language models for downstream tasks which is a critical topic in nlp 71 which weight matrices in transformer should we apply lora to given a limited parameter budget which types of weights should we adapt with lora to obtain the best performance on downstream tasks as mentioned in section 42 we only consider weight matrices in the selfattention module we set a parameter budget of 18m roughly 35mb if stored in fp16 on gpt3 175b which corresponds to r  8 if we adapt one type of attention weights or r  4 if we adapt two types for all 96 layers the result is presented in table 5  of trainable parameters  18m weight type wq wk wv wo wq wk wq wv wq wk wv wo rank r 8 8 8 8 4 4 2 wikisql 05 704 700 730 732 714 737 737 multinli 01 910 908 910 913 913 913 917 table 5 validation accuracy on wikisql and multinli after applying lora to different types of attention weights in gpt3 given the same number of trainable parameters adapting both wq and wv gives the best performance overall we find the standard deviation across random seeds to be consistent for a given dataset which we report in the first column note that putting all the parameters in wq or wk results in significantly lower performance while adapting both wq and wv yields the best result this suggests that even a rank of four captures enough information in w such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank 72 what is the optimal rank r for lora we turn our attention to the effect of rank r on model performance we adapt wq wv wq wk wv wc and just wq for a comparison weight type r  1 r  2 r  4 r  8 r  64 wikisql05 wq 688 696 705 704 700 wq wv 734 733 737 738 735 wq wk wv wo 741 737 740 740 739 multinli 01 wq 907 909 911 907 907 wq wv 913 914 913 916 914 wq wk wv wo 912 917 917 915 914 table 6 validation accuracy on wikisql and multinli with different rank r to our surprise a rank as small as one suffices for adapting both wq and wv on these datasets while training wq alone needs a larger r we conduct a similar experiment on gpt2 in section h2 table 6 shows that surprisingly lora already performs competitively with a very small r more so for wq wv than just wq this suggests the update matrix w could have a very small intrinsic rank6 to further support this finding we check the overlap of the subspaces learned by different choices of r and by different random seeds we argue that increasing r does not cover a more meaningful subspace which suggests that a lowrank adaptation matrix is sufficient 6however we do not expect a small r to work for every task or dataset consider the following thought experiment if the downstream task were in a different language than the one used for pretraining retraining the entire model similar to lora with r  dmodel could certainly outperform lora with a small r 10 subspace similarity between different r given ar8 and ar64 which are the learned adapta tion matrices with rank r  8 and 64 using the same pretrained model we perform singular value decomposition and obtain the rightsingular unitary matrices uar8 and uar64  7 we hope to an swer how much of the subspace spanned by the top i singular vectors in uar8 for 1  i  8 is contained in the subspace spanned by top j singular vectors of uar64 for 1  j  64 we mea sure this quantity with a normalized subspace similarity based on the grassmann distance see ap pendix g for a more formal discussion ar8 ar64 i j  u i ar8 u j ar64 2 f mini j  0 1 4 where u i ar8 represents the columns of uar8 corresponding to the topi singular vectors  has a range of 0 1 where 1 represents a complete overlap of subspaces and 0 a complete separation see figure 3 for how  changes as we vary i and j we only look at the 48th layer out of 96 due to space constraint but the conclusion holds for other layers as well as shown in section h1 00 02 04 06 08 10 1 6 12 18 23 29 35 40 46 52 58 j 1 2 3 4 5 6 7 8 i wq 1 6 12 18 23 29 35 40 46 52 58 j wv 1 2 3 4 5 6 7 8 j wq 1 2 3 4 5 6 7 8 j wv ar  64 ar  8 i j figure 3 subspace similarity between column vectors of ar8 and ar64 for both wq and wv the third and the fourth figures zoom in on the lowerleft triangle in the first two figures the top directions in r  8 are included in r  64 and vice versa we make an important observation from figure 3 directions corresponding to the top singular vector overlap significantly between ar8 and ar64 while others do not specifically wv resp wq of ar8 and wv resp wq of ar64 share a subspace of dimension 1 with normalized similarity  05 providing an explanation of why r  1 performs quite well in our downstream tasks for gpt3 since both ar8 and ar64 are learned using the same pretrained model figure 3 indicates that the top singularvector directions of ar8 and ar64 are the most useful while other directions potentially contain mostly random noises accumulated during training hence the adaptation matrix can indeed have a very low rank subspace similarity between different random seeds we further confirm this by plotting the normalized subspace similarity between two randomly seeded runs with r  64 shown in figure 4 wq appears to have a higher intrinsic rank than wv since more common singular value direc tions are learned by both runs for wq which is in line with our empirical observation in table 6 as a comparison we also plot two random gaussian matrices which do not share any common singular value directions with each other 73 how does the adaptation matrix w compare to w  we further investigate the relationship between w and w in particular does w highly correlate with w or mathematically is w mostly contained in the top singular directions of w also 7note that a similar analysis can be carried out with b and the leftsingular unitary matrices  we stick with a for our experiments 11 00 01 02 03 04 05 1 5 10 15 20 25 30 34 39 44 49 54 59 j 1 8 16 24 32 40 48 56 i wq 1 5 10 15 20 25 30 34 39 44 49 54 59 j ar  64 a 0 r  64 i j wv 1 5 10 15 20 25 30 34 39 44 49 54 59 j random gaussian figure 4 left and middle normalized subspace similarity between the column vectors of ar64 from two random seeds for both wq and wv in the 48th layer right the same heatmap between the column vectors of two random gaussian matrices see section h1 for other layers how large is w comparing to its corresponding directions in w this can shed light on the underlying mechanism for adapting pretrained language models to answer these questions we project w onto the rdimensional subspace of w by comput ing u w v  with uv being the leftright singularvector matrix of w then we com pare the frobenius norm between ku w v kf and kwkf  as a comparison we also compute ku w v kf by replacing u v with the top r singular vectors of w or a random matrix r  4 r  64 wq wq random wq wq random u wqv f  032 2167 002 190 3771 033 wqf  6195 wqf  691 wqf  357 table 7 the frobenius norm of u wqv  where u and v are the leftright top r singular vector directions of either 1 wq 2 wq or 3 a random matrix the weight matrices are taken from the 48th layer of gpt3 we draw several conclusions from table 7 first w has a stronger correlation with w compared to a random matrix indicating that w amplifies some features that are already in w second instead of repeating the top singular directions of w w only amplifies directions that are not emphasized in w third the amplification factor is rather huge 215  691032 for r  4 see section h4 for why r  64 has a smaller amplification factor we also provide a visualization in section h3 for how the correlation changes as we include more top singular directions from wq this suggests that the lowrank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pretraining model 8 conclusion and future work finetuning enormous language models is prohibitively expensive in terms of the hardware required and the storageswitching cost for hosting independent instances for different tasks we propose lora an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality importantly it allows for quick taskswitching when deployed as a service by sharing the vast majority of the model parameters while we focused on transformer language models the proposed principles are generally applicable to any neural networks with dense layers there are many directions for future works 1 lora can be combined with other efficient adapta tion methods potentially providing orthogonal improvement 2 the mechanism behind finetuning or lora is far from clear  how are features learned during pretraining transformed to do well on downstream tasks we believe that lora makes it more tractable to answer this than full fine 12 tuning 3 we mostly depend on heuristics to select the weight matrices to apply lora to are there more principled ways to do it 4 finally the rankdeficiency of w suggests that w could be rankdeficient as well which can also be a source of inspiration for future works', 'abstract we introduce llama a collection of founda tion language models ranging from 7b to 65b parameters we train our models on trillions of tokens and show that it is possible to train stateoftheart models using publicly avail able datasets exclusively without resorting to proprietary and inaccessible datasets in particular llama13b outperforms gpt3 175b on most benchmarks and llama 65b is competitive with the best models chinchilla70b and palm540b we release all our models to the research community1  1 introduction large languages models llms trained on mas sive corpora of texts have shown their ability to per form new tasks from textual instructions or from a few examples brown et al 2020 these fewshot properties first appeared when scaling models to a sufficient size kaplan et al 2020 resulting in a line of work that focuses on further scaling these models chowdhery et al 2022 rae et al 2021 these efforts are based on the assumption that more parameters will lead to better performance however recent work from hoffmann et al 2022 shows that for a given compute budget the best performances are not achieved by the largest mod els but by smaller models trained on more data the objective of the scaling laws from hoff mann et al 2022 is to determine how to best scale the dataset and model sizes for a particular training compute budget however this objective disregards the inference budget which becomes critical when serving a language model at scale in this context given a target level of performance the preferred model is not the fastest to train but the fastest at inference and although it may be cheaper to train a large model to reach a certain level of  equal contribution correspondence htouvron thibautlavgizacardegraveglamplemetacom 1 httpsgithubcomfacebookresearchllama performance a smaller one trained longer will ultimately be cheaper at inference for instance although hoffmann et al 2022 recommends training a 10b model on 200b tokens we find that the performance of a 7b model continues to improve even after 1t tokens the focus of this work is to train a series of language models that achieve the best possible per formance at various inference budgets by training on more tokens than what is typically used the resulting models called llama ranges from 7b to 65b parameters with competitive performance compared to the best existing llms for instance llama13b outperforms gpt3 on most bench marks despite being 10 smaller we believe that this model will help democratize the access and study of llms since it can be run on a single gpu at the higherend of the scale our 65bparameter model is also competitive with the best large lan guage models such as chinchilla or palm540b unlike chinchilla palm or gpt3 we only use publicly available data making our work com patible with opensourcing while most existing models rely on data which is either not publicly available or undocumented eg books  2tb or social media conversations there exist some exceptions notably opt zhang et al 2022 gptneox black et al 2022 bloom scao et al 2022 and glm zeng et al 2022 but none that are competitive with palm62b or chinchilla in the rest of this paper we present an overview of the modifications we made to the transformer architecture vaswani et al 2017 as well as our training method we then report the performance of our models and compare with others llms on a set of standard benchmarks finally we expose some of the biases and toxicity encoded in our models using some of the most recent benchmarks from the responsible ai community arxiv230213971v1 cscl 27 feb 2023 2 approach our training approach is similar to the methods described in previous work brown et al 2020 chowdhery et al 2022 and is inspired by the chinchilla scaling laws hoffmann et al 2022 we train large transformers on a large quantity of textual data using a standard optimizer 21 pretraining data our training dataset is a mixture of several sources reported in table 1 that cover a diverse set of do mains for the most part we reuse data sources that have been leveraged to train other llms with the restriction of only using data that is publicly available and compatible with open sourcing this leads to the following mixture of data and the per centage they represent in the training set english commoncrawl 67 we preprocess five commoncrawl dumps ranging from 2017 to 2020 with the ccnet pipeline wenzek et al 2020 this process deduplicates the data at the line level performs language identification with a fasttext linear classifier to remove nonenglish pages and filters low quality content with an n gram language model in addition we trained a linear model to classify pages used as references in wikipedia vs randomly sampled pages and discarded pages not classified as references c4 15 during exploratory experiments we observed that using diverse preprocessed com moncrawl datasets improves performance we thus included the publicly available c4 dataset raffel et al 2020 in our data the preprocessing of c4 also contains deduplication and language identifi cation steps the main difference with ccnet is the quality filtering which mostly relies on heuris tics such as presence of punctuation marks or the number of words and sentences in a webpage github 45 we use the public github dataset available on google bigquery we only kept projects that are distributed under the apache bsd and mit licenses additionally we filtered low quality files with heuristics based on the line length or proportion of alphanumeric characters and removed boilerplate such as headers with reg ular expressions finally we deduplicate the result ing dataset at the file level with exact matches wikipedia 45 we add wikipedia dumps from the juneaugust 2022 period covering 20 dataset sampling prop epochs disk size commoncrawl 670 110 33 tb c4 150 106 783 gb github 45 064 328 gb wikipedia 45 245 83 gb books 45 223 85 gb arxiv 25 106 92 gb stackexchange 20 103 78 gb table 1 pretraining data data mixtures used for pre training for each subset we list the sampling propor tion number of epochs performed on the subset when training on 14t tokens and disk size the pretraining runs on 1t tokens have the same sampling proportion languages which use either the latin or cyrillic scripts bg ca cs da de en es fr hr hu it nl pl pt ro ru sl sr sv uk we process the data to remove hyperlinks comments and other formatting boilerplate gutenberg and books3 45 we include two book corpora in our training dataset the guten berg project which contains books that are in the public domain and the books3 section of thep ile gao et al 2020 a publicly available dataset for training large language models we perform deduplication at the book level removing books with more than 90 content overlap arxiv 25 we process arxiv latex files to add scientific data to our dataset following lewkowycz et al 2022 we removed everything before the first section as well as the bibliography we also removed the comments from the tex files and inlineexpanded definitions and macros written by users to increase consistency across papers stack exchange 2 we include a dump of stack exchange a website of high quality ques tions and answers that covers a diverse set of do mains ranging from computer science to chemistry we kept the data from the 28 largest websites re moved the html tags from text and sorted the answers by score from highest to lowest tokenizer we tokenize the data with the byte pair encoding bpe algorithm sennrich et al 2015 using the implementation from sentence piece kudo and richardson 2018 notably we split all numbers into individual digits and fallback to bytes to decompose unknown utf8 characters params dimension n heads n layers learning rate batch size n tokens 67b 4096 32 32 30e 4 4m 10t 130b 5120 40 40 30e 4 4m 10t 325b 6656 52 60 15e 4 4m 14t 652b 8192 64 80 15e 4 4m 14t table 2 model sizes architectures and optimization hyperparameters overall our entire training dataset contains roughly 14t tokens after tokenization for most of our training data each token is used only once dur ing training with the exception of the wikipedia and books domains over which we perform ap proximately two epochs 22 architecture following recent work on large language models our network is based on the transformer architec ture vaswani et al 2017 we leverage various improvements that were subsequently proposed and used in different models such as palm here are the main difference with the original architec ture and where we were found the inspiration for this change in bracket prenormalization gpt3 to improve the training stability we normalize the input of each transformer sublayer instead of normalizing the output we use the rmsnorm normalizing func tion introduced by zhang and sennrich 2019 swiglu activation function palm we re place the relu nonlinearity by the swiglu ac tivation function introduced by shazeer 2020 to improve the performance we use a dimension of 2 3 4d instead of 4d as in palm rotary embeddings gptneo we remove the absolute positional embeddings and instead add rotary positional embeddings rope introduced by su et al 2021 at each layer of the network the details of the hyperparameters for our dif ferent models are given in table 2 23 optimizer our models are trained using the adamw opti mizer loshchilov and hutter 2017 with the fol lowing hyperparameters 1  09 2  095 we use a cosine learning rate schedule such that the final learning rate is equal to 10 of the maxi mal learning rate we use a weight decay of 01 and gradient clipping of 10 we use 2 000 warmup 0 200 400 600 800 1000 1200 1400 billion of tokens 15 16 17 18 19 20 21 22 training loss llama 7b llama 13b llama 33b llama 65b figure 1 training loss over train tokens for the 7b 13b 33b and 65 models llama33b and llama 65b were trained on 14t tokens the smaller models were trained on 10t tokens all models are trained with a batch size of 4m tokens steps and vary the learning rate and batch size with the size of the model see table 2 for details 24 efficient implementation we make several optimizations to improve the train ing speed of our models first we use an efficient implementation of the causal multihead attention to reduce memory usage and runtime this imple mentation available in the xformers library2 is inspired by rabe and staats 2021 and uses the backward from dao et al 2022 this is achieved by not storing the attention weights and not com puting the keyquery scores that are masked due to the causal nature of the language modeling task to further improve training efficiency we re duced the amount of activations that are recom puted during the backward pass with checkpoint ing more precisely we save the activations that are expensive to compute such as the outputs of linear layers this is achieved by manually imple menting the backward function for the transformer layers instead of relying on the pytorch autograd to fully benefit from this optimization we need to 2 httpsgithubcomfacebookresearchxformers boolq piqa siqa hellaswag winogrande arce arcc obqa gpt3 175b 605 810  789 702 688 514 576 gopher 280b 793 818 506 792 701    chinchilla 70b 837 818 513 808 749    palm 62b 848 805  797 770 752 525 504 palmcont 62b 839 814  806 770    palm 540b 880 823  834 811 766 530 534 llama 7b 765 798 489 761 701 728 476 572 13b 781 801 504 792 730 748 527 564 33b 831 823 504 828 760 800 578 586 65b 853 828 523 842 770 789 560 602 table 3 zeroshot performance on common sense reasoning tasks reduce the memory usage of the model by using model and sequence parallelism as described by korthikanti et al 2022 moreover we also over lap the computation of activations and the commu nication between gpus over the network due to all_reduce operations as much as possible when training a 65bparameter model our code processes around 380 tokenssecgpu on 2048 a100 gpu with 80gb of ram this means that training over our dataset containing 14t tokens takes approximately 21 days 3 main results following previous work brown et al 2020 we consider zeroshot and fewshot tasks and report results on a total of 20 benchmarks  zeroshot we provide a textual description of the task and a test example the model either provides an answer using openended generation or ranks the proposed answers  fewshot we provide a few examples of the task between 1 and 64 and a test example the model takes this text as input and gener ates the answer or ranks different options we compare llama with other foundation mod els namely the nonpublicly available language models gpt3 brown et al 2020 gopher rae et al 2021 chinchilla hoffmann et al 2022 and palm chowdhery et al 2022 as well as the opensourced opt models zhang et al 2022 gptj wang and komatsuzaki 2021 and gpt neo black et al 2022 in section 4 we also briefly compare llama with instructiontuned models such as optiml iyer et al 2022 and flanpalm chung et al 2022 we evaluate llama on freeform generation tasks and multiple choice tasks in the multiple choice tasks the objective is to select the most appropriate completion among a set of given op tions based on a provided context we select the completion with the highest likelihood given the provided context we follow gao et al 2021 and use the likelihood normalized by the number of characters in the completion except for certain datasets openbookqa boolq for which we fol low brown et al 2020 and select a completion based on the likelihood normalized by the likeli hood of the completion given answer as context pcompletioncontextpcompletionanswer 0shot 1shot 5shot 64shot gpt3 175b 146 230  299 gopher 280b 101  245 282 chinchilla 70b 166  315 355 palm 8b 84 106  146 62b 181 265  276 540b 212 293  396 llama 7b 168 187 220 261 13b 201 234 281 319 33b 249 283 329 360 65b 238 310 350 399 table 4 naturalquestions exact match performance 31 common sense reasoning we consider eight standard common sense rea soning benchmarks boolq clark et al 2019 piqa bisk et al 2020 siqa sap et al 2019 hellaswag zellers et al 2019 winogrande sak aguchi et al 2021 arc easy and challenge clark et al 2018 and openbookqa mihaylov et al 2018 these datasets include cloze and winograd style tasks as well as multiple choice question an swering we evaluate in the zeroshot setting as done in the language modeling community in table 3 we compare with existing models of various sizes and report numbers from the cor responding papers first llama65b outper forms chinchilla70b on all reported benchmarks but boolq similarly this model surpasses palm 540b everywhere but on boolq and winogrande llama13b model also outperforms gpt3 on most benchmarks despite being 10 smaller 32 closedbook question answering we compare llama to existing large language models on two closedbook question answering benchmarks natural questions kwiatkowski et al 2019 and triviaqa joshi et al 2017 for both benchmarks we report exact match perfor mance in a closed book setting ie where the mod els do not have access to documents that contain evidence to answer the question in table 4 we report performance on naturalquestions and in ta ble 5 we report on triviaqa on both benchmarks llama65b achieve stateofthearts performance in the zeroshot and fewshot settings more im portantly the llama13b is also competitive on these benchmarks with gpt3 and chinchilla de spite being 510 smaller this model runs on a single v100 gpu during inference 0shot 1shot 5shot 64shot gopher 280b 435  570 572 chinchilla 70b 554  641 646 llama 7b 500 534 563 576 13b 566 605 631 640 33b 651 679 699 704 65b 682 716 726 730 table 5 triviaqa zeroshot and fewshot exact match performance on the filtered dev set 33 reading comprehension we evaluate our models on the race reading com prehension benchmark lai et al 2017 this dataset was collected from english reading com prehension exams designed for middle and high racemiddle racehigh gpt3 175b 584 455 palm 8b 579 423 62b 643 475 540b 681 491 llama 7b 611 469 13b 616 472 33b 641 483 65b 679 516 table 6 reading comprehension zeroshot accu racy school chinese students we follow the evaluation setup from brown et al 2020 and report results in table 6 on these benchmarks llama65b is competitive with palm540b and llama13b outperforms gpt3 by a few percents 34 mathematical reasoning we evaluate our models on two mathematical rea soning benchmarks math hendrycks et al 2021 and gsm8k cobbe et al 2021 math is a dataset of 12k middle school and high school mathematics problems written in latex gsm8k is a set of middle school mathematical problems in table 7 we compare with palm and min erva lewkowycz et al 2022 minerva is a series of palm models finetuned on 385b tokens ex tracted from arxiv and math web pages while neither palm or llama are finetuned on mathe matical data the numbers for palm and minerva are taken from lewkowycz et al 2022 and we compare with and without maj1k maj1k de notes evaluations where we generate k samples for each problem and perform a majority voting wang et al 2022 on gsm8k we observe that llama 65b outperforms minerva62b although it has not been finetuned on mathematical data 35 code generation we evaluate the ability of our models to write code from a natural language description on two benchmarks humaneval chen et al 2021 and mbpp austin et al 2021 for both tasks the model receives a description of the program in a few sentences as well as a few inputoutput ex amples in humaneval it also receives a function signature and the prompt is formatted as natural code with the textual description and tests in a math maj1k gsm8k maj1k palm 8b 15  41  62b 44  330  540b 88  565  minerva 8b 141 254 162 284 62b 276 434 524 685 540b 336 503 685 785 llama 7b 29 69 110 181 13b 39 88 178 293 33b 71 152 356 531 65b 106 205 509 697 table 7 model performance on quantitative reason ing datasets for majority voting we use the same setup as minerva with k  256 samples for math and k  100 for gsm8k minerva 540b uses k  64 for math and and k  40 for gsm8k llama65b outperforms minerva 62b on gsm8k although it has not been finetuned on mathematical data docstring the model needs to generate a python program that fits the description and satisfies the test cases in table 8 we compare the pass1 scores of our models with existing language mod els that have not been finetuned on code namely palm and lamda thoppilan et al 2022 palm and llama were trained on datasets that contain a similar number of code tokens as show in table 8 for a similar number of parameters llama outperforms other gen eral models such as lamda and palm which are not trained or finetuned specifically for code llama with 13b parameters and more outper forms lamda 137b on both humaneval and mbpp llama 65b also outperforms palm 62b even when it is trained longer the pass1 results reported in this table were obtained by sampling with temperature 01 the pass100 and pass80 metrics were obtained with temperature 08 we use the same method as chen et al 2021 to obtain unbiased estimates of the passk it is possible to improve the performance on code by finetuning on codespecific tokens for instance palmcoder chowdhery et al 2022 increases the pass1 score of palm on humaneval from 262 for palm to 36 other models trained specifically for code also perform better than gen eral models on these tasks chen et al 2021 ni jkamp et al 2022 fried et al 2022 finetuning on code tokens is beyond the scope of this paper params humaneval mbpp pass 1 100 1 80 lamda 137b 140 473 148 624 palm 8b 36 187 50 357 palm 62b 159 463 214 632 palmcont 62b 237  312  palm 540b 262 762 368 750 llama 7b 105 365 177 562 13b 158 525 220 640 33b 217 707 302 734 65b 237 793 377 768 table 8 model performance for code generation we report the pass score on humaneval and mbpp humaneval generations are done in zeroshot and mbbp with 3shot prompts similar to austin et al 2021 the values marked with  are read from figures in chowdhery et al 2022 36 massive multitask language understanding the massive multitask language understanding benchmark or mmlu introduced by hendrycks et al 2020 consists of multiple choice questions covering various domains of knowledge includ ing humanities stem and social sciences we evaluate our models in the 5shot setting using the examples provided by the benchmark and report results in table 9 on this benchmark we observe that the llama65b is behind both chinchilla 70b and palm540b by a few percent in average and across most domains a potential explanation is that we have used a limited amount of books and academic papers in our pretraining data ie arxiv gutenberg and books3 that sums up to only 177gb while these models were trained on up to 2tb of books this large quantity of books used by gopher chinchilla and palm may also explain why gopher outperforms gpt3 on this benchmark while it is comparable on other benchmarks 37 evolution of performance during training during training we tracked the performance of our models on a few question answering and common sense benchmarks and report them in figure 2 on most benchmarks the performance improves steadily and correlates with the training perplexity of the model see figure 1 the exceptions are siqa and winogrande most notably on siqa we observe a lot of variance in performance humanities stem social sciences other average gptneox 20b 298 349 337 377 336 gpt3 175b 408 367 504 488 439 gopher 280b 562 474 719 661 600 chinchilla 70b 636 549 793 739 675 palm 8b 256 238 241 278 254 62b 595 419 627 558 537 540b 770 556 810 696 693 llama 7b 340 305 383 381 351 13b 450 358 538 533 469 33b 558 460 667 634 578 65b 618 517 729 674 634 table 9 massive multitask language understanding mmlu fiveshot accuracy that may indicate that this benchmark is not reliable on winogrande the performance does not correlate as well with training perplexity the llama33b and llama65b have similar performance during the training 4 instruction finetuning in this section we show that briefly finetuning on instructions data rapidly leads to improvements on mmlu although the nonfinetuned version of llama65b is already able to follow basic in structions we observe that a very small amount of finetuning improves the performance on mmlu and further improves the ability of the model to follow instructions since this is not the focus of this paper we only conducted a single experiment following the same protocol as chung et al 2022 to train an instruct model llamai opt 30b 261 glm 120b 448 palm 62b 551 palmcont 62b 628 chinchilla 70b 675 llama 65b 634 optimlmax 30b 432 flant5xxl 11b 551 flanpalm 62b 596 flanpalmcont 62b 661 llamai 65b 689 table 10 instruction finetuning  mmlu 5shot comparison of models of moderate size with and with out instruction finetuning on mmlu in table 10 we report the results of our instruct model llamai on mmlu and compare with ex isting instruction finetuned models of moderate sizes namely optiml iyer et al 2022 and the flanpalm series chung et al 2022 all the re ported numbers are from the corresponding papers despite the simplicity of the instruction finetuning approach used here we reach 689 on mmlu llamai 65b outperforms on mmlu existing instruction finetuned models of moderate sizes but are still far from the stateoftheart that is 774 for gpt codedavinci002 on mmlu numbers taken from iyer et al 2022 the details of the performance on mmlu on the 57 tasks can be found in table 16 of the appendix 5 bias toxicity and misinformation large language models have been showed to re produce and amplify biases that are existing in the training data sheng et al 2019 kurita et al 2019 and to generate toxic or offensive con tent gehman et al 2020 as our training dataset contains a large proportion of data from the web we believe that it is crucial to determine the po tential for our models to generate such content to understand the potential harm of llama65b we evaluate on different benchmarks that measure toxic content production and stereotypes detection while we have selected some of the standard bench marks that are used by the language model com munity to indicate some of the issues with these models these evaluations are not sufficient to fully understand the risks associated with these models 0 250 500 750 1000 1250 1500 20 30 40 50 60 70 accuracy triviaqa 0 250 500 750 1000 1250 1500 50 55 60 65 70 75 80 85 hellaswag 0 250 500 750 1000 1250 1500 0 5 10 15 20 25 30 35 naturalquestions 0 250 500 750 1000 1250 1500 billion of tokens 40 42 44 46 48 50 52 accuracy siqa 0 250 500 750 1000 1250 1500 billion of tokens 50 55 60 65 70 75 80 winogrande 0 250 500 750 1000 1250 1500 billion of tokens 650 675 700 725 750 775 800 825 piqa llama 7b llama 13b llama 33b llama 65b chinchilla figure 2 evolution of performance on question answering and common sense reasoning during training 51 realtoxicityprompts language models can generate toxic language eg insults hate speech or threats there is a very large range of toxic content that a model can generate making a thorough evaluation challenging several recent work zhang et al 2022 hoffmann et al 2022 have considered the realtoxicityprompts benchmark gehman et al 2020 as an indicator of how toxic is their model realtoxicityprompts consists of about 100k prompts that the model must complete then a toxicity score is automatically evaluated by making a request to perspectiveapi 3  we do not have control over the pipeline used by the thirdparty perspectiveapi making comparison with previous models difficult for each of the 100k prompts we greedily gen erate with our models and measure their toxic ity score the score per prompt ranges from 0 nontoxic to 1 toxic in table 11 we report our averaged score on basic and respectful prompt cat egories of realtoxicityprompts these scores are comparable with what we observe in the litera ture eg 0087 for chinchilla but the method ologies differ between these work and ours in terms of sampling strategy number of prompts and time of api we observe that toxicity increases 3 httpsperspectiveapicom basic respectful llama 7b 0106 0081 13b 0104 0095 33b 0107 0087 65b 0128 0141 table 11 realtoxicityprompts we run a greedy de coder on the 100k prompts from this benchmark the respectful versions are prompts starting with com plete the following sentence in a polite respectful and unbiased manner and basic is without it scores were obtained using the perplexityapi with higher score indicating more toxic generations with the size of the model especially for respect ful prompts this was also observed in previous work zhang et al 2022 with the notable excep tion of hoffmann et al 2022 where they do not see a difference between chinchilla and gopher despite different sizes this could be explained by the fact that the larger model gopher has worse performance than chinchilla suggesting that the relation between toxicity and model size may only apply within a model family llama gpt3 opt gender 706 626 657 religion 790 733 686 racecolor 570 647 686 sexual orientation 810 762 786 age 701 644 678 nationality 642 616 629 disability 667 767 767 physical appearance 778 746 762 socioeconomic status 715 738 762 average 666 672 695 table 12 crowspairs we compare the level of bi ases contained in llama65b with opt175b and gpt3175b higher score indicates higher bias 52 crowspairs we evaluate the biases in our model on the crows pairs nangia et al 2020 this dataset allows to measure biases in 9 categories gender religion racecolor sexual orientation age nationality dis ability physical appearance and socioeconomic sta tus each example is composed of a stereotype and an antistereotype we measure the model prefer ence for the stereotypical sentence using the per plexity of both sentences in a zeroshot setting higher scores thus indicate higher bias we com pare with gpt3 and opt175b in table 12 llama compares slightly favorably to both models on average our model is particularly bi ased in the religion category 10 compared to opt175b followed by age and gender we ex pect these biases to come from commoncrawl de spite multiple filtering steps 53 winogender to further investigate the biases of our model on the gender category we look at the winogender benchmark rudinger et al 2018 a coreference resolution dataset winogender is made of wino grad schema and biases are evaluated by determin ing if a model coreference resolution performance is impacted by the gender of the pronoun more precisely each sentence has three men tions an occupation a participant and a pronoun where the pronoun is coreferencing either the occupation or participant we prompt the model to determine the coreference relation and measure if it does so correctly according to the context of the sentence the goal is to reveal if societal biases associated with occupations have been captured by the model for example a sentence in the winogender dataset is the nurse notified the patient that his shift would be ending in an hour which is followed by his refers to we then compare the perplexity of the continuations the nurse and the patient to per form coreference resolution with the model we evaluate the performance when using 3 pronouns herhershe hishimhe and theirthemsome one the different choices corresponding to the grammatical function of the pronoun in table 13 we report the coreference scores for the three different pronouns contained in the dataset we observe that our model is significantly better at performing coreference resolution for the theirthemsomeone pronouns than for the herhershe and hishimhe pronouns a simi lar observation was made in previous work rae et al 2021 hoffmann et al 2022 and is likely indicative of gender bias indeed in the case of the herhershe and hishimhe pronouns the model is probably using the majority gender of the occu pation to perform coreference resolution instead of using the evidence of the sentence to further investigate this hypothesis we look at the set of gotcha cases for the herhershe and hishimhe pronouns in the winogender dataset theses cases correspond to sentences in which the pronoun does not match the majority gender of the occupation and the occupation is the correct answer in table 13 we observe that our model llama65b makes more errors on the gotcha examples clearly showing that it capture societal biases related to gender and occupation the drop of performance exists for herhershe and hishimhe pronouns which is indicative of biases regardless of gender 54 truthfulqa truthfulqa lin et al 2021 aims to measure the truthfulness of a model ie its ability to identify when a claim is true lin et al 2021 consider the definition of true in the sense of literal truth about the real world and not claims that are only true in the context of a belief system or tradition this benchmark can evaluate the risks of a model to generate misinformation or false claims the questions are written in diverse style cover 38 cat egories and are designed to be adversarial 7b 13b 33b 65b all 660 647 690 775 herhershe 650 667 667 788 hishimhe 608 625 621 721 theirthemsomeone 721 650 783 817 herhershe gotcha 642 658 617 750 hishimhe gotcha 550 558 558 633 table 13 winogender coreference resolution ac curacy for the llama models for different pronouns herhershe and hishimhe we observe that our models obtain better performance on theirthemsome one pronouns than on herhershe and hishimhe which is likely indicative of biases truthful truthfulinf gpt3 13b 031 019 6b 022 019 175b 028 025 llama 7b 033 029 13b 047 041 33b 052 048 65b 057 053 table 14 truthfulqa we report the fraction of truth ful and truthfulinformative answers as scored by spe cially trained models via the openai api we follow the qa prompt style used in ouyang et al 2022 and report the performance of gpt3 from the same paper in table 14 we report the performance of our models on both questions to measure truthful mod els and the intersection of truthful and informative compared to gpt3 our model scores higher in both categories but the rate of correct answers is still low showing that our model is likely to hallu cinate incorrect answers 6 carbon footprint the training of our models have consumed a mas sive quantity of energy responsible for the emis sion of carbon dioxide we follow the recent liter ature on the subject and breakdown both the total energy consumption and the resulting carbon foot print in table 15 we follow a formula for wu et al 2022 to estimate the watthour wh needed to train a model as well as the tons of carbon emis sions tco2eq for the wh we use the formula wh  gpuhgpu power consumptionpue where we set the power usage effectiveness pue at 11 the resulting carbon emission depends on the location of the data center used to train the net work for instance bloom uses a grid that emits 0057 kg co2eqkwh leading to 27 tco2eq and opt a grid that emits 0231 kg co2eqkwh lead ing to 82 tco2eq in this study we are interested in comparing the cost in carbon emission of training of these models if they were trained in the same data center hence we do not take the location of data center in consideration and use instead the us national average carbon intensity factor of 0385 kg co2eqkwh this leads to the following formula for the tons of carbon emissions tco2eq  mwh  0385 we apply the same formula to opt and bloom for fair comparison for opt we assume training required 34 days on 992 a10080b see their logs4  finally we estimate that we used 2048 a10080gb for a period of approximately 5 months to develop our models this means that developing these mod els would have cost around 2638 mwh under our assumptions and a total emission of 1015 tco2eq we hope that releasing these models will help to reduce future carbon emission since the training is already done and some of the models are relatively small and can be run on a single gpu 7 related work language models are probability distributions over sequences of words tokens or charac ters shannon 1948 1951 this task often framed as next token prediction has long been considered a core problem in natural language processing bahl et al 1983 brown et al 1990 because turing 1950 proposed to measure machine intelligence by using language through the imitation game language modeling has been proposed as a bench mark to measure progress toward artificial intelli gence mahoney 1999 architecture traditionally language models were based on ngram count statistics bahl et al 1983 and various smoothing techniques were proposed to improve the estimation of rare events katz 1987 kneser and ney 1995 in the past two decades neural networks have been suc cessfully applied to the language modelling task 4 httpsgithubcomfacebookresearchmetaseq treemainprojectsoptchronicles gpu type gpu power gpuhours total power carbon emitted consumption consumption tco2eq opt175b a10080gb 400w 809472 356 mwh 137 bloom175b a10080gb 400w 1082880 475 mwh 183 llama7b a10080gb 400w 82432 36 mwh 14 llama13b a10080gb 400w 135168 59 mwh 23 llama33b a10080gb 400w 530432 233 mwh 90 llama65b a10080gb 400w 1022362 449 mwh 173 table 15 carbon footprint of training different models in the same data center we follow wu et al 2022 to compute carbon emission of training opt bloom and our models in the same data center for the power consumption of a a10080gb we take the thermal design power for nvlink systems that is 400w we take a pue of 11 and a carbon intensity factor set at the national us average of 0385 kg co2e per kwh starting from feed forward models bengio et al 2000 recurrent neural networks elman 1990 mikolov et al 2010 and lstms hochreiter and schmidhuber 1997 graves 2013 more recently transformer networks based on selfattention have led to important improvements especially for cap turing long range dependencies vaswani et al 2017 radford et al 2018 dai et al 2019 scaling there is a long history of scaling for language models for both the model and dataset sizes brants et al 2007 showed the benefits of using language models trained on 2 trillion tokens resulting in 300 billion ngrams on the quality of machine translation while this work relied on a simple smoothing technique called stupid backoff heafield et al 2013 later showed how to scale kneserney smoothing to webscale data this allowed to train a 5gram model on 975 billions to kens from commoncrawl resulting in a model with 500 billions ngrams buck et al 2014 chelba et al 2013 introduced the one billion word benchmark a large scale training dataset to measure the progress of language models in the context of neural language models joze fowicz et al 2016 obtained stateoftheart re sults on the billion word benchmark by scaling lstms to 1 billion parameters later scaling transformers lead to improvement on many nlp tasks notable models include bert devlin et al 2018 gpt2 radford et al 2019 megatron lm shoeybi et al 2019 and t5 raffel et al 2020 a significant breakthrough was obtained with gpt3 brown et al 2020 a model with 175 billion parameters this lead to a series of large language models such as jurassic1 lieber et al 2021 megatronturing nlg smith et al 2022 gopher rae et al 2021 chinchilla hoff mann et al 2022 palm chowdhery et al 2022 opt zhang et al 2022 and glm zeng et al 2022 hestness et al 2017 and rosenfeld et al 2019 studied the impact of scaling on the perfor mance of deep learning models showing the exis tence of power laws between the model and dataset sizes and the performance of the system kaplan et al 2020 derived power laws specifically for transformer based language models which were later refined by hoffmann et al 2022 by adapting the learning rate schedule when scaling datasets finally wei et al 2022 studied the effect of scal ing on the abilities of large language models 8 conclusion in this paper we presented a series of language models that are released openly and competitive with stateoftheart foundation models most notably llama13b outperforms gpt3 while being more than 10 smaller and llama65b is competitive with chinchilla70b and palm540b unlike previous studies we show that it is possible to achieve stateoftheart performance by training exclusively on publicly available data without resorting to proprietary datasets we hope that releasing these models to the research community will accelerate the development of large language models and help efforts to improve their robust ness and mitigate known issues such as toxicity and bias additionally we observed like chung et al 2022 that finetuning these models on instructions lead to promising results and we plan to further investigate this in future work finally we plan to release larger models trained on larger pretraining corpora in the future since we have seen a constant improvement in performance as we were scaling', 'abstract we present position interpolation pi that extends the context window sizes of ropebased su et al 2021 pretrained llms such as llama touvron et al 2023 models to up to 32768 with minimal finetuning within 1000 steps while demonstrating strong empirical results on various tasks that require long context including passkey retrieval language modeling and long document summariza tion from llama 7b to 65b meanwhile the extended model by position inter polation preserve quality relatively well on tasks within its original context win dow to achieve this goal position interpolation linearly downscales the input position indices to match the original context window size rather than extrapo lating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the selfattention mechanism our theoretical study shows that the upper bound of interpolation is at least  600 smaller than that of extrapolation further demonstrating its stability models extended via po sition interpolation retain its original architecture and can reuse most preexisting optimization and infrastructure 1 introduction large language models llms typically come with a predefined context window size for exam ple inputs to llama models touvron et al 2023 must be fewer than 2048 tokens this preset context window limit is frequently exceeded in applications such as conducting long conversations summarizing long documents or executing longterm planning for these applications llms with longer context windows are preferred however training an llm from scratch with long context windows requires significant investments this naturally leads to a question can we extend the context window of an existing pretrained llm one straightforward approach is to finetune an existing pretrained transformer with a longer con text window however empirically we found that models trained this way adapt to long context windows very slowly after training for more than 10000 batches the effective context window saw a minimal increase moving from 2048 to 2560 table 4 this suggests that such method is inefficient for extending to substantially longer context windows while certain techniques such as alibi press et al 2022 and lex sun et al 2022 enable length extrapolation of transformers ie train on short context windows and inference on longer ones many existing pretrained llms including llama touvron et al 2023 use positional encodings that have weak extrapolation properties eg rope su et al 2021 therefore the applicability of these techniques for extending the context window sizes of such llms remains limited in this work we introduce position interpolation to enable context window extensions for certain existing pretrained llms including llama the key idea is instead of extrapolation we directly downscale the position indices so that the maximum position index matches the previous context window limit in the pretraining stage see figure 1 for an illustration in other words to accom modate more input tokens we interpolate the position encodings at neighboring integer positions utilizing the fact that position encodings can be applied on noninteger positions as opposed to extrapolating outside the trained positions which may lead to catastrophic values we verify our approach theoretically by showing that the interpolated attention score has a much smaller upper 1 arxiv230615595v2 cscl 28 jun 2023 normal pretrained range pretrained range unseen range position interpolation fx m  fx m2 extrapolation figure 1 an illustration of our position interpolation method consider a llama model pretrained with a 2048 context window length upper left illustrates the normal usage of an llm model input position indices blue dots are within the pretrained range upper right illustrates length extrapolation where models are required to operate unseen positions red dots up to 4096 lower left illustrates position interpolation where we downscale the position indices blue and green dots themselves from 0 4096 to 0 2048 to force them to reside in the pretrained range bound  600 smaller in llama 7b setting than the extrapolated one and is thus much more stable therefore interpolated position encodings are easier for the model to adapt empirically we found that position interpolation is highly effective and efficient requiring only a very short period of finetuning for the model to fully adapt to greatly extended context windows we present experimental results for extending the context window to up to 32768 from the initial 2048 across 7b to 65b llama models using position interpolation our results show that 1 position interpolation can easily enable very long context windows eg 32768 requiring only finetuning for 1000 steps on the pile gao et al 2020 to achieve a good quality the cost of finetuning is negligible compared to the pretraining costs this confirms our hypothesis that it is relatively easy for the models to adapt to interpolated position encodings 2 position interpolation generates strong models that can effectively make use of much ex tended context window we show that models extended by position interpolation enjoy significant perplexity gains from greatly extended context windows for text modeling and we show that the perplexity reduces graceful with the enlargement of context windows we also applied position interpolation in a long text summarization task and demonstrate competitive performances 3 position interpolation preserves model quality relatively well for tasks within its original context window sizes we present a variety of evaluation results for the extended llama models on the original llama benchmark compared with original llama models the extended llama models saw a minor degradation on several standard benchmarks within a 2048 token limit our results highlight the innate ability of transformer models to extrapolate to sequence lengths longer than the ones encountered during training as hypothesized in the seminal work of vaswani et al 2017 we reaffirm this hypothesis and suggest that the previously known weakness of ex trapolating to longer sequences for language modeling press et al 2022 may be due to direct 2 extrapolation of positional encodings and it can be largely mitigated by interpolating position en codings instead concurrent work right before our release we are informed with a concurrent blogpost super hot kaiokendev 2023 that also interpolates positional encoding in rope to extend the context window from 2k to 8k recently open source community picks it up in reddit post 1 and github issues 2  which shows that finetuning with lora hu et al 2021 also seems to work well our paper shows a full finetuning with up to 65b model work well with position interpolation and we also give theoretical explanations why interpolation achieves much more stable results than extrap olation by showing that the upper bound of interplated attention score is much lower than that of extrapolated ones 2 method 21 background rotary position embedding rope transformer models require explicit positional information to be injected typically in the form of positional encodings to represent the order of inputs we consider rotary position embedding rope su et al 2021 which is the position encoding used in the llama model touvron et al 2023 given a position index m  0 c and an embedding vector x  x0 x1     xd1  where d is the dimension of the attention head rope defines a vectorvalued complex function fx m as follows fx m  x0  ix1e im0 x2  ix3e im1     xd2  ixd1e imd21   1 where i   1 is the imaginary unit and j  100002jd using rope the selfattention score am n  refq mfk n  re   d x 21 j0 q2j  iq2j1k2j  ik2j1e imnj    d x 21 j0 q2jk2j  q2j1k2j1 cosm  nj   q2jk2j1  q2j1k2j  sinm  nj   am  n 2 is only dependent on relative position m  n through trigonometric functions here q and k are the query and key vector for a specific attention head at each layer rope is applied on both query and key embeddings for computing attention scores 22 direct extrapolation while the attention score in rope only depends on the relative positions which is what we want its extrapolation performance is not great  in particular when directly extending to larger context windows unseen in the training the perplexity may shoot up to very high numbers ie  103  comparable to untrained models ideally we want to see the model trained on a context window of size l  2048 to still work reasonably well on longer context window but may not have the capability to leverage information that appears beyond l for example to answer a question located at 3000 the model trained on maximal window size of l  2048 cannot leverage evidences provided at location 0 but still can leverage the evidences provided at location 2900 in contrast in reality we see catastrophic behaviors ie question at location 3000 cannot be answered correctly even if the evidences are located at location 2900 what is the reason behind how could this happen if the attention score amn decays as the relative distance m  n increases according to section 343 of su et al 2021 and content from very 1httpswwwredditcomrlocalllamacomments14fgjqja_simple_way_to_ extending_context_to_8k 2httpsgithubcomggerganovllamacppdiscussions1965 3 0 500 1000 1500 2000 positional difference s 3 2 1 0 1 2 3 atte ntio n score as 0 1000 2000 3000 4000 positional difference s 0 2000 4000 6000 8000 effect of extrapolation 30 40 50 60 70 positional difference s 02 01 00 01 02 effect of interpolation figure 2 extrapolation versus interpolation left a fitted attention score function in red in the form of eqn 3 with d  dmodelnhead  409632  128 setting of llama 7b dots are random input points to be fitted and red curve is the fitted score function via least square which is approximately within 1 1 middle while the fitted function seems to be well bounded in 0 l where l  2048 out of this region it may goes beyond 8000 causing catastrophic issues in attention computation note that here we do not cherry pick at all almost every learned curve from a set of randomly generated input points within 0 l has the extrapolation issue right on the other hand interpolation is much more stable curves in between vertical dotted lines ie integer positional difference are smooth and wellbehaved please check appendix c1 for the source code used to generate the figure far distances should not matter that much it turns out that the upper bound derived in section 343 of su et al 2021 may be too loose while it indeed decays with respect to m  n the bound can still be quite large ie the bound can be critically depends on the magnitude of vj  and thus vacuous in fact if we treat all trigonometric functions as basis functions ie j s  e isj  and think about eqn 2 as basis expansion as the following as  re   d x 21 j0 hj e isj   3 where s is the positional span between a query and a key and hj  q2j  iq2j1k2j  ik2j1 are complex coefficients depending on q and k here the definition of hj is exactly the same as the definition of hj in sec 343 in rope su et al 2021 now the the issue becomes clear as shown in fig 2 as can be small in magnitude in the range of 0 2048 but gives huge values out of the region the underlying reason is that the trigonometric family j with sufficiently large d is a universal approximator and can fit any arbitrary functions therefore for as there always exist coefficients hj ie key and query that corresponds to small function values in 0 2048 but much larger in regions beyond 23 proposed approach position interpolation pi in fig 2 thanks to the smoothness of bases functions j interpolation is much more stable and will not lead to wild values therefore instead of extrapolate the attention score in eqn 3 to s  l how about we define an attention score a s  alsl  where l  is the longer context window formally we replace rope f by f  defined as follows f  x m  f x ml l  4 we call this transformation on the position encoding position interpolation in this step we reduce position indices from 0 l  to 0 l to match the original range of indices before computing rope consequently as inputs to rope the maximum relative distance between any two tokens has been reduced from l  to l since we align the ranges of position indices and relative distances before and after extension we mitigate the effect on attention score computation due to context window extensions which can allow the model easier to adapt to further demonstrate this is the case in the following theorem we show that the interpolated attention score is wellbehaved 4 theorem 21 interpolation bound for attention score as  re hpd21 j0 hj e isj i  where j  c 2jd its interpolation value as for s  s1 s2 is bounded as follows as  alinears  d max j hj  s  s1s2  s 8 ln c 5 where alinears is the linear interpolation of two grid point as1 and as2 that are known to behave well enforced by llm pretraining alinears  1  sas1  sas2 s  s  s1 s2  s1 6 please check appendix a for the proof intuitively in llm pretraining we know that the attention score as behaves well on integer grid s1 and s2 therefore for any interpolation s  s1 s2 we have s  s1s2  s  14 note that c  10000 the bound becomes as  alinears  d 32 ln c max j hj   d maxj hj  29473 7 in comparison sec 343 in rope su et al 2021 yields an extrapolation bound ie it works for all positional distance s as  max j hj  hj1 d x 21 k0 ak1s  2 max j hj  d x 21 k0 ak1s 8 where aks  pk1 j0 e isj  while there is no close form for bs  pd21 k0 ak1s numer ically it is at least larger than d and for many positional difference s bs is much larger than d check appendix b for the plot therefore the interpolation bound is at least 2  29473  600 smaller than the extrapolation bound and thus the interpolated attention score is much more stable than extrapolated one notably our method of rescaling of position indices does not introduce extra weight or modify the model architecture in any way this makes it attractive in practical applications since most infrastructure and optimization for the original model can be reused after the extension finetuning we can further finetune the interpolated model using the next token prediction task with interpolated position encodings on the extended context window size using a pretraining cor pus such as the pile gao et al 2020 in the next section we show that our finetuning process only needs tens to hundreds thousands of examples we also find that the result of the finetuning is not sensitive to the choice of examples the reason may be that the model is only adapting to the new context window during the finetuning phase starting from a good initialization as opposed to acquiring new knowledge other ways to reduce interpolationextrapolation bound from the expression of the interpola tion eqn 5 and extrapolation bound eqn 8 a common term is maxj hj  which is the maximal magnitude of querykey products if we enforce a regularization on hj  during llm training it is possible that the catastrophic extrapolation error can be mitigated or even resolved in fact if we apply ridge regression with proper regularization to fit a curve in fig 2 the magnitude of extrapo lated as when s  l can be comparable to that within 0 l to our knowledge we are not aware of existing llm pretraining techniques that leverage this regularization and will leave it for future work 3 experiments we show position interpolation can effectively extend context window up to 32 times of the original size and such extension can be done with only several hundreds of training steps we show the resulting models are strong llms with fully effective long context windows we demonstrate its performance in a number of tasks including language modeling passkey retrieval and long doc ument summarization we also present benchmark results of the extended models on the original llama evaluation benchmarks 5 31 setup model variants we extended the pretrained 7b 13b 33b and 65b llama models touvron et al 2023 to various context window of sizes up to 32768 using either direct finetuning or position interpoloation method except for rescaling the position indices for models extended with position interpolation we did not modify llama model architectures touvron et al 2023 in any ways training procedure we finetune all model variants using the next token prediction objective we use adamw loshchilov  hutter 2019 with 1  09 and 2  095 we use a linear learning rate warmup of 20 steps starting from 10 of the maximum learning rate for 7b and 13b models we set the learning rate to 2105 and for 33b and 65b models we set the learning rate to 105  we set the weight decay to zero for extending 7b 13b and 33b models to the 8192 context window size we use 32 a100 gpus and 64 global batch size for all other cases we use 128 a100 gpus and 128 global batch size we note that the main need of using more gpus is memory limitation during finetuning and it is possible to use fewer gpus in certain cases we train all models using pytorch paszke et al 2019 with fully sharded data parallel zhao et al 2023 and flash attention dao et al 2022 if not specified otherwise for the position interpolation method we finetune the models for 1000 steps for the direct finetuning method we use 10000 steps we primarily finetune using the pile training dataset gao et al 2020 in section 34 we also compared finetuning performance on the redpajama dataset computer 2023 32 long sequence language modeling we evaluate the long sequence language modeling performance of our extended models and base lines on two datasets book corpus pg19 rae et al 2020 and cleaned arxiv math proofpile dataset azerbayev et al 2022 we use the test splits of pg19 rae et al 2020 and proofpile azerbayev et al 2022 for pg19 we use the whole test split consisting of 100 documents for the proofpile dataset we use a random subsample of 128 documents with at least 32768 sentencepiece kudo  richardson 2018 tokens and truncate to the first 32768 tokens for each test document we evaluate perplexity at various context window size by using a sliding window approach following press et al 2022 with stride s  256 in table 1 and table 2 we report the perplexity results for our models and baselines on the datasets from the results we found that models extended with our method enjoy a significantly improved perplexity from longer context window sizes by increasing the context window size from 2048 to 16384 we observed 028 and 05 reductions of perplexity for extending llama 7b models on both datasets 027 and 048 reductions for extending llama 13b models and 014 and 042 reductions for extending llama 33b models for llama 65b models we observed 012 and 03 reductions of perplexity by extending to the 8192 context window size in general we observed a consistent trend of our models achieving better perplexity with longer context windows this indicates our models can effectively make use of the longer context windows to better predict next tokens in language modeling tasks moreover we found this trend extends to 32768 window size without diminishing on the pg19 dataset for llama 7b and 13b models this indicates that our method may enable extension to even longer context windows in contrast we observed that models extended via the direct finetuning method has shown regres sion up to 048 or minor improvement up to 012 on the perplexity at longer context windows this indicates that models extended this way have limited capability of making use of context win dows longer than their pretrained settings we saw a minor degradation of the perplexity on the original context window of 2048 for our ex tended models in some cases for example on the proofpile dataset we saw a degradation ranging from 001 to 005 across all models with extended with position interpolation a small degradation of performance within original evaluation context window is expected since position interpolation forces position encodings in original context window to reside in a much narrower region which 6 may negatively affect the language models performance we present more benchmark results on the original context window size in section 34 in table 3 we report the relationship between perplexity and the number of finetuning steps for llama 7b model extending to 8192 and 16384 context window sizes using position interpolation evaluated on the pg19 dataset we can see without finetuning at step 0 the model can exhibit certain language modeling capability as indicated by  20 perplexity for extending to 8192 context window in contrast the direct extrapolation method leads to  103 perplexity with finetuning we observed that the perplexity improves quickly at 200 steps the models surpassed the original models perplexity on 2048 context window size indicating the models gaining ability of effectively using sequences longer than the pretraining settings for language modeling at 1000 steps we can see the models have improved steadily and achieve a significantly better perplexity model evaluation context window size size context window method 2048 4096 8192 16384 32768 7b 2048 none 720  103  103  103  103 7b 8192 ft 721 734 769   7b 8192 pi 713 696 695   7b 16384 pi 711 693 682 683  7b 32768 pi 723 704 691 680 677 13b 2048 none 659     13b 8192 ft 656 657 669   13b 8192 pi 655 642 642   13b 16384 pi 656 642 631 632  13b 32768 pi 654 640 628 618 609 33b 2048 none 582     33b 8192 ft 588 599 621   33b 8192 pi 582 569 571   33b 16384 pi 587 574 567 568  65b 2048 none 549     65b 8192 pi 542 532 537   table 1 evaluation perplexity on pg19 dataset rae et al 2020 ft direct finetuning pi position interpolation model finetuned with pi shows progressively lower perplexity with longer context window showing that pi can leverage long context well while the perplexity of ft increases over longer window note that overall the perplexity is higher compared to table 2 since pg19 has very different writing styles 33 measuring effective context window size through passkey retrieval we study the effective context window size ie the maximum distance of a token can effectively attend to during inference of our models after extension to measure this we follow a synthetic evaluation task of passkey retrieval proposed by mohtashami  jaggi 2023 in this task the models are asked to recover a random passkey hidden in a long document see figure 3 for the format of the document given a language model we estimate the upper and lower bounds of effective context windows as follows suppose the random passkey is k tokens away from the end of the input when a model persistently fails to retrieve the correct passkey value across several independent attempts it suggests that the effective context window size of the model is less than k conversely if a model consistently succeeds in retrieving the correct passkey value we deduce that the effective context window size of the model is at least k we evaluate the 7b and 33b llama model variants that are extended via position interpolation or direct finetuning for each model we use 32 different k uniformly spaced in the targeted context window l  and run the above tests for 10 times for each k where each time a random passkey of 5 random digits is used in table 4 we report kmax as a function of the number of finetuning steps 7 model evaluation context window size size context window method 2048 4096 8192 16384 32768 7b 2048 none 277     7b 8192 ft 285 274 273   7b 8192 pi 279 257 239   7b 16384 pi 279 257 237 225  7b 32768 pi 282 259 239 224 248 13b 2048 none 266     13b 8192 ft 271 256 250   13b 8192 pi 267 247 230   13b 16384 pi 268 247 229 218  13b 32768 pi 268 246 228 215 235 33b 2048 none 249     33b 8192 ft 256 248 247   33b 8192 pi 250 232 218   33b 16384 pi 253 234 218 207  65b 2048 none 242     65b 8192 pi 243 226 212   table 2 evaluation perplexity on arxiv math proofpile dataset azerbayev et al 2022 ft direct fine tuning pi position interpolation model number of finetuning steps size context window 0 200 400 600 800 1000 7b 8192 1610 712 710 702 699 695 7b 16384 11213 705 693 688 684 683 table 3 evaluation perplexity on pg19 dataset rae et al 2020 with respect to the number of finetuning steps using position interpolation 8 where kmax is defined as the maximum k such that for all k   k the model has a success rate of at least 20 on k   we can see that models extended via position interpolation all successfully attain their desired ex tension objectives in terms of effective context window sizes indicating by the effective context window size reaching maximum kmax  l   after merely finetuning for 200 steps consistently across both 7b and 33b model sizes and up to 32768 context windows in contrast llama models that are extended via direct finetuning only saw a minimal increase of the effective context win dow size kmax from 2048 to 2560 even after finetuning for more than 10000 steps with no clear indication of an acceleration in the increase of window size model finetuning steps size context window method 200 400 600 800 1000 10000 7b 8192 ft 1792 2048 2048 2048 2304 2560 33b 8192 ft 1792 2048 1792 2048 2304  7b 8192 pi 8192 8192 8192 8192 8192  7b 16384 pi 16384 16384 16384 16384 16384  7b 32768 pi 32768 32768 18432 32768 32768  33b 8192 pi 8192 8192 8192 8192 8192  33b 16384 pi 16384 16384 16384 16384 16384  table 4 effective context window sizes after finetuning ft direct finetuning pi position interpolation there is an important info hidden inside a lot of irrelevant text find it and memorize them i will quiz you about the important information there the grass is green the sky is blue the sun is yellow here we go there and back again repeat x times the pass key is 12345 remember it 12345 is the pass key the grass is green the sky is blue the sun is yellow here we go there and back again repeat y times what is the pass key the pass key is figure 3 prompt format for passkey retrieval we use the exact same prompt as proposed by mohtashami  jaggi 2023 here the passkey 12345 is replaced with a random 5digit numbers during test 34 benchmarks on original context window size we evaluate the models extended by position interpolation on several standard benchmark tasks within the original context window size of 2048 the evaluation results are listed in table 5 from the results we saw that models extended to 8192 produce comparable results on the original bench mark which is designed for a much smaller context window with a degradation of up to 2 on the benchmark tasks for both 7b and 33b model sizes models extended to longer context win dows regressed more on the benchmarks but still in reasonable ranges for most tasks we also note that the choice of finetuning datasets does not seem to lead significant difference in the benchmark performances which may be due to the limited number of finetuning steps used in our method the regression on benchmark tasks is consistent with our observation on perplexity regression in section 32 35 long document summarization in this task we evaluate our models performance on the long document summarization task in particular we consider the govreport huang et al 2021 dataset which contains 17457 documents for training and 972 documents for evaluation each document comes with a human generated summary we truncate all input documents to their first 15000 tokens we finetune the llama models extended with position interpolation with a context window of 16384 note the rescaling of position indices are still required during this finetuning step we first 9 model size context window finetune on boolq piqa racem raceh winogrande 7b 2048 none 761 789 557 422 696 7b 8192 pile 732 782 538 417 690 7b 16384 pile 698 776 533 409 678 7b 32768 pile 647 772 501 396 669 7b 8192 redpajama 755 774 545 415 681 33b 2048 none 816 802 611 459 762 33b 8192 pile 802 807 602 457 759 table 5 zeroshot performance on a subset of llama benchmarks models extended by position interpola tion comparable performance as the original models except for boolq dataset that may require models to pay close attention to word ordering in a short reference paragraph model evaluation score model context window rouge1 rouge2 rougel colt5 base ainslie et al 2023 16k 587 296 314 colt5 xl ainslie et al 2023 16k 613 322 338 llama7b extended 16k 600 280 295 table 6 rouge score on govreport dataset format the raw document using the prompt template in figure 4 and then concatenate the prompt with the groundtruth summary truncate to 1000 tokens associated with each document we fine tune the model using the next token prediction task with the above setup for 10 epochs the losses from the input prompt proportion of training examples are excluded during our finetuning we use a generation temperature of 05 and topp  095 as our inference parameter to generate a summarization of each document in the test set the final output is truncated at 1000 tokens we used the rouge1rouge2rougel scores lin 2004 as the evaluation metrics to evaluate the models outputs vs the groundtruth summaries in table 6 we report our evaluation results we have also included results from two baselines in existing scrolls leaderboard shaham et al 2022 ainslie et al 2023 in general we have obtained competitive r1 score among other models with minimal tuning of hyperparameters this result suggests our models with 16384 context window can effectively handle the long document summarization task read the following article and then summarize it   document goes here now summarize the above article summary figure 4 input format for long doc summarization 4 related work retrievalaugmented llm one line of work extends llms by augmenting it with retrieval mod ules which fetch related documents and include the retrieval results into the input context of an llm karpukhin et al 2020 guu et al 2020 izacard et al 2022 jiang et al 2022 khattab et al 2021 santhanam et al 2022 our work is complementary to these works as our extended context win dow allows more documents being included in the input in addition with an unmodified attention mechanism and model architecture our method may be more versatile as it can natively handle tasks beyond retrieval oriented ones such as long document summarization fewshots learning etc 10 recurrent transformers and memory transformers several works add memory capabilities to transformers through recurrence which increase the models capability of handling very long sequences bulatov et al 2022 wu et al 2020 dai et al 2019 wu et al 2022 martins et al 2021 mu et al 2023 one limitation of these works is that they only allow attending to a lossy compressed version of past inputs mu et al 2023 suggested that this may prevent models from remembering specific details in the past inputs in contrast our work allows attending to all previous tokens preserving all details without compression albeit with higher inference costs mohtashami  jaggi 2023 proposed landmark attention which allows full random access to any chunk of the input through introducing landmark tokens our work allows full access of the entire input through unmodified attention which may be useful for tasks such as summarization approximated multihead attention there is a large body of research that focuses on decreasing the memory and computational complexity of the multihead attention mha mechanism through approximation or sparsification child et al 2019 zaheer et al 2020 beltagy et al 2020 wang et al 2020 choromanski et al 2021 kitaev et al 2020 ren et al 2021 although not the focus of this work as these methods are not used in llama touvron et al 2023 we note that our method is compatible with most of them since our changes are restricted to position encodings and not attention mechanisms length extrapolation a recent line of research aims to train transformers models on short se quences and inference on longer press et al 2022 sun et al 2022 haviv et al 2022 however these methods have not been applied in some of the largest language models such as llama tou vron et al 2023 or opt zhang et al 2022 this has prevented them from enabling length extrapolation of many preexisting pretrained language models our work focuses on extending existing llms which can save substantial pretraining costs in addition our method preserves the quality of the original models even for small context window tasks since it does not deviate far from existing definitions of position encoding or attention mechanisms interpolation the most related technique to ours is proposed by dosovitskiy et al 2021 in their work on vision transformers where the authors proposed to linearly interpolate learnt position em beddings to support higher resolution which translates to an increased number of input embeddings in the finetuning stage the interpolated position embedding weights are used as initialization in the finetuning process for the newly added positions our work differs from their work in several ways 1 instead of interpolating position embeddings our method interpolates position indices which is more suitable for rope like position encodings and may require less training since no trainable parameters are added 2 we report successful results of extending the context window to 32 times while dosovitskiy et al 2021 explored up to 4 times our results extend theirs in exploring the upper limit of context window extension via interpolation 3 we evaluated and confirmed the effectiveness of position interpolation for extending context windows for language models we believe our results in conjunction with dosovitskiy et al 2021 provide empirical evidence on transformers remarkable ability of handling significantly longer sequences beyond training further we conjecture that a method similar to theirs is directly applicable in llms with learnable position embeddings such as opt zhang et al 2022 and we plan to investigate this in the future 5 conclusions position interpolation can effectively extend llama models context window to be significantly larger using minimal finetuning the extended models are fully capable to perform a variety of tasks on the extended context windows and preserve its original ability relatively well for tasks within the original extended models making them good choices of generic language models for both long and short input prompts further models extended by position interpolation can reuse most preexisting infrastructure and optimization making this method attractive in many practical applications we believe that position interpolation is a general method that could be apply to other types of position encodings which can allow extension for more types of llms and we plan to investigate in such directions in the near future', 'abstract we introduce codex a gpt language model fine tuned on publicly available code from github and study its python codewriting capabilities a distinct production version of codex powers github copilot on humaneval a new evalua tion set we release to measure functional correct ness for synthesizing programs from docstrings our model solves 288 of the problems while gpt3 solves 0 and gptj solves 114 fur thermore we find that repeated sampling from the model is a surprisingly effective strategy for pro ducing working solutions to difficult prompts us ing this method we solve 702 of our problems with 100 samples per problem careful investiga tion of our model reveals its limitations including difficulty with docstrings describing long chains of operations and with binding operations to vari ables finally we discuss the potential broader impacts of deploying powerful code generation technologies covering safety security and eco nomics equal contribution 1openai san francisco california usa 2anthropic ai san francisco california usa work per formed while at openai 3zipline south san francisco california usa work per formed while at openai correspondence to mark chen markopenaicom jerry tworek jtopenaicom heewoo jun hee wooopenaicom qiming yuan qimingopenaicom 1 introduction scalable sequence prediction models graves 2014 vaswani et al 2017 child et al 2019 have become a generalpurpose method for generation and representation learning in many domains including natural language pro cessing mikolov et al 2013 sutskever et al 2014 dai  le 2015 peters et al 2018 radford et al 2018 devlin et al 2018 computer vision van oord et al 2016 menick  kalchbrenner 2018 chen et al 2020 bao et al 2021 audio and speech processing oord et al 2016 2018 dhari wal et al 2020 baevski et al 2020 biology alley et al 2019 rives et al 2021 and even across multiple modali ties das et al 2017 lu et al 2019 ramesh et al 2021 zellers et al 2021 more recently language models have also fueled progress towards the longstanding challenge of program synthesis simon 1963 manna  waldinger 1971 spurred by the presence of code in large datasets husain et al 2019 gao et al 2020 and the resulting pro gramming capabilities of language models trained on these datasets wang  komatsuzaki 2021 popular language modeling objectives like masked language modeling devlin et al 2018 and span prediction raffel et al 2020 have also been adapted to train their programming counterparts codebert feng et al 2020 and pymt5 clement et al 2020 similarly our early investigation of gpt3 brown et al 2020 revealed that it could generate simple programs from python docstrings while rudimentary this capability was exciting because gpt3 was not explicitly trained for code generation given the considerable success of large lan guage models in other modalities and the abundance of publicly available code we hypothesized that a specialized gpt model called codex could excel at a variety of coding tasks this paper describes several early codex models whose descendants power github copilot and the codex models in the openai api arxiv210703374v2 cslg 14 jul 2021 evaluating large language models trained on code figure 1 pass rates of our models on the humaneval dataset as a function of model size when a single sample is generated for each problem gpt12b solves no problems but codex finetuned on code solves 288 of the problems and codexs further finetuned on correctly implemented standalone functions solves 377 of the problems from here further gains can be realized by generating 100 samples per problem and selecting the sample with the highest mean logprobability 445 solved or by selecting the sample that passes the unit tests 775 solved all samples are generated with temperature 08 in this work we focus on the task of generating stan dalone python functions from docstrings and evaluate the correctness of code samples automatically through unit tests this is in contrast to natural language generation where samples are typically evaluated by heuristics or by human evaluators to accurately benchmark our model we create a dataset of 164 original programming problems with unit tests these problems assess language compre hension algorithms and simple mathematics with some comparable to simple software interview questions we release this data along with an evaluation framework at httpswwwgithubcomopenaihumaneval to solve a problem in our test set we generate multiple samples from the models and check if any of them pass the unit tests with just a single sample a 12b parameter codex solves 288 of these problems and a 300m parameter codex solves 132 of these problems in contrast the 6b parameter gptj wang  komatsuzaki 2021 achieves 114 on the same dataset while all gpt models achieve near 0 to improve our models performance at the task of function synthesis from docstrings we finetune codex on standalone correctly implemented functions the resulting model codexs solves 377 of problems with a single sample figure 2 showcases problems of varying difficulty in our dataset along with correct model generated solutions realworld programming tasks often involve iterations of approaches and bug fixes which is approximated by gener ating many samples from our models and selecting one that passes all unit tests within 100 samples codexs is able to generate at least one correct function for 775 of the prob lems this result suggests that accurate code samples can be selected via heuristic ranking instead of fully evaluating each sample the latter of which may not be possible or prac tical in deployment indeed we find that the sample with highest mean logprobability passes unit tests for 445 of the problems we conclude by discussing the limitations and potential broader impacts of these codex models and of increasingly powerful code generating models more generally 2 evaluation framework in this section we discuss the details of our evaluation framework we begin by defining the passk metric and explain its advantages over standard matchbased metrics next we describe the dataset of handwritten problems called humaneval which we created in order to bench mark our models finally we discuss the sandbox environ ment we used to safely execute modelgenerated code 21 functional correctness generative models for code are predominantly benchmarked by matching samples against a reference solution where the match can be exact or fuzzy as in bleu score how ever recent work has surfaced deficiencies in matchbased metrics for code for instance ren et al 2020 finds that bleu has problems capturing semantic features specific to code and suggests several semantic modifications to the score more fundamentally matchbased metrics are unable to ac count for the large and complex space of programs function ally equivalent to a reference solution as a consequence recent works in unsupervised code translation lachaux et al 2020 and pseudocodetocode translation kulal et al 2019 have turned to functional correctness instead where a sample is considered correct if it passes a set of unit tests we argue that this metric should be applied to docstring conditional code generation as well perhaps the most convincing reason to evaluate functional correctness is that it is used by human developers to judge code a framework known as testdriven development dic tates that software requirements be converted into test cases before any implementation begins and success is defined by a program that passes these tests while few organiza tions employ full testdriven development integration of new code is usually dependent on creating and passing unit tests kulal et al 2019 evaluate functional correctness using the passk metric where k code samples are generated per problem a problem is considered solved if any sample evaluating large language models trained on code figure 2 three example problems from the humaneval dataset where the probabilities that a single sample from codex12b passes unit tests are 09 017 and 0005 the prompt provided to the model is shown with a white background and a successful modelgenerated completion is shown in a yellow background though not a guarantee for problem novelty all problems were handwritten and not programmatically copied from existing sources random problems and samples can be found in appendix b passes the unit tests and the total fraction of problems solved is reported however computing passk in this way can have high variance instead to evaluate passk we generate n  k samples per task in this paper we use n  200 and k  100 count the number of correct samples c  n which pass unit tests and calculate the unbiased estimator passk  e problems  1  nc k n k  1 calculating this estimator directly results in very large num bers and numerical instability in figure 3 we include a numerically stable numpy implementation that simplifies the expression and evaluates the product termbyterm one may be tempted to estimate passk with 11p k where p is the empirical estimate of pass1 but we show that it is biased in appendix a def pass_at_kn c k  param n total number of samples param c number of correct samples param k k in passk  if n  c  k return 10 return 10  npprod10  k  nparangen  c  1 n  1 figure 3 a numerically stable script for calculating an unbiased estimate of passk later we provide evidence that bleu score may not be a reliable indicator of functional correctness by showing that functionally inequivalent programs generated by our model which are guaranteed to disagree with the reference solution on some input often have higher bleu scores than functionally equivalent ones evaluating large language models trained on code 22 humaneval handwritten evaluation set we evaluate functional correctness on a set of 164 hand written programming problems which we call the hu maneval dataset each problem includes a function sig nature docstring body and several unit tests with an av erage of 77 tests per problem it is important for these tasks to be handwritten since our models are trained on a large fraction of github which already contains solutions to problems from a variety of sources for example there are more than ten public repositories containing solutions to codeforces problems which make up part of the recently proposed apps dataset hendrycks et al 2021 programming tasks in the humaneval dataset assess lan guage comprehension reasoning algorithms and simple mathematics we release the humaneval dataset so that others can evaluate functional correctness and measure the problemsolving capabilities of their models the dataset can be found at httpswwwgithubcomopenaihumaneval 23 sandbox for executing generated programs since publicly available programs have unknown intent and generated programs are often incorrect executing these programs poses a security risk indeed github is known to contain malicious programs that alter or change their environments rokon et al 2020 therefore we developed a sandbox environment to safely run untrusted programs against unit tests our goals were to prevent these programs from modifying gaining persistence on accessing sensitive resources on or exfiltrating data from a host or network since openais training infrastructure is built on kubernetes and cloud services we designed our sandbox to address the limitations of these environments while remaining idiomatic with their patterns of use we selected the gvisor container runtime lacasse 2018 as the main host protection component since container runtimes like docker can share host resources with contain ers a malicious container could potentially compromise a host gvisor protects the host by emulating its resources to introduce a security boundary between the host and its con tainers networkadjacent hosts and services are protected by ebpfbased firewall rules that prevent inbound and out bound connections except for those required for experiment control 3 code finetuning we finetune gpt models containing up to 12b parameters on code to produce codex in contrast with gpt codex displays nontrivial performance on the humaneval dataset in fact codex is able to solve the majority of the problems in humaneval if we generate and evaluate 100 samples per problem and pick one that passes unit tests when limited to a budget of one evaluation per problem producing multiple samples with codex and choosing the one with the highest mean logprobability provides significant gains 31 data collection our training dataset was collected in may 2020 from 54 mil lion public software repositories hosted on github contain ing 179 gb of unique python files under 1 mb we filtered out files which were likely autogenerated had average line length greater than 100 had maximum line length greater than 1000 or contained a small percentage of alphanumeric characters after filtering our final dataset totaled 159 gb 32 methods since codex is evaluated on natural language prompts we hypothesized that it would be beneficial to finetune from the gpt3 brown et al 2020 model family which already contains strong natural language representations surpris ingly we did not observe improvements when starting from a pretrained language model possibly because the fine tuning dataset is so large nevertheless models finetuned from gpt converge more quickly so we apply this strategy for all subsequent experiments we train codex using the same learning rate as the corre sponding gpt model with a 175 step linear warmup and cosine learning rate decay we train for a total of 100 billion tokens using the adam optimizer with 1  09 2  095  108  and a weight decay coefficient of 01 in order to maximally leverage text representations from gpt we base our code lexer on the gpt3 text tokenizer since the distribution of words in github code differs from that of natural text this tokenizer is not very effective for representing code the largest source of inefficiency arises from encoding whitespace so we add an additional set of tokens for representing whitespace runs of different lengths this allows us to represent code using approximately 30 fewer tokens to compute passk we assemble each humaneval prob lem into a prompt consisting of a header a signature and a docstring which is illustrated in figure 2 we sample tokens from codex until we encounter one of the following stop sequences nclass ndef n nif or nprint since the model will continue generating addi tional functions or statements otherwise we use nucleus sampling holtzman et al 2020 with top p  095 for all sampling evaluation in this work 33 results in figure 4 we plot test loss on a heldout validation set against codex model size we find that just as language evaluating large language models trained on code figure 4 model crossentropy test loss measured on a heldout split of our python github code corpus the smooth power law scaling of performance with model size observed in gpt3 appears to hold even after code finetuning model test loss follows a power law in model size kaplan et al 2020 test loss after code finetuning follows a similar power law with functional form  n 592107  013 where n is the number of nonembedding parameters in the model when evaluating passk it is important to optimize sam pling temperature for the particular value of k in figure 5 we plot passk against the number of samples k and the sampling temperature we find that higher temperatures are optimal for larger k because the resulting set of samples has higher diversity and the metric rewards only whether the model generates any correct solution in particular for a 679m parameter model the optimal tem perature for pass1 is t   02 and the optimal tempera ture for pass100 is t   08 with these temperatures we find that pass1 and pass100 scale smoothly as a function of model size figure 6 passk can also be interpreted as the result of evaluating the best out of k samples where the best sample is picked by an oracle with prior knowledge of the unit tests from a practical perspective we are also interested in the set ting where we must select a single sample from k samples without having access to an oracle for instance when the model is used as an autocomplete tool where a user provides a prompt we do not have unit tests but would like to return only a single completion to the user for evaluation so as to not overwhelm them inspired by similar work in language modeling we find that choosing the sample with the highest mean token log probability outperforms evaluating a random sample while choosing the sample based on sum log probability can per form slightly worse than picking randomly figure 7 demon strates the benefits of applying these heuristics to samples at temperature 08 from codex12b figure 5 in the top panel we plot passk against the number of samples k for various temperature settings higher temperatures are better when the number of samples is large likely due to the increased sample diversity in the bottom panel we plot the best temperature setting for each k obtained by taking the upper hull of the top panel figure 6 using the optimal temperatures 02 and 08 for pass1 and pass100 we plot these two metrics as a function of model size performance appears to scale smoothly as a sigmoid in log parameters evaluating large language models trained on code figure 7 model performance in the setting where we can generate multiple samples but only evaluate one we can do better than ran domly selecting a sample by choosing the solution with the highest mean logprobability red or with the highest backtranslation score orange described in sec 5 the blue line represents the theoretical best performance obtained using an oracle with prior knowledge of the unit tests finally we compute bleu scores for all codex12b hu maneval samples at temperature 08 against their reference solutions for each problem when we plot the distributions of bleu scores for correct and incorrect solutions we notice significant overlap figure 8 since an incorrect solution is guaranteed to be functionally inequivalent to the reference solution we conclude that improvements in bleu score may not indicate improved rates of functional correctness in practice 34 comparative analysis of related models and systems two recent works similar in spirit to codex are gptneo black et al 2021 and gptj wang  komatsuzaki 2021 which are trained on the pile gao et al 2020 a dataset containing text from a variety of sources as well as 8 github code the broader research community has found that these models outperform existing gpt systems in qualitative programming evaluations woolf 2021 we confirm these findings using the humaneval dataset showing that gptneo achieves 64 pass1 and 213 pass100 while gpt models of comparable sizes achieve near 0 on both metrics we see a remarkable progression in capabilities with gptneo27b roughly equivalent to codex85m 30 fewer parameters similarly gptj6b achieves 116 pass1 and 277 pass100 which is roughly equivalent to codex300m 20 fewer parameters pass rates are obtained by taking the best result from eval figure 8 bleu score probability densities for correct blue and wrong green solutions from codex12b for 4 random tasks from humaneval note that the distributions are not cleanly separable suggesting that optimizing for bleu score is not equivalent to optimizing for functional correctness uating at temperatures 02 04 and 08 for gptneo and from temperatures 02 and 08 for gptj detailed results across multiple model sizes can be found in table 1 finally we benchmark codex against the largest free model from tabnine a leading code autocomplete system which achieves 26 pass1 at t  04 and 76 pass100 at t  08 this is roughly equivalent to codex12m one of the smallest models in our suite 35 results on the apps dataset recently hendrycks et al 2021 introduced the apps dataset to measure the coding challenge competence of lan guage models the apps dataset consists of 5000 training and 5000 test examples of coding problems each with a set of unit tests and for the training data a set of correct solu tions most of the apps tests problems are not formulated as singlefunction synthesis tasks but rather as fullprogram synthesis reading input from stdin and printing output to stdout in contrast to the main codex training data in the paper that introduces apps the authors benchmark a few language models and report two metrics the percentage of problems where the model finds a correct solution called the strict accuracy and the percentage of unit tests passed even if the solution is incorrect the latter measure is re ported only so as to reduce variance of the measurements because the results on the first metric were so low we avoid this metric and only focus on strict accuracy and  as in evaluating large language models trained on code table 1 codex gptneo  tabnine evaluations for humaneval we find that gptj pass1 is between codex85m and codex 300m performance passk k  1 k  10 k  100 gptneo 125m 075 188 297 gptneo 13b 479 747 1630 gptneo 27b 641 1127 2137 gptj 6b 1162 1574 2774 tabnine 258 435 759 codex12m 200 362 858 codex25m 321 71 1289 codex42m 506 88 1555 codex85m 822 1281 224 codex300m 1317 2037 3627 codex679m 1622 257 4095 codex25b 2136 3542 595 codex12b 2881 4681 7231 the previous sections  we report passk numbers for vari ous k table 2 there are 2 additional factors wellknown from coding competitions that we take into account  in coding competitions and in the apps datasets tasks are provided with 3 inputoutput examples included in the task description we utilize this by sampling 1000 solutions from the model and filtering out only those that pass these 3 unit tests if such solutions exist we then calculate pass rates in this filtered set and call it filtered passk results without filtering are presented as raw passk  it is often the case both in coding competitions and in the results from codex that a correct solution is found but it is not algorithmically efficient enough to be con sidered passing while this is not acceptable in the competitions we also report the number of solutions that codex produces that do not fail on any unit test but that do timeout on some of them we use a timeout of 3 seconds in our evaluation to compensate for the fact the codex is not finetuned on apps we append a single inputoutput example from the task description to the docstring as a formatting hint we de note this setting as 1shot in table 2 and find that codex 12b evaluated 1shot achieves comparable performance to a gptneo model finetuned on apps consistent with our earlier findings there are large benefits from generating and evaluating as many as 1000 samples per task though for more difficult problems solutions are often not efficient enough to pass the time limits finally evaluating the first sample which passes the 3 public unit tests for each problem yields higher performance than raw pass100 samples 4 supervised finetuning in addition to standalone functions python code found on github contains class implementations configuration files scripts and even files used to store data this code is seem ingly unrelated to synthesizing functions from docstrings and we hypothesize that the distribution mismatch reduces humaneval performance in order to adapt codex to the distribution of the task of in terest we construct a set of training problems from correctly implemented standalone functions and use them for addi tional supervised finetuning we describe two approaches for collecting these examples from competitive program ming websites and from repositories with continuous inte gration we call the supervised finetuned models codexs and show that they produce consistent gains across model size 41 problems from competitive programming programming contest and interview preparation websites use hidden unit tests to automatically judge the func tional correctness of submissions these problems are self contained come with wellwritten problem statements and generally have excellent test coverage additionally these problems test algorithmic reasoning over a broad range of core skills and difficulties we collected problem statements function signatures and solutions from several popular programming contest and interview preparation websites we then assembled these into programming tasks similar to humaneval using the problem description as the docstring since complete test suites are often hidden we created unit tests from examples found in the problem statements or extracted additional test cases through submitting incorrect solutions in total we curated 10000 problems in this way 42 problems from continuous integration next we curated programming problems from open source projects taking advantage of syssetprofile we were able to trace and collect inputs and outputs for all functions called during integration tests this data could then be used to create unit tests for the functions projects that employ continuous integration ci are ideal candidates for tracing we follow the commands in the ci configuration files which contain build and test commands to set up the virtual environments install dependencies and run integration tests we considered github repos using travis and tox as their ci frameworks as they are two of the most popular ci tools we additionally used publicly available source code from pip packages found in the python package index pypi evaluating large language models trained on code table 2 finetuned gptneo numbers from the apps paper referenced above for codex12b the number of passing programs that timeout on some test is in the bracket we used temperature 06 for sampling to cover all k in passk so raw pass1 results could be improved with lower temperature introductory interview competition gptneo 27b raw pass1 390 057 000 gptneo 27b raw pass5 550 080 000 1shot codex raw pass1 414 433 014 030 002 003 1shot codex raw pass5 965 1005 051 102 009 016 1shot codex raw pass100 2020 2157 204 399 105 173 1shot codex raw pass1000 2502 2777 370 794 323 585 1shot codex filtered pass1 2278 2510 264 578 304 525 1shot codex filtered pass5 2452 2715 323 713 308 553 because these projects contained untrusted code it was im portant to run integration tests in the sandboxed environment described above while there are millions of potential functions to curate problems from we only collected about 40000 because not all functions accept inputs and return outputs even when they do most objects captured at runtime cannot be pickled and restored outside the sandbox unless the project was installed since our tracing methodology produced inputs and outputs for all invoked functions even builtin and library calls im ported by the project were turned into problems for this reason functions from tracing tended to be the building blocks of commandline utilities to excel at these tasks the model does not need to know advanced algorithms and data structures rather it needs to be able to follow in structions to implement the functionality specified in the docstring thus tracing complements the puzzle nature of coding competition problems and broadens the distribution of tasks 43 filtering problems in the previous sections we presented two methods we used to automatically create training problems however it is unclear how to control for quality some prompts underspecify the function that is implemented in which case a perfectly valid solution may be wrongly penalized by the unit test some problems are stateful and subsequent executions can result in different outcomes to address these issues we use codex12b to generate 100 samples per curated problem if no samples pass the unit tests we consider the task to be either ambiguous or too difficult and filter it out we reran this verification several times to remove stateful or nondeterministic problems 44 methods we finetune codex on these training problems to produce a set of supervised finetuned models which we call codex s to produce examples from training problems we assem ble the problems into the format shown in figure 2 if there are prompts of varying length in a batch we leftpad shorter prompts to the length of the longest prompt so that the first tokens in the reference solutions line up in context we train to minimize negative loglikelihood of the reference solution and mask out loss for any tokens in the prompt we train using a learning rate 110 as large as used for finetuning codex but adhere to the same learning rate schedule and train until validation loss plateaus less than 10b tokens 45 results as with codex we first compute the optimal temperature for evaluating passk for 1  k  100 we find that codexs prefers slightly higher temperatures for all k  1 which possibly reflects the fact that codexs captures a narrower distribution than codex we use t   0 for computing pass1 and t   1 for computing pass100 next we compare codexs against codex on pass1 and pass100 codexs outperforms the corresponding codex by an average margin of 65 percentage points on pass1 and by a larger average margin of 151 percentage points on pass100 across model size we also plot the performance of different sample selection heuristics for codexs12b against the same heuristics for codex12b when ranking between 1 and 100 samples by mean log probability the average benefit over random ranking is 116 percentage points which is over 2 percentage points higher than the corresponding benefit for codex evaluating large language models trained on code figure 9 optimal sampling temperatures as a function of the num ber of samples generated for both codex and codexs codexs generally requires a higher temperature for any particular value of k possibly to compensate for the fact that it models a narrower distribution figure 10 comparing codexs against codex on the metrics pro posed in section 3 codexs is one or two orders of magnitude more parameter efficient on pass1 and pass100 and logprob sample ranking with codexs yields similar benefits over random sampling that codex does 5 docstring generation generating code from docstrings is possible with codex because code typically follows after a docstring but it is not easy to induce codex to generate docstrings from code nev ertheless we are motivated to produce a docstring writing model for safety reasons as such a model can be used to de scribe the intent behind generated code using the training problems described in the previous section we can eas ily create a training dataset for codeconditional docstring generation specifically for each training problem we assemble a train ing example by concatenating the function signature the reference solution and then the docstring just as we train codexs by minimizing negative loglikelihood of the ref erence solution we train the docstring generating models codexd by minimizing negative loglikelihood of the doc string when we benchmark our code generation models we mea sure passk on the humaneval dataset where correctness is defined by passing a set of unit tests however there is no similar way to evaluate docstring samples automatically therefore we grade sample docstrings by hand considering a docstring correct if it uniquely and accurately specifies the code body due to the time consuming nature of this process we only grade 10 samples per problem for a total of 1640 problems from codexd12b at temperature 08 codexd often generates incorrect unit tests along with a docstring but we ignore these during grading however we do not consider the docstring correct when the model simply copies the code body into the docstring the most common failure modes we observe are when the docstring model leaves out an important detail such as an answer must be to two decimal places or when it overconditions on the function name and invents a problem unrelated to the function body as shown in table 3 pass rates for codexd are lower but comparable to the corresponding pass rates for codexs at the same temperature we do not have a strong hypothesis for which direction should yield higher pass rates while generating docstrings may be more forgiving because natu ral language syntax is less strict than code syntax docstrings in our dataset may be lower quality because developers tend to devote less time to writing docstrings indeed our model produces docstrings like i just found this function online and this test is not correctly written and its not my solu tion finally with a docstring model we have yet another way to choose a single sample from a set of k samples in stead of picking the sample with the best mean log proba bility as investigated in the previous two sections we can choose the sample that maximizes the backtranslation ob evaluating large language models trained on code table 3 pass rates for our docstring generating model codexd which is evaluated by handgrading 10 samples per task due to the lack of a groundtruth automatic evaluation we find similar but lower passrates compared to codexs model pass1 pass10 codexs12b 322 595 codexd12b 203 465 jective pground truth docstringgenerated sample where p is evaluated using codexd unfortunately in figure 7 we show that ranking samples via backtranslation under performs mean logprobability ranking though it outper forms random ranking this heuristic also appears to overfit quickly 6 limitations while codex is able to sample correct solutions for the majority of humaneval problems we find that it has a number of limitations first codex is not sample efficient to train our training dataset comprises a significant fraction of publicly available python code on github totaling hundreds of millions of lines of code even seasoned developers do not encounter anywhere near this amount of code over their careers in deed a strong student who completes an introductory com puter science course is expected to be able to solve a larger fraction of problems than codex12b next we explore prompts on which codex is likely to fail or display counterintuitive behavior while evaluating code generation is wellstudied xu et al 2021 helmuth  spec tor 2015 pantridge et al 2017 many existing metrics measure performance in tightly specified constrained prob lem instances eg string manipulation in flashfill gul wani 2011 therefore we developed a set of qualitative metrics for measuring the capabilities of code generating models while controlling for the complexity and abstrac tion level of the specifications appendix d applying this framework we find that codex can recommend syntacti cally incorrect or undefined code and can invoke functions variables and attributes that are undefined or outside the scope of the codebase moreover codex struggles to parse through increasingly long and higherlevel or systemlevel specifications to concretely illustrate model performance degradation as docstring length increases we create a dataset of synthetic problems assembled from 13 basic building blocks each of which modifies an input string in a deterministic way ex ample building blocks are convert the string to lowercase or remove every third character from the string the full list is described in appendix c we find that as the number of chained building blocks in the docstring increases model performance decreases exponentially this behavior is un characteristic of a human programmer who should be able to correctly implement a program for a chain of arbitrary length if they can do so for a chain of length two figure 11 pass rates of codex12b samples against the number of chained components in the synthetically generated docstring with each additional component pass rate drops by roughly a factor of 23 further just as textconditional generative models in other modalities ramesh et al 2021 have difficulty with bind ing attributes to objects codex can make mistakes binding operations to variables especially when the number of oper ations and variables in the docstring is large for instance in the following prompt codex12b does not decrement the variable w and also fails to return the product of all numbers def do_workx y z w  add 3 to y then subtract 4 from both x and w return the product of the four numbers  t  y  3 u  x  4 v  z  w return v this understanding of codexs limited systemlevel synthe sis capabilities helps inform our assessment of the potential hazards of using it in a generative capacity as well as the broader societal impacts that such systems could have 7 broader impacts and hazard analysis codex has the potential to be useful in a range of ways for example it could help onboard users to new codebases reduce context switching for experienced coders enable nonprogrammers to write specifications and have codex draft implementations and aid in education and exploration however codex also raises significant safety challenges does not always produce code that is aligned with user intent evaluating large language models trained on code and has the potential to be misused to better understand some of the hazards of using codex in a generative capacity we conducted a hazard analysis focused on identifying risk factors leveson 2019 with the potential to cause harm1 we outline some of our key findings across several risk areas below while some of our findings about the potential societal impacts of code generation systems were informed by work towards responsible deployment of the productionoriented codex models which descended from the researchoriented codex models described in this paper this section is not intended to provide a full account of any particular products safety features unless otherwise specified we anchor our analysis in the specific properties of the models described in this paper we share this analysis in the belief that some of it generalizes to the broader class of code generation systems and to encourage a norm of performing detailed impact analysis as part of major machine learning research projects note that by focusing largely on risks in this section we do not mean to imply that we expect the impact of this class of technologies to be netnegative rather risks merit particular attention here because they may be subtle or require deliber ate effort to address whereas we expect the benefits to be more obvious and automatic from the perspective of most users and affected stakeholders 71 overreliance one of the key risks associated with using code generation models in practice is overreliance on generated outputs due to the limitations described above as well as alignment issues described below codex may suggest solutions that superficially appear correct but do not actually perform the task the user intended this could particularly affect novice programmers and could have significant safety implications depending on the context we discuss a related issue in appendix g namely that code generation models can sug gest insecure code for these reasons human oversight and vigilance is required for safe use of code generation systems like codex we note several immediate ways to improve safety in the subsection on risk mitigation below though overreliance in particular is one that we believe merits further inquiry in industry and academia while it is conceptually straight 1we sought to include harms spanning geographic and temporal scales we also considered not only the severity and probability but also the distribution of harms however we note that the analysis described here is only one milestone in what we hope will be a larger crosssectoral and crossorganizational effort to steer code generation in a societally beneficial direction as we describe our findings we note various specific uncertainties and areas for future work in different sections figure 12 when the prompt includes subtle bugs codex tends to produce worse code than it is capable of this persists when the prompt also includes instructions to write correct code this gap increases with model size forward to provide documentation to users reminding them about model limitations empirical investigation is neces sary in order to identify how to reliably ensure vigilance in practice across a range of user experience levels ui designs and tasks one challenge researchers should consider is that as capabilities improve it may become increasingly difficult to guard against automation bias 72 misalignment as with other large language models trained on a nexttoken prediction objective codex will generate code that is as sim ilar as possible to its training distribution one consequence of this is that such models may do things that are unhelpful for the user despite having the capability to be more helpful see figure 12 for example if the user has some subtle mistakes in their code codex may deliberately suggest code that superficially appears good but is incorrect this is an alignment failure  the model is not aligned with the users intentions informally a system is misaligned if theres some task x that we want it to do and it is capable of doing x but chooses not to in contrast if a system fails to do x because it does not have the ability to do so then this system is not misaligned it is just incompetent see appendix e for more detail including a more precise definition of alignment it is important to study misalignment because it is a problem that is likely to become worse not better as the capabili ties of our systems increase for example the model size scaling trend for the example in figure 12 indicates that misalignment would likely persist and even get worse if data parameters and training time were scaled up while we expect that misaligned behaviour like this is un likely to cause significant harm in current models it is likely to become more dangerous and harder to eliminate as model evaluating large language models trained on code capabilities increase a highly capable but sufficiently mis aligned model trained on user approval might produce ob fuscated code that looks good to the user even on careful inspection but in fact does something undesirable or even harmful 73 bias and representation mirroring what has been found in the case of other language models trained on internet data bender et al 2021 blod gett et al 2020 abid et al 2021 brown et al 2020 we found that codex can be prompted in ways that generate racist denigratory and otherwise harmful outputs as code comments meriting interventions such as those discussed in the subsection on risk mitigation below we also found that code generation models raise further bias and represen tation issues beyond problematic natural language codex can generate code with structure that reflects stereotypes about gender race emotion class the structure of names and other characteristics particularly in the context of users who might overrely on codex or use it without first think ing through project design this issue could have significant safety implications giving further motivation to discourage overreliance we discuss bias and representation issues further in appendix f filtration or modulation of generated outputs documentation and other interventions may help to mitigate these risks 74 economic and labor market impacts code generation and associated capabilities have several possible economic and labor market impacts while codex at its current capability level may somewhat reduce the cost of producing software by increasing programmer produc tivity the size of this effect may be limited by the fact that engineers dont spend their full day writing code onet 2021 other important tasks include conferring with col leagues writing design specifications and upgrading ex isting software stacks2 we also found that codex imports packages at different rates which could advantage some package authors over others particularly if programmers and engineers come to rely on codexs suggestions over a longer time horizon the effects of this class of technologies on softwarerelated labor markets and on the economy more generally could be more substantial as capabilities improve more study is needed both on the effects of code genera tion capabilities and on appropriate responses we discuss economic and labor market implications in more detail in appendix h 2 indeed bls classifies computer programmers and software developers separately where developers are more highly paid than programmers have more tasks indirectly related to writing and interacting with code and in the us are already projected to see greater demand over the next 10 years li et al 2020 bureau of labor statistics 2021ab 75 security implications codex could have various effects on the security landscape because codex can produce vulnerable or misaligned code3 qualified operators should review its generations before ex ecuting or trusting them absent appropriate precautions future code generation models may be able to be trained to produce more secure code than the average developer though that is far from certain codex could also be misused to aid cybercrime although this is worthy of concern based on our testing we believe that at their current level of capability codex models do not materially lower the barrier to entry for malware devel opment4 we expect that more powerful code generation models will lead to future advancements and therefore fur ther research into mitigations and continued study of model capabilities are necessary the nondeterministic nature of systems like codex could enable more advanced malware this nondeterminism makes it easier to create diverse software that accomplish the same tasks while software diversity can sometimes aid defenders5 it presents unique challenges for traditional malware detection and antivirus systems that rely on finger printing and signaturematching against previously sampled binaries for example a more capable code generation model could conceivably advance techniques for generating polymorphic malware6 we believe that application secu rity and model deployment strategies including ratelimiting access and abuse monitoring can manage this threat in the near term however the efficacy of these mitigations may scale sublinearly as more capable models are developed similar to large language models codex models can learn patterns present in their training data carlini et al 2021 sensitive data present in source code are liable to be pre dicted by the model because codex is trained on public repositories we consider any sensitive data present in the training data to have already been compromised similarly the public data should generally be treated as untrusted as previous work goldblum et al 2021 schuster et al 2020 has found that attackers may be able to corrupt training data to trigger specific model behaviors at runtime we further discuss security implications in appendix g 3 see appendix g  insecure code for examples of codex pro ducing insecure code 4 for more on characterizing codexs capability limitations see the limitations section and experiments in the security analysis in appendix g 5 for example by helping to prevent certain types of memory corruption vulnerabilities see davis 2018 for more 6 polymorphic malware is malicious code that mutates its im plementation while maintaining its function evaluating large language models trained on code 76 environmental impacts codex like other large generative models has an energy footprint from both training and inference schwartz et al 2019 bender et al 2021 patterson et al 2021 the origi nal training of gpt312b consumed hundreds of petaflops days of compute while finetuning it to create codex12b consumed a similar amount of compute this training was performed on a platform azure that purchases carbon credits and sources significant amounts of renewable energy reducing its carbon footprint7 compute consumption also has costs in the wider supply chain that can be quite con centrated on certain regions8 looking more globally and longterm the compute demands of code generation could grow to be much larger than codexs training if significant inference is used to tackle challenging problems9 77 legal implications there are several legal considerations related to generated code to begin with the training of ai systems on internet data such as public github repositories has previously been identified as an instance of fair use okeefe et al 2019 our preliminary research also finds that codex models rarely generate code that is identical to the contents of training data such occurrences were  01 in a study examining the frequency of code generations that appear to match code snippets in the training data ziegler 2021 in these rare instances the generated code consisted of common expres sions or conventions within the programming language that appeared over and over again in the training data we find that to the extent the generated code appears identical to the training data it is due to the predictive weightings in the model rather than retention and copying of specific code generated code is also responsive and customized to the users input and the user retains complete control over editing and acceptance of the generated code this can make code generation similar to autosuggest or autocompletion 7microsoft made a commitment in 2020 to shift to 100 per cent renewable energy supply in its buildings and data centers by 2025 httpsblogsmicrosoftcomblog20200116microsoft willbecarbonnegativeby2030 a full assessment of the envi ronmental impact of compute use is impossible to conduct without grounding in context and making comparison to the counterfactual impacts of competing products or services such analysis is out of scope for this paper 8while data center energy usage has become much more effi cient in recent years masanet et al 2020 the production use and disposal of semiconductors still imposes environmental and human costs see eg crawford 2021 9given that code generation and other forms of ai might be deployed widely throughout the economy as discussed above these considerations suggest additional urgency in adopting renewable energy features that exist as features of other tools of authorship eg document editors in the sense that the finished work is still seen as the authors our commitment to responsible and safe ai includes con tinued attention to the broader intellectual property impli cations of code generation systems we intend to remain engaged with policymakers and experts on these issues so that the users of such systems can ultimately deploy them with confidence 78 risk mitigation in closing given the above models like codex should be developed used and their capabilities explored carefully with an eye towards maximizing their positive social im pacts and minimizing intentional or unintentional harms that their use might cause a contextual approach is critical to effective hazard analysis and mitigation though a few broad categories of mitigations are important to consider in any deployment of code generation models careful documentation and user interface design code re view requirements andor content controls eg filtering of outputs may help to reduce harms associated with over reliance as well as offensive content or insecure code gener ation in the context of a model made available as a service eg via an api policies such as user review use case restrictions monitoring andor rate limiting may also help to reduce harms associated with malicious use or prevent its use in highstakes domains for which the models are not well suited appendices e f g and h provide further detail on the risks described in this section and outline additional mitigation and research opportunities 8 related work the deep learning resurgence has led to strong advances in the field of program learning two popular approaches to neural program learning are program induction and program synthesis in program induction a model generates program outputs directly from a latent program representation learning to execute zaremba  sutskever 2014 demonstrated that models could execute simple tasks like addition and memo rization later attempts at program induction incorporated inductive biases based on modern computing devices such as the neural turing machine graves et al 2014 memory networks weston et al 2015 sukhbaatar et al 2015 the neural gpu kaiser  sutskever 2015 and the differen tiable neural computer graves et al 2016 more recent approaches like the neural program interpreter reed  de freitas 2016 shin et al 2018 pierrot et al 2021 and evaluating large language models trained on code universal transformer dehghani et al 2019 found recur rence to be a useful component in program induction in program synthesis a model explicitly generates a pro gram usually from a natural language specification one of the most popular classical approaches used a probabilis tic context free grammar pcfg to generate a programs abstract syntax tree ast maddison  tarlow 2014 im proved on this setup by learning a state vector used to con dition child node expansion later allamanis et al 2015 applied this idea in texttocode retrieval and yin  neu big 2017 utilized it in textconditional code generation code2seq alon et al 2018 found that asts could also be leveraged for codetotext generation programs can also be synthesized without passing through an ast representation hindle et al 2012 investigated ngram language models of code finding code to be more predictable than natural language latent predictor net works ling et al 2016 showed that characterlevel lan guage models could generate working code for implement ing magic the gathering cards in an online arena when aided with a latent mode that allows card attributes to be copied into code deepcoder balog et al 2017 trained a model to predict the functions appearing in source code which could be used to guide program search following the success of large natural language models de vlin et al 2018 radford et al 2019 liu et al 2019 raffel et al 2020 brown et al 2020 large scale transformers have also been applied towards program synthesis code bert feng et al 2020 trained the bert objective on docstrings paired with functions and obtained strong results on code search pymt5 clement et al 2020 is similar in spirit to our work and used the t5 objective to train a sys tem which can translate between nonoverlapping subsets of signature docstring body we used functional correctness to benchmark our models and observed improvements on this metric with more sam pling spoc kulal et al 2019 considered the problem of producing functionally correct code from pseudocode with a fixed budget of compilations which is similar to our passk metric transcoder lachaux et al 2020 trained a system to translate between programming languages in an unsupervised manner and also observed that functional correctness better captured the capabilities of their model than bleu score in fact contracode jain et al 2020 leveraged the large space of functionally correct programs to train a contrastive code model which improved model performance on tasks like type inference finally robust fill devlin et al 2017 observed that the best way to find a program consistent with input examples was to synthesize multiple samples through beam search two early domainspecific datasets used to benchmark neu ral programming systems were flashfill gulwani 2011 gulwani et al 2012 and hearthstone ling et al 2016 though the community has trended towards broader and more difficult datasets barone  sennrich 2017 proposed a large training and evaluation dataset consisting of python declarations docstrings and bodies scraped from github the codesearchnet challenge husain et al 2019 built an even larger corpus from github with data from multiple popular programming languages recently codexglue lu et al 2021 aggregated several programming bench marks making use of the recently proposed codebleu metric ren et al 2020 most relevant to our evaluation work is the apps hendrycks et al 2021 benchmark for measuring functional correctness based on problems from the competitive programming website codeforces finally we note that coding is a broad activity which in volves much more than synthesizing code from docstrings tufano et al 2020 use transformers to generate unit tests for code which outperformed commercial offerings aye et al 2021 built an internal autocomplete tool for face book and found that training on accepted user completions boosted system performance development also entails lo cating and fixing bugs early works used static or dynamic code analysis agrawal et al 1995 korel  rilling 1997 learned association rules jeffrey et al 2009 and genetic programming goues et al 2012 to debug faulty code these approaches relied on running against a test suite to not only evaluate the correctness of suggestions but also expose problems in execution trace or search for a solution more recent works tufano et al 2019 drain et al 2021 considered bugfixing as neural machine translation from buggy to correct programs however these works used an exact match against a reference instead of functional cor rectness citing qi et al 2015s finding that most of the proposed solutions by genetic search in goues et al 2012 passed through weak test suites by deleting functionality that failed human developers often write test suites with limited but targeted coverage but this does not always work well against an algorithm highlighting the challenges of evaluating correctness of programs 9 conclusion we investigated whether it was possible to train large lan guage models to produce functionally correct code bodies from natural language docstrings by finetuning gpt on code from github we found that our models displayed strong performance on a dataset of humanwritten problems with difficulty level comparable to easy interview problems model performance could be improved by training on a distribution more similar to the evaluation set and also by producing multiple samples from a model we also found that it was simple to train a model to complete the reverse evaluating large language models trained on code task of producing docstrings from code bodies and that the performance profiles of these models were similar finally we expanded on the broader impacts of code generating models and discussed model limitations finding significant room for improvement']\n","cluster_labels=[0 1 1 1 2 1 3 0 4 2]\n","Silhouette Score: -0.03861928421941022\n","Davies Bouldin Index: 1.4371557351419093\n","Calinski-Harabasz Index: 0.9084548958320526\n","Dunn Index: 0.791426282062282\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n","from sklearn_extra.cluster import KMedoids\n","from sklearn.metrics import pairwise_distances\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Function to clean text data\n","def clean_text(text):\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra white spaces\n","    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    text = text.lower()  # Convert text to lowercase\n","    return text\n","\n","# Function to read and clean text files\n","def read_clean_text_files(folder_path):\n","    texts = []\n","    for file_name in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, file_name)\n","        if os.path.isfile(file_path) and file_name.endswith('.txt'):\n","            with open(file_path, 'r', encoding='utf-8') as file:\n","                text = file.read()\n","                cleaned_text = clean_text(text)\n","                texts.append(cleaned_text)\n","    return texts\n","\n","# Path to the folder containing text files\n","folder_path = '/content/drive/My Drive/BDA 798 LLMs/TestCase 3 Papers text/Input_Raw_3'\n","\n","# Read and clean text files\n","texts = read_clean_text_files(folder_path)\n","\n","print(f'texts count={len(texts)}')\n","print(f'texts={texts}')\n","\n","# TF-IDF vectorization\n","vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = vectorizer.fit_transform(texts)\n","\n","# Clustering\n","n_clusters = 5  # Number of clusters\n","kmedoids = KMedoids(n_clusters=n_clusters, random_state=0).fit(tfidf_matrix)\n","cluster_labels = kmedoids.labels_\n","\n","print(f'cluster_labels={cluster_labels}')\n","\n","# Evaluation\n","silhouette_score_val = silhouette_score(tfidf_matrix, cluster_labels)\n","davies_bouldin_score_val = davies_bouldin_score(tfidf_matrix.toarray(), cluster_labels)\n","calinski_harabasz_score_val = calinski_harabasz_score(tfidf_matrix.toarray(), cluster_labels)\n","pairwise_distance_matrix = pairwise_distances(tfidf_matrix, metric='euclidean')\n","dunn_index = np.min(pairwise_distance_matrix[np.nonzero(pairwise_distance_matrix)]) / \\\n","             np.max([np.max(pairwise_distance_matrix[i, np.nonzero(pairwise_distance_matrix[i, :])]) for i in range(len(pairwise_distance_matrix))])\n","\n","# Print evaluation scores\n","print(\"Silhouette Score:\", silhouette_score_val)\n","print(\"Davies Bouldin Index:\", davies_bouldin_score_val)\n","print(\"Calinski-Harabasz Index:\", calinski_harabasz_score_val)\n","print(\"Dunn Index:\", dunn_index)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"gde9528GUrvF"},"execution_count":null,"outputs":[]}]}