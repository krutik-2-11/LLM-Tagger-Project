{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 : This is the notebook to evaluate the performance of our LLM based approach on its ability to cluster similar documents together based on their generated Keywords."
      ],
      "metadata": {
        "id": "4CzdjjHm1fBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To reproduce this output you need to adjust the file_path to read the raw `.txt` files as input available in the github repository https://github.com/krutik-2-11/LLM-Tagger-Project/tree/main*"
      ],
      "metadata": {
        "id": "KmpxCr_c2AK-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vvmo31mg0BS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25cde43-0167-4eb6-c30f-4c21530c52a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below is the sample clusters created by LLMs based on Keywords generated in Part 1. The title of clusters are also created by LLM on its own. The clusters are based on the prompts given to LLM."
      ],
      "metadata": {
        "id": "o_uUqq7d2T33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Natural Language Processing\":[\n",
        "        \"HyperTuning: Toward Adapting Large Language Models without Back-propagation\",\n",
        "        \"Using LLM for Improving Key Event Discovery:Temporal-Guided News Stream Clustering with Event Summaries\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"Programming and Software Engineering\":[\n",
        "        \"Automated Repair of Programs from Large Language Models\",\n",
        "        \"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"Disaster and Emergency Response\":[\n",
        "        \"Categorization of disaster‑related deaths in Minamisoma city after the Fukushima nuclear disaster using clustering analysis\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"Health and Medicine\":[\n",
        "        \"Effects of a Home-Based Stretching Program on Bench Press Maximum Strength and Shoulder Flexibility\",\n",
        "        \"The effect of verbal praise on prospective memory\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"Quantum Computing\":[\n",
        "        \"Implementation of quantum compression on IBM quantum computers\"\n",
        "    ],\n",
        "\n",
        "\n",
        "    \"Astronomy and Cosmology\":[\n",
        "        \"The alignment between brightest cluster galaxies and host clusters\"\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Ss7F0LhHECPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_1 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper1raw.txt'\n",
        "file_path_2 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper2raw.txt'\n",
        "file_path_3 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper3raw.txt'\n",
        "file_path_4 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper4raw.txt'\n",
        "file_path_5 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper5raw.txt'\n",
        "file_path_6 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper6raw.txt'\n",
        "file_path_7 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper7raw.txt'\n",
        "file_path_8 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper8raw.txt'\n",
        "file_path_9 = '/content/drive/My Drive/BDA 798 LLMs/TestCase 1 Papers text/paper9raw.txt'\n",
        "with open(file_path_1, 'r') as file:\n",
        "    paper1raw = file.read()\n",
        "with open(file_path_2, 'r') as file:\n",
        "    paper2raw = file.read()\n",
        "with open(file_path_3, 'r') as file:\n",
        "    paper3raw = file.read()\n",
        "with open(file_path_4, 'r') as file:\n",
        "    paper4raw = file.read()\n",
        "with open(file_path_5, 'r') as file:\n",
        "    paper5raw = file.read()\n",
        "with open(file_path_6, 'r') as file:\n",
        "    paper6raw = file.read()\n",
        "with open(file_path_7, 'r') as file:\n",
        "    paper7raw = file.read()\n",
        "with open(file_path_8, 'r') as file:\n",
        "    paper8raw = file.read()\n",
        "with open(file_path_9, 'r') as file:\n",
        "    paper9raw = file.read()"
      ],
      "metadata": {
        "id": "7IdHKkKGKqkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Clustering"
      ],
      "metadata": {
        "id": "G8NDx-GaIxMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Mapping paper titles to their raw text\n",
        "paper_dict = {\n",
        "    \"The alignment between brightest cluster galaxies and host clusters\":paper1raw,\n",
        "    \"Automated Repair of Programs from Large Language Models\": paper2raw,\n",
        "    \"Using LLM for Improving Key Event Discovery:Temporal-Guided News Stream Clustering with Event Summaries\": paper3raw,\n",
        "    \"HyperTuning: Toward Adapting Large Language Models without Back-propagation\":paper4raw,\n",
        "    \"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\":paper5raw,\n",
        "    \"Implementation of quantum compression on IBM quantum computers\":paper6raw,\n",
        "    \"The effect of verbal praise on prospective memory\":paper7raw,\n",
        "    \"Categorization of disaster‑related deaths in Minamisoma city after the Fukushima nuclear disaster using clustering analysis\":paper8raw,\n",
        "    \"Effects of a Home-Based Stretching Program on Bench Press Maximum Strength and Shoulder Flexibility\":paper9raw\n",
        "}\n",
        "\n",
        "# Convert paper text to a list for vectorization\n",
        "papers_raw_text = list(paper_dict.values())\n",
        "\n",
        "# Vectorize the paper texts\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(papers_raw_text)\n",
        "\n",
        "# Calculate cosine similarity matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Convert normalized similarity to distance matrix\n",
        "distance_matrix = 1 - cosine_sim_matrix\n",
        "\n",
        "#print(f'distance_matrix = {distance_matrix}')\n",
        "\n",
        "# Define clusters based on your initial categorization\n",
        "clusters = data\n",
        "\n",
        "# Create a mapping of paper to its cluster number\n",
        "paper_to_cluster = {}\n",
        "for cluster_num, (cluster_name, papers) in enumerate(clusters.items()):\n",
        "    for paper in papers:\n",
        "        paper_to_cluster[paper] = cluster_num\n",
        "\n",
        "# Create a list of cluster labels based on the order of papers in `papers_raw_text`\n",
        "cluster_labels = [paper_to_cluster[title] for title in paper_dict.keys()]\n",
        "\n",
        "# Calculate silhouette scores for each paper\n",
        "silhouette_values = silhouette_samples(distance_matrix, cluster_labels, metric=\"euclidean\")\n",
        "\n",
        "# Calculate the overall silhouette score\n",
        "overall_silhouette_score = silhouette_score(distance_matrix, cluster_labels, metric=\"euclidean\")\n",
        "\n",
        "# Enhanced print statements for better readability\n",
        "print(\"Individual Silhouette Scores for Each Paper:\")\n",
        "for idx, score in enumerate(silhouette_values):\n",
        "    print(f\"Paper {idx + 1} ({list(paper_dict.keys())[idx]}): Silhouette Score = {score:.4f}\")\n",
        "\n",
        "print(\"\\nOverall Silhouette Score for the Dataset: {:.4f}\".format(overall_silhouette_score))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0-NSowAg7i5",
        "outputId": "b15f3510-6c82-425d-a2a1-26b0c2919f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Silhouette Scores for Each Paper:\n",
            "Paper 1 (The alignment between brightest cluster galaxies and host clusters): Silhouette Score = 0.0000\n",
            "Paper 2 (Automated Repair of Programs from Large Language Models): Silhouette Score = -0.0048\n",
            "Paper 3 (Using LLM for Improving Key Event Discovery:Temporal-Guided News Stream Clustering with Event Summaries): Silhouette Score = -0.1764\n",
            "Paper 4 (HyperTuning: Toward Adapting Large Language Models without Back-propagation): Silhouette Score = -0.0839\n",
            "Paper 5 (LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS): Silhouette Score = -0.2910\n",
            "Paper 6 (Implementation of quantum compression on IBM quantum computers): Silhouette Score = 0.0000\n",
            "Paper 7 (The effect of verbal praise on prospective memory): Silhouette Score = 0.0091\n",
            "Paper 8 (Categorization of disaster‑related deaths in Minamisoma city after the Fukushima nuclear disaster using clustering analysis): Silhouette Score = 0.0000\n",
            "Paper 9 (Effects of a Home-Based Stretching Program on Bench Press Maximum Strength and Shoulder Flexibility): Silhouette Score = -0.0903\n",
            "\n",
            "Overall Silhouette Score for the Dataset: -0.0708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hierarchical Clustering"
      ],
      "metadata": {
        "id": "0rC3iAeDH-2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchal clustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "# Convert paper text to a list for vectorization\n",
        "papers_raw_text = list(paper_dict.values())\n",
        "\n",
        "# Vectorize the paper texts\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(papers_raw_text)\n",
        "\n",
        "# Calculate cosine similarity matrix\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Convert similarity to distance matrix (for hierarchical clustering)\n",
        "distance_matrix = 1 - cosine_sim_matrix\n",
        "\n",
        "n_clusters = 6\n",
        "clustering_model = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')\n",
        "cluster_labels = clustering_model.fit_predict(distance_matrix)\n",
        "\n",
        "# Calculate silhouette scores for each paper\n",
        "silhouette_values = silhouette_samples(distance_matrix, cluster_labels, metric=\"euclidean\")\n",
        "\n",
        "# Calculate the overall silhouette score\n",
        "overall_silhouette_score = silhouette_score(distance_matrix, cluster_labels, metric=\"euclidean\")\n",
        "\n",
        "print(\"Individual Silhouette Scores for Each Paper:\")\n",
        "for idx, score in enumerate(silhouette_values):\n",
        "    print(f\"Paper {idx + 1} ({list(paper_dict.keys())[idx]}): Silhouette Score = {score:.4f}\")\n",
        "\n",
        "print(f\"\\nOverall Silhouette Score for the Dataset: {overall_silhouette_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzoljcVShAXI",
        "outputId": "c536f183-b998-43f5-cae5-261973185dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Silhouette Scores for Each Paper:\n",
            "Paper 1 (The alignment between brightest cluster galaxies and host clusters): Silhouette Score = 0.0867\n",
            "Paper 2 (Automated Repair of Programs from Large Language Models): Silhouette Score = 0.0000\n",
            "Paper 3 (Using LLM for Improving Key Event Discovery:Temporal-Guided News Stream Clustering with Event Summaries): Silhouette Score = 0.0707\n",
            "Paper 4 (HyperTuning: Toward Adapting Large Language Models without Back-propagation): Silhouette Score = 0.2973\n",
            "Paper 5 (LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS): Silhouette Score = 0.1614\n",
            "Paper 6 (Implementation of quantum compression on IBM quantum computers): Silhouette Score = 0.0882\n",
            "Paper 7 (The effect of verbal praise on prospective memory): Silhouette Score = 0.0000\n",
            "Paper 8 (Categorization of disaster‑related deaths in Minamisoma city after the Fukushima nuclear disaster using clustering analysis): Silhouette Score = 0.0000\n",
            "Paper 9 (Effects of a Home-Based Stretching Program on Bench Press Maximum Strength and Shoulder Flexibility): Silhouette Score = 0.0000\n",
            "\n",
            "Overall Silhouette Score for the Dataset: 0.0782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kmeans clustering"
      ],
      "metadata": {
        "id": "ujEhaD4wH6wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kmeans clustering\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Convert paper text to a list for vectorization\n",
        "papers_raw_text = list(paper_dict.values())\n",
        "\n",
        "# Vectorize the paper texts\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(papers_raw_text)\n",
        "\n",
        "# Apply K-means clustering\n",
        "# Choose an appropriate number of clusters\n",
        "n_clusters = 6\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# *** K-means works directly with the TF-IDF features, so WE SHOUDL NOT convert to a distance matrix for silhouette calculation\n",
        "# Calculate silhouette scores for each paper\n",
        "silhouette_values = silhouette_samples(tfidf_matrix, cluster_labels, metric='euclidean')\n",
        "\n",
        "# Calculate the overall silhouette score\n",
        "overall_silhouette_score = silhouette_score(tfidf_matrix, cluster_labels, metric='euclidean')\n",
        "\n",
        "print(\"Individual Silhouette Scores for Each Paper:\")\n",
        "for idx, score in enumerate(silhouette_values):\n",
        "    print(f\"Paper {idx + 1} ({list(paper_dict.keys())[idx]}): Silhouette Score = {score:.4f}\")\n",
        "\n",
        "print(f\"\\nOverall Silhouette Score for the Dataset: {overall_silhouette_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idh_PNA2h18p",
        "outputId": "3f59811a-23c6-4604-8373-2cbc30cc0203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Silhouette Scores for Each Paper:\n",
            "Paper 1 (The alignment between brightest cluster galaxies and host clusters): Silhouette Score = 0.0465\n",
            "Paper 2 (Automated Repair of Programs from Large Language Models): Silhouette Score = 0.0000\n",
            "Paper 3 (Using LLM for Improving Key Event Discovery:Temporal-Guided News Stream Clustering with Event Summaries): Silhouette Score = 0.0207\n",
            "Paper 4 (HyperTuning: Toward Adapting Large Language Models without Back-propagation): Silhouette Score = 0.1796\n",
            "Paper 5 (LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS): Silhouette Score = 0.0993\n",
            "Paper 6 (Implementation of quantum compression on IBM quantum computers): Silhouette Score = 0.0404\n",
            "Paper 7 (The effect of verbal praise on prospective memory): Silhouette Score = 0.0000\n",
            "Paper 8 (Categorization of disaster‑related deaths in Minamisoma city after the Fukushima nuclear disaster using clustering analysis): Silhouette Score = 0.0000\n",
            "Paper 9 (Effects of a Home-Based Stretching Program on Bench Press Maximum Strength and Shoulder Flexibility): Silhouette Score = 0.0000\n",
            "\n",
            "Overall Silhouette Score for the Dataset: 0.0430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Index"
      ],
      "metadata": {
        "id": "f6FkG5OsHP2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.downloader import load\n",
        "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "papers_raw_text = [paper1raw,paper2raw, paper3raw, paper4raw,paper5raw, paper6raw, paper7raw,paper8raw, paper9raw]\n",
        "\n",
        "# Define a preprocessing function\n",
        "def preprocess(text):\n",
        "    filters = [strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short]\n",
        "    return preprocess_string(text, filters)\n",
        "\n",
        "# Preprocess the documents\n",
        "papers_preprocessed = [preprocess(text) for text in papers_raw_text]\n",
        "\n",
        "# Load a pre-trained Word2Vec model from Gensim's Data Repository\n",
        "model = load('word2vec-google-news-300')\n",
        "\n",
        "# Function to vectorize a document using Word2Vec embeddings\n",
        "def document_vector(doc):\n",
        "    # Remove words not in model's vocabulary\n",
        "    doc = [word for word in doc if word in model.key_to_index]\n",
        "    if not doc:\n",
        "        return np.zeros(model.vector_size)\n",
        "    # Aggregate word vectors into a single document vector by averaging\n",
        "    return np.mean([model[word] for word in doc], axis=0)\n",
        "\n",
        "# Vectorize each document\n",
        "doc_vectors = np.array([document_vector(doc) for doc in papers_preprocessed])\n",
        "\n",
        "# Apply K-means clustering\n",
        "n_clusters = 6  # Adjust\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(doc_vectors)\n",
        "\n",
        "# Calculate silhouette scores for each paper\n",
        "silhouette_values = silhouette_samples(doc_vectors, cluster_labels, metric='euclidean')\n",
        "\n",
        "# Calculate the overall silhouette score\n",
        "overall_silhouette_score = silhouette_score(doc_vectors, cluster_labels, metric='euclidean')\n",
        "\n",
        "print(\"Individual Silhouette Scores for Each Paper:\")\n",
        "for idx, score in enumerate(silhouette_values):\n",
        "    print(f\"Paper {idx + 1}: Silhouette Score = {score:.4f}\")\n",
        "\n",
        "print(f\"\\nOverall Silhouette Score for the Dataset: {overall_silhouette_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-79zU5ZjVr7",
        "outputId": "ece286ec-f816-43ae-84a8-88d0582a89bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Silhouette Scores for Each Paper:\n",
            "Paper 1: Silhouette Score = 0.0000\n",
            "Paper 2: Silhouette Score = 0.1615\n",
            "Paper 3: Silhouette Score = 0.1758\n",
            "Paper 4: Silhouette Score = 0.1260\n",
            "Paper 5: Silhouette Score = -0.0011\n",
            "Paper 6: Silhouette Score = 0.2294\n",
            "Paper 7: Silhouette Score = 0.0000\n",
            "Paper 8: Silhouette Score = 0.0000\n",
            "Paper 9: Silhouette Score = 0.0000\n",
            "\n",
            "Overall Silhouette Score for the Dataset: 0.0768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Davis Bouldin and Calinski-Harabasz Index"
      ],
      "metadata": {
        "id": "aT3qaFaoHFUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DBI\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# # Mapping paper titles to their raw text\n",
        "# paper_dict = {\n",
        "#     \"Improving Language Understanding by Generative Pre-Training\": \"paper1raw\",\n",
        "#     \"Colour measurements by computer vision for food quality control – A review\": \"paper3raw\",\n",
        "#     \"Learning techniques used in computer vision for food quality evaluation: a review\": \"paper2raw\"\n",
        "# }\n",
        "\n",
        "# Convert paper text to a list for vectorization\n",
        "papers_raw_text = list(paper_dict.values())\n",
        "\n",
        "# Vectorize the paper texts\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(papers_raw_text)\n",
        "\n",
        "# Define clusters based on your initial categorization, this part needs to be adjusted to your data\n",
        "clusters = data\n",
        "\n",
        "# Create a mapping of paper to its cluster number\n",
        "paper_to_cluster = {}\n",
        "for cluster_num, (cluster_name, papers) in enumerate(clusters.items()):\n",
        "    for paper in papers:\n",
        "        paper_to_cluster[paper] = cluster_num\n",
        "\n",
        "# Create a list of cluster labels based on the order of papers in `papers_raw_text`\n",
        "cluster_labels = [paper_to_cluster[title] for title in paper_dict.keys()]\n",
        "\n",
        "\n",
        "# Calculate Davies-Bouldin Index\n",
        "dbi_score = davies_bouldin_score(tfidf_matrix.toarray(), cluster_labels)\n",
        "# Calculate Calinski-Harabasz Index\n",
        "chi_score = calinski_harabasz_score(tfidf_matrix.toarray(), cluster_labels)\n",
        "\n",
        "print(\"Davies-Bouldin Index for the Dataset: {:.4f}\".format(dbi_score))\n",
        "print(\"Calinski-Harabasz Index for the Dataset: {:.13f}\".format(chi_score))\n"
      ],
      "metadata": {
        "id": "FOrafQM1rAgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c35068-418f-4b7d-bb96-a0f70d0c8be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Davies-Bouldin Index for the Dataset: 1.0398\n",
            "Calinski-Harabasz Index for the Dataset: 1.0040536177716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dunn Index"
      ],
      "metadata": {
        "id": "60OtJlrXHx51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def dunn_index(tfidf_matrix, cluster_labels):\n",
        "    # Convert TF-IDF matrix to dense array\n",
        "    tfidf_dense_matrix = tfidf_matrix.toarray()\n",
        "\n",
        "    # Calculate pairwise Euclidean distances between all points\n",
        "    distances = euclidean_distances(tfidf_dense_matrix)\n",
        "\n",
        "    unique_clusters = np.unique(cluster_labels)\n",
        "    min_intercluster_distance = np.inf\n",
        "    max_intracluster_diameter = 0\n",
        "\n",
        "    for i in unique_clusters:\n",
        "        cluster_i_indices = np.where(cluster_labels == i)[0]\n",
        "        for j in unique_clusters:\n",
        "            if i == j:\n",
        "                # Skip if comparing the same cluster\n",
        "                continue\n",
        "            cluster_j_indices = np.where(cluster_labels == j)[0]\n",
        "            inter_distances = distances[np.ix_(cluster_i_indices, cluster_j_indices)]\n",
        "            if inter_distances.size > 0:\n",
        "                min_intercluster_distance = min(min_intercluster_distance, np.min(inter_distances))\n",
        "\n",
        "        intra_distances = distances[np.ix_(cluster_i_indices, cluster_i_indices)]\n",
        "        if intra_distances.size > 0:\n",
        "            # Calculate max only for non-zero distances to avoid self-comparison\n",
        "            non_zero_distances = intra_distances[np.nonzero(intra_distances)]\n",
        "            if non_zero_distances.size > 0:\n",
        "                max_intracluster_diameter = max(max_intracluster_diameter, np.max(non_zero_distances))\n",
        "            else:\n",
        "                # Handle the case where a cluster might have only one point or all points are identical\n",
        "                max_intracluster_diameter = max(max_intracluster_diameter, 0)\n",
        "\n",
        "    dunn_index = min_intercluster_distance / max_intracluster_diameter if max_intracluster_diameter > 0 else 0\n",
        "    return dunn_index\n",
        "\n",
        "# Calculate Dunn Index\n",
        "dunn_score = dunn_index(tfidf_matrix, np.array(cluster_labels))\n",
        "\n",
        "print(f\"Dunn Index for the Dataset: {dunn_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80koJwGmHy99",
        "outputId": "a0c9bf19-b7c0-4d70-f074-73adddd03bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dunn Index for the Dataset: 0.8140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAgTBv-KVN0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}