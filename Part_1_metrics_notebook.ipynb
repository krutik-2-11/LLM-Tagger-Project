{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Evaluation Notebook: This is the notebook to evaluate the performance of LLM based approach for extracting relevant keywords from a research paper."
      ],
      "metadata": {
        "id": "i4meDcUYw4vA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To reproduce this output you need to adjust the file_path to read the raw `.txt` files as input available in the github repository https://github.com/krutik-2-11/LLM-Tagger-Project/tree/main*"
      ],
      "metadata": {
        "id": "r8LOMWF40HYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_O_NBJZbc1J",
        "outputId": "b71c57fb-6c9d-4963-d99f-269351f17629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the test text file"
      ],
      "metadata": {
        "id": "hdueBR1iyaUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'Evaluating Large Language Models Trained on Code.txt'\n",
        "file_path = f'/content/drive/My Drive/BDA 798 LLMs/TestCase 3 Papers text/{file_name}'\n",
        "with open(file_path, 'r') as file:\n",
        "    paper1raw = file.read()"
      ],
      "metadata": {
        "id": "VFdJKqeNb_6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below is the raw input we got after reading a sample file."
      ],
      "metadata": {
        "id": "OR-03YffzMBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper1raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "XPpJw8LqfESp",
        "outputId": "e1d3d7bd-cefb-4953-aff8-fa716ac153bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abstract\\n\\nWe introduce Codex, a GPT language model fine-\\ntuned on publicly available code from GitHub,\\n\\nand study its Python code-writing capabilities.\\nA distinct production version of Codex powers\\n\\nGitHub Copilot. On HumanEval, a new evalua-\\ntion set we release to measure functional correct-\\nness for synthesizing programs from docstrings,\\n\\nour model solves 28.8% of the problems, while\\n\\nGPT-3 solves 0% and GPT-J solves 11.4%. Fur-\\nthermore, we find that repeated sampling from the\\n\\nmodel is a surprisingly effective strategy for pro-\\nducing working solutions to difficult prompts. Us-\\ning this method, we solve 70.2% of our problems\\n\\nwith 100 samples per problem. Careful investiga-\\ntion of our model reveals its limitations, including\\n\\ndifficulty with docstrings describing long chains\\n\\nof operations and with binding operations to vari-\\nables. Finally, we discuss the potential broader\\n\\nimpacts of deploying powerful code generation\\n\\ntechnologies, covering safety, security, and eco-\\nnomics.\\n\\n*Equal contribution\\n1OpenAI, San Francisco, California, USA.\\n\\n2Anthropic AI, San Francisco, California, USA. Work per-\\nformed while at OpenAI.\\n\\n3Zipline, South San Francisco, California, USA. Work per-\\nformed while at OpenAI.\\n\\nCorrespondence to: Mark Chen <mark@openai.com>,\\n\\nJerry Tworek <jt@openai.com>, Heewoo Jun <hee-\\nwoo@openai.com>, Qiming Yuan <qiming@openai.com>.\\n\\n1. Introduction\\nScalable sequence prediction models (Graves, 2014;\\nVaswani et al., 2017; Child et al., 2019) have become a\\ngeneral-purpose method for generation and representation\\n\\nlearning in many domains, including natural language pro-\\ncessing (Mikolov et al., 2013; Sutskever et al., 2014; Dai &\\n\\nLe, 2015; Peters et al., 2018; Radford et al., 2018; Devlin\\net al., 2018), computer vision (Van Oord et al., 2016; Menick\\n& Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021),\\n\\naudio and speech processing (Oord et al., 2016; 2018; Dhari-\\nwal et al., 2020; Baevski et al., 2020), biology (Alley et al.,\\n\\n2019; Rives et al., 2021), and even across multiple modali-\\nties (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021;\\n\\nZellers et al., 2021). More recently, language models have\\nalso fueled progress towards the longstanding challenge\\nof program synthesis (Simon, 1963; Manna & Waldinger,\\n1971), spurred by the presence of code in large datasets\\n\\n(Husain et al., 2019; Gao et al., 2020) and the resulting pro-\\ngramming capabilities of language models trained on these\\n\\ndatasets (Wang & Komatsuzaki, 2021). Popular language\\nmodeling objectives like masked language modeling (Devlin\\net al., 2018) and span prediction (Raffel et al., 2020) have\\nalso been adapted to train their programming counterparts\\nCodeBERT (Feng et al., 2020) and PyMT5 (Clement et al.,\\n2020).\\nSimilarly, our early investigation of GPT-3 (Brown et al.,\\n2020) revealed that it could generate simple programs from\\nPython docstrings. While rudimentary, this capability was\\nexciting because GPT-3 was not explicitly trained for code\\n\\ngeneration. Given the considerable success of large lan-\\nguage models in other modalities and the abundance of\\n\\npublicly available code, we hypothesized that a specialized\\nGPT model, called Codex, could excel at a variety of coding\\ntasks. This paper describes several early Codex models,\\nwhose descendants power GitHub Copilot and the Codex\\nmodels in the OpenAI API.\\n\\narXiv:2107.03374v2 [cs.LG] 14 Jul 2021\\n\\nEvaluating Large Language Models Trained on Code\\n\\nFigure 1. Pass rates of our models on the HumanEval dataset as a\\nfunction of model size. When a single sample is generated for each\\nproblem, GPT-12B solves no problems, but Codex (fine-tuned\\non code) solves 28.8% of the problems, and Codex-S (further\\nfine-tuned on correctly implemented standalone functions) solves\\n37.7% of the problems. From here, further gains can be realized by\\ngenerating 100 samples per problem and selecting the sample with\\nthe highest mean log-probability (44.5% solved) or by selecting\\nthe sample that passes the unit tests (77.5% solved). All samples\\nare generated with temperature 0.8.\\n\\nIn this work, we focus on the task of generating stan-\\ndalone Python functions from docstrings, and evaluate the\\n\\ncorrectness of code samples automatically through unit\\ntests. This is in contrast to natural language generation,\\nwhere samples are typically evaluated by heuristics or by\\nhuman evaluators. To accurately benchmark our model,\\nwe create a dataset of 164 original programming problems\\n\\nwith unit tests. These problems assess language compre-\\nhension, algorithms, and simple mathematics, with some\\n\\ncomparable to simple software interview questions. We\\nrelease this data along with an evaluation framework at\\nhttps://www.github.com/openai/human-eval.\\nTo solve a problem in our test set, we generate multiple\\nsamples from the models, and check if any of them pass the\\nunit tests. With just a single sample, a 12B parameter Codex\\nsolves 28.8% of these problems, and a 300M parameter\\nCodex solves 13.2% of these problems. In contrast, the 6B\\nparameter GPT-J (Wang & Komatsuzaki, 2021) achieves\\n11.4% on the same dataset, while all GPT models achieve\\nnear 0%. To improve our model’s performance at the task of\\nfunction synthesis from docstrings, we fine-tune Codex on\\nstandalone, correctly implemented functions. The resulting\\nmodel, Codex-S, solves 37.7% of problems with a single\\nsample. Figure 2 showcases problems of varying difficulty\\nin our dataset, along with correct model generated solutions.\\nReal-world programming tasks often involve iterations of\\n\\napproaches and bug fixes, which is approximated by gener-\\nating many samples from our models and selecting one that\\n\\npasses all unit tests. Within 100 samples, Codex-S is able to\\n\\ngenerate at least one correct function for 77.5% of the prob-\\nlems. This result suggests that accurate code samples can\\n\\nbe selected via heuristic ranking instead of fully evaluating\\n\\neach sample, the latter of which may not be possible or prac-\\ntical in deployment. Indeed, we find that the sample with\\n\\nhighest mean log-probability passes unit tests for 44.5% of\\nthe problems.\\nWe conclude by discussing the limitations and potential\\nbroader impacts of these Codex models and of increasingly\\npowerful code generating models more generally.\\n2. Evaluation Framework\\nIn this section, we discuss the details of our evaluation\\nframework. We begin by defining the pass@k metric, and\\nexplain its advantages over standard match-based metrics.\\nNext, we describe the dataset of hand-written problems,\\n\\ncalled “HumanEval,” which we created in order to bench-\\nmark our models. Finally, we discuss the sandbox environ-\\nment we used to safely execute model-generated code.\\n\\n2.1. Functional Correctness\\nGenerative models for code are predominantly benchmarked\\nby matching samples against a reference solution, where\\n\\nthe match can be exact or fuzzy (as in BLEU score). How-\\never, recent work has surfaced deficiencies in match-based\\n\\nmetrics for code. For instance, Ren et al. (2020) finds that\\nBLEU has problems capturing semantic features specific\\nto code, and suggests several semantic modifications to the\\nscore.\\n\\nMore fundamentally, match-based metrics are unable to ac-\\ncount for the large and complex space of programs function-\\nally equivalent to a reference solution. As a consequence,\\n\\nrecent works in unsupervised code translation (Lachaux\\net al., 2020) and pseudocode-to-code translation (Kulal et al.,\\n2019) have turned to functional correctness instead, where\\na sample is considered correct if it passes a set of unit tests.\\n\\nWe argue that this metric should be applied to docstring-\\nconditional code generation as well.\\n\\nPerhaps the most convincing reason to evaluate functional\\ncorrectness is that it is used by human developers to judge\\n\\ncode. A framework known as test-driven development dic-\\ntates that software requirements be converted into test cases\\n\\nbefore any implementation begins, and success is defined\\n\\nby a program that passes these tests. While few organiza-\\ntions employ full test-driven development, integration of\\n\\nnew code is usually dependent on creating and passing unit\\ntests.\\nKulal et al. (2019) evaluate functional correctness using\\nthe pass@k metric, where k code samples are generated\\nper problem, a problem is considered solved if any sample\\n\\nEvaluating Large Language Models Trained on Code\\n\\nFigure 2. Three example problems from the HumanEval dataset, where the probabilities that a single sample from Codex-12B passes unit\\ntests are 0.9, 0.17, and 0.005. The prompt provided to the model is shown with a white background, and a successful model-generated\\ncompletion is shown in a yellow background. Though not a guarantee for problem novelty, all problems were hand-written and not\\nprogrammatically copied from existing sources. Random problems and samples can be found in Appendix B.\\npasses the unit tests, and the total fraction of problems\\nsolved is reported. However, computing pass@k in this\\nway can have high variance. Instead, to evaluate pass@k,\\nwe generate n ≥ k samples per task (in this paper, we\\nuse n = 200 and k ≤ 100), count the number of correct\\nsamples c ≤ n which pass unit tests, and calculate the\\nunbiased estimator\\n\\npass@k := E\\nProblems \"\\n1 −\\nn−c\\nk\\n\\nn\\nk\\n\\n#\\n(1)\\n\\nCalculating this estimator directly results in very large num-\\nbers and numerical instability. In Figure 3, we include a\\n\\nnumerically stable numpy implementation that simplifies\\nthe expression and evaluates the product term-by-term. One\\nmay be tempted to estimate pass@k with 1−(1−pˆ)\\nk where\\npˆ is the empirical estimate of pass@1, but we show that it is\\nbiased in Appendix A.\\n\\ndef pass_at_k(n, c, k):\\n\"\"\"\\n:param n: total number of samples\\n:param c: number of correct samples\\n:param k: k in pass@$k$\\n\"\"\"\\nif n - c < k: return 1.0\\nreturn 1.0 - np.prod(1.0 - k /\\nnp.arange(n - c + 1, n + 1))\\nFigure 3. A numerically stable script for calculating an unbiased\\nestimate of pass@k.\\n\\nLater, we provide evidence that BLEU score may not be\\na reliable indicator of functional correctness by showing\\nthat functionally inequivalent programs generated by our\\nmodel (which are guaranteed to disagree with the reference\\nsolution on some input) often have higher BLEU scores than\\nfunctionally equivalent ones.\\n\\nEvaluating Large Language Models Trained on Code\\n\\n2.2. HumanEval: Hand-Written Evaluation Set\\n\\nWe evaluate functional correctness on a set of 164 hand-\\nwritten programming problems, which we call the Hu-\\nmanEval dataset. Each problem includes a function sig-\\nnature, docstring, body, and several unit tests, with an av-\\nerage of 7.7 tests per problem. It is important for these\\n\\ntasks to be hand-written, since our models are trained on a\\nlarge fraction of GitHub, which already contains solutions\\nto problems from a variety of sources. For example, there\\nare more than ten public repositories containing solutions to\\nCodeforces problems, which make up part of the recently\\nproposed APPS dataset (Hendrycks et al., 2021).\\n\\nProgramming tasks in the HumanEval dataset assess lan-\\nguage comprehension, reasoning, algorithms, and simple\\n\\nmathematics. We release the HumanEval dataset so that\\nothers can evaluate functional correctness and measure the\\nproblem-solving capabilities of their models. The dataset\\ncan be found at https://www.github.com/openai/human-eval.\\n2.3. Sandbox for Executing Generated Programs\\nSince publicly available programs have unknown intent and\\ngenerated programs are often incorrect, executing these\\nprograms poses a security risk. Indeed, GitHub is known\\nto contain malicious programs that alter or change their\\nenvironments (Rokon et al., 2020).\\nTherefore, we developed a sandbox environment to safely\\nrun untrusted programs against unit tests. Our goals were to\\nprevent these programs from modifying, gaining persistence\\non, accessing sensitive resources on, or exfiltrating data from\\na host or network. Since OpenAI’s training infrastructure\\nis built on Kubernetes and cloud services, we designed our\\nsandbox to address the limitations of these environments\\nwhile remaining idiomatic with their patterns of use.\\nWe selected the gVisor container runtime (Lacasse, 2018)\\nas the main host protection component. Since container\\n\\nruntimes like Docker can share host resources with contain-\\ners, a malicious container could potentially compromise a\\n\\nhost. gVisor protects the host by emulating its resources to\\n\\nintroduce a security boundary between the host and its con-\\ntainers. Network-adjacent hosts and services are protected\\n\\nby eBPF-based firewall rules that prevent inbound and out-\\nbound connections except for those required for experiment\\n\\ncontrol.\\n3. Code Fine-Tuning\\nWe fine-tune GPT models containing up to 12B parameters\\non code to produce Codex. In contrast with GPT, Codex\\ndisplays non-trivial performance on the HumanEval dataset.\\nIn fact, Codex is able to solve the majority of the problems\\nin HumanEval if we generate and evaluate 100 samples per\\n\\nproblem, and pick one that passes unit tests. When limited to\\na budget of one evaluation per problem, producing multiple\\nsamples with Codex and choosing the one with the highest\\nmean log-probability provides significant gains.\\n3.1. Data Collection\\n\\nOur training dataset was collected in May 2020 from 54 mil-\\nlion public software repositories hosted on GitHub, contain-\\ning 179 GB of unique Python files under 1 MB. We filtered\\n\\nout files which were likely auto-generated, had average line\\nlength greater than 100, had maximum line length greater\\nthan 1000, or contained a small percentage of alphanumeric\\ncharacters. After filtering, our final dataset totaled 159 GB.\\n3.2. Methods\\nSince Codex is evaluated on natural language prompts, we\\nhypothesized that it would be beneficial to fine-tune from\\nthe GPT-3 (Brown et al., 2020) model family, which already\\n\\ncontains strong natural language representations. Surpris-\\ningly, we did not observe improvements when starting from\\n\\na pre-trained language model, possibly because the fine-\\ntuning dataset is so large. Nevertheless, models fine-tuned\\n\\nfrom GPT converge more quickly, so we apply this strategy\\nfor all subsequent experiments.\\n\\nWe train Codex using the same learning rate as the corre-\\nsponding GPT model, with a 175 step linear warmup and\\n\\ncosine learning rate decay. We train for a total of 100 billion\\ntokens, using the Adam optimizer with β1 = 0.9, β2 = 0.95,\\n = 10−8\\n, and a weight decay coefficient of 0.1.\\nIn order to maximally leverage text representations from\\nGPT, we base our code lexer on the GPT-3 text tokenizer.\\nSince the distribution of words in GitHub code differs from\\nthat of natural text, this tokenizer is not very effective for\\nrepresenting code. The largest source of inefficiency arises\\nfrom encoding whitespace, so we add an additional set of\\ntokens for representing whitespace runs of different lengths.\\nThis allows us to represent code using approximately 30%\\nfewer tokens.\\n\\nTo compute pass@k, we assemble each HumanEval prob-\\nlem into a prompt consisting of a header, a signature, and\\n\\na docstring, which is illustrated in Figure 2. We sample\\ntokens from Codex until we encounter one of the following\\nstop sequences: ‘\\\\nclass’, ‘\\\\ndef’, ‘\\\\n#’, ‘\\\\nif’, or\\n\\n‘\\\\nprint’, since the model will continue generating addi-\\ntional functions or statements otherwise. We use nucleus\\n\\nsampling (Holtzman et al., 2020) with top p = 0.95 for all\\nsampling evaluation in this work.\\n3.3. Results\\nIn Figure 4, we plot test loss on a held-out validation set\\nagainst Codex model size. We find that just as language\\n\\nEvaluating Large Language Models Trained on Code\\n\\nFigure 4. Model cross-entropy test loss measured on a held-out\\nsplit of our Python GitHub code corpus. The smooth power law\\nscaling of performance with model size observed in GPT-3 appears\\nto hold even after code fine-tuning.\\n\\nmodel test loss follows a power law in model size (Kaplan\\net al., 2020), test loss after code fine-tuning follows a similar\\npower law with functional form (\\nN\\n5.92×107 )\\n−0.13 where N\\nis the number of non-embedding parameters in the model.\\n\\nWhen evaluating pass@k, it is important to optimize sam-\\npling temperature for the particular value of k. In Figure 5,\\n\\nwe plot pass@k against the number of samples k and the\\nsampling temperature. We find that higher temperatures are\\noptimal for larger k, because the resulting set of samples\\nhas higher diversity, and the metric rewards only whether\\nthe model generates any correct solution.\\n\\nIn particular, for a 679M parameter model, the optimal tem-\\nperature for pass@1 is T\\n\\n∗ = 0.2 and the optimal tempera-\\nture for pass@100 is T\\n\\n∗ = 0.8. With these temperatures,\\nwe find that pass@1 and pass@100 scale smoothly as a\\nfunction of model size (Figure 6).\\nPass@k can also be interpreted as the result of evaluating\\nthe best out of k samples, where the best sample is picked\\nby an oracle with prior knowledge of the unit tests. From\\n\\na practical perspective, we are also interested in the set-\\nting where we must select a single sample from k samples\\n\\nwithout having access to an oracle. For instance, when the\\nmodel is used as an autocomplete tool where a user provides\\na prompt, we do not have unit tests, but would like to return\\nonly a single completion to the user for evaluation so as to\\nnot overwhelm them.\\nInspired by similar work in language modeling, we find\\nthat choosing the sample with the highest mean token log\\nprobability outperforms evaluating a random sample, while\\n\\nchoosing the sample based on sum log probability can per-\\nform slightly worse than picking randomly. Figure 7 demon-\\nstrates the benefits of applying these heuristics to samples\\n\\n(at temperature 0.8) from Codex-12B.\\n\\nFigure 5. In the top panel, we plot pass@k against the number of\\nsamples (k) for various temperature settings. Higher temperatures\\nare better when the number of samples is large, likely due to the\\nincreased sample diversity. In the bottom panel, we plot the best\\ntemperature setting for each k, obtained by taking the upper hull\\nof the top panel.\\n\\nFigure 6. Using the optimal temperatures 0.2 and 0.8 for pass@1\\nand pass@100, we plot these two metrics as a function of model\\n\\nsize. Performance appears to scale smoothly as a sigmoid in log-\\nparameters.\\n\\nEvaluating Large Language Models Trained on Code\\n\\nFigure 7. Model performance in the setting where we can generate\\n\\nmultiple samples, but only evaluate one. We can do better than ran-\\ndomly selecting a sample by choosing the solution with the highest\\n\\nmean log-probability (red) or with the highest back-translation\\nscore (orange) described in Sec. 5. The blue line represents the\\ntheoretical best performance obtained using an oracle with prior\\nknowledge of the unit tests.\\n\\nFinally, we compute BLEU scores for all Codex-12B Hu-\\nmanEval samples (at temperature 0.8) against their reference\\n\\nsolutions. For each problem, when we plot the distributions\\nof BLEU scores for correct and incorrect solutions, we\\nnotice significant overlap (Figure 8). Since an incorrect\\nsolution is guaranteed to be functionally inequivalent to\\nthe reference solution, we conclude that improvements in\\nBLEU score may not indicate improved rates of functional\\ncorrectness in practice.\\n3.4. Comparative Analysis of Related Models and\\nSystems\\nTwo recent works similar in spirit to Codex are GPT-Neo\\n(Black et al., 2021) and GPT-J (Wang & Komatsuzaki,\\n2021), which are trained on The Pile (Gao et al., 2020),\\na dataset containing text from a variety of sources as well\\nas 8% GitHub code. The broader research community has\\nfound that these models outperform existing GPT systems\\nin qualitative programming evaluations (Woolf, 2021).\\nWe confirm these findings using the HumanEval dataset,\\nshowing that GPT-Neo achieves 6.4% pass@1 and 21.3%\\npass@100, while GPT models of comparable sizes achieve\\nnear 0% on both metrics. We see a remarkable progression\\nin capabilities, with GPT-Neo-2.7B roughly equivalent to\\nCodex-85M (30× fewer parameters). Similarly, GPT-J-6B\\nachieves 11.6% pass@1 and 27.7% pass@100, which is\\nroughly equivalent to Codex-300M (20× fewer parameters).\\n\\nPass rates are obtained by taking the best result from eval-\\nFigure 8. BLEU score probability densities for correct (blue) and\\n\\nwrong (green) solutions from Codex-12B for 4 random tasks from\\nHumanEval. Note that the distributions are not cleanly separable,\\nsuggesting that optimizing for BLEU score is not equivalent to\\noptimizing for functional correctness.\\nuating at temperatures 0.2, 0.4, and 0.8 for GPT-Neo, and\\nfrom temperatures 0.2 and 0.8 for GPT-J. Detailed results\\nacross multiple model sizes can be found in Table 1.\\nFinally, we benchmark Codex against the largest free model\\nfrom Tabnine, a leading code autocomplete system, which\\nachieves 2.6% pass@1 (at T = 0.4) and 7.6% pass@100\\n(at T = 0.8). This is roughly equivalent to Codex-12M, one\\nof the smallest models in our suite.\\n3.5. Results on the APPS Dataset\\nRecently, Hendrycks et al. (2021) introduced the APPS\\n\\ndataset to measure the coding challenge competence of lan-\\nguage models. The APPS dataset consists of 5000 training\\n\\nand 5000 test examples of coding problems, each with a set\\n\\nof unit tests and, for the training data, a set of correct solu-\\ntions. Most of the APPS tests problems are not formulated\\n\\nas single-function synthesis tasks, but rather as full-program\\nsynthesis, reading input from stdin and printing output to\\nstdout, in contrast to the main Codex training data.\\nIn the paper that introduces APPS, the authors benchmark a\\nfew language models and report two metrics: the percentage\\nof problems where the model finds a correct solution (called\\nthe “strict accuracy”) and the percentage of unit tests passed,\\n\\neven if the solution is incorrect. The latter measure is re-\\nported only so as to reduce variance of the measurements,\\n\\nbecause the results on the first metric were so low. We avoid\\nthis metric and only focus on “strict accuracy”, and - as in\\n\\nEvaluating Large Language Models Trained on Code\\n\\nTable 1. Codex, GPT-Neo, & TabNine evaluations for HumanEval.\\n\\nWe find that GPT-J pass@1 is between Codex-85M and Codex-\\n300M performance.\\n\\nPASS@k\\nk = 1 k = 10 k = 100\\nGPT-NEO 125M 0.75% 1.88% 2.97%\\nGPT-NEO 1.3B 4.79% 7.47% 16.30%\\nGPT-NEO 2.7B 6.41% 11.27% 21.37%\\nGPT-J 6B 11.62% 15.74% 27.74%\\nTABNINE 2.58% 4.35% 7.59%\\nCODEX-12M 2.00% 3.62% 8.58%\\nCODEX-25M 3.21% 7.1% 12.89%\\nCODEX-42M 5.06% 8.8% 15.55%\\nCODEX-85M 8.22% 12.81% 22.4%\\nCODEX-300M 13.17% 20.37% 36.27%\\nCODEX-679M 16.22% 25.7% 40.95%\\nCODEX-2.5B 21.36% 35.42% 59.5%\\nCODEX-12B 28.81% 46.81% 72.31%\\n\\nthe previous sections - we report pass@k numbers for vari-\\nous k (Table 2). There are 2 additional factors, well-known\\n\\nfrom coding competitions, that we take into account:\\n• In coding competitions and in the APPS datasets, tasks\\nare provided with 3 input/output examples included in\\nthe task description. We utilize this by sampling 1000\\nsolutions from the model and filtering out only those\\nthat pass these 3 unit tests (if such solutions exist). We\\nthen calculate pass rates in this filtered set, and call it\\nfiltered pass@k. Results without filtering are presented\\nas raw pass@k.\\n• It is often the case both in coding competitions and in\\nthe results from Codex that a correct solution is found,\\n\\nbut it is not algorithmically efficient enough to be con-\\nsidered passing. While this is not acceptable in the\\n\\ncompetitions, we also report the number of solutions\\nthat Codex produces that do not fail on any unit test,\\nbut that do time-out on some of them. We use a timeout\\nof 3 seconds in our evaluation.\\nTo compensate for the fact the Codex is not fine-tuned on\\nAPPS, we append a single input/output example from the\\n\\ntask description to the docstring as a formatting hint. We de-\\nnote this setting as “1-shot” in Table 2, and find that Codex-\\n12B evaluated 1-shot achieves comparable performance to a\\n\\nGPT-Neo model fine-tuned on APPS. Consistent with our\\nearlier findings, there are large benefits from generating and\\nevaluating as many as 1000 samples per task, though for\\nmore difficult problems, solutions are often not efficient\\nenough to pass the time limits. Finally, evaluating the first\\nsample which passes the 3 public unit tests for each problem\\nyields higher performance than raw pass@100 samples.\\n\\n4. Supervised Fine-Tuning\\nIn addition to standalone functions, Python code found on\\nGitHub contains class implementations, configuration files,\\n\\nscripts, and even files used to store data. This code is seem-\\ningly unrelated to synthesizing functions from docstrings,\\n\\nand we hypothesize that the distribution mismatch reduces\\nHumanEval performance.\\n\\nIn order to adapt Codex to the distribution of the task of in-\\nterest, we construct a set of training problems from correctly\\n\\nimplemented standalone functions, and use them for addi-\\ntional supervised fine-tuning. We describe two approaches\\n\\nfor collecting these examples: from competitive program-\\nming websites and from repositories with continuous inte-\\ngration. We call the supervised fine-tuned models Codex-S,\\n\\nand show that they produce consistent gains across model\\nsize.\\n4.1. Problems from Competitive Programming\\nProgramming contest and interview preparation websites\\n\\nuse hidden unit tests to automatically judge the func-\\ntional correctness of submissions. These problems are self-\\ncontained, come with well-written problem statements, and\\n\\ngenerally have excellent test coverage. Additionally, these\\nproblems test algorithmic reasoning over a broad range of\\ncore skills and difficulties.\\nWe collected problem statements, function signatures, and\\nsolutions from several popular programming contest and\\ninterview preparation websites. We then assembled these\\ninto programming tasks similar to HumanEval, using the\\nproblem description as the docstring. Since complete test\\nsuites are often hidden, we created unit tests from examples\\nfound in the problem statements, or extracted additional test\\ncases through submitting incorrect solutions. In total, we\\ncurated 10,000 problems in this way.\\n4.2. Problems from Continuous Integration\\nNext, we curated programming problems from open source\\nprojects. Taking advantage of sys.setprofile, we\\nwere able to trace and collect inputs and outputs for all\\nfunctions called during integration tests. This data could\\nthen be used to create unit tests for the functions.\\nProjects that employ continuous integration (CI) are ideal\\ncandidates for tracing. We follow the commands in the CI\\nconfiguration files, which contain build and test commands,\\nto set up the virtual environments, install dependencies, and\\nrun integration tests.\\nWe considered GitHub repos using travis and tox as their CI\\nframeworks, as they are two of the most popular CI tools.\\nWe additionally used publicly available source code from\\npip packages found in the python package index (PyPI).\\n\\nEvaluating Large Language Models Trained on Code\\n\\nTable 2. Finetuned GPT-Neo numbers from the APPS paper referenced above. For Codex-12B, the number of passing programs that\\ntimeout on some test is in the bracket. We used temperature 0.6 for sampling to cover all k in pass@k, so raw pass@1 results could be\\nimproved with lower temperature.\\n\\nINTRODUCTORY INTERVIEW COMPETITION\\nGPT-NEO 2.7B RAW PASS@1 3.90% 0.57% 0.00%\\nGPT-NEO 2.7B RAW PASS@5 5.50% 0.80% 0.00%\\n1-SHOT CODEX RAW PASS@1 4.14% (4.33%) 0.14% (0.30%) 0.02% (0.03%)\\n1-SHOT CODEX RAW PASS@5 9.65% (10.05%) 0.51% (1.02%) 0.09% (0.16%)\\n1-SHOT CODEX RAW PASS@100 20.20% (21.57%) 2.04% (3.99%) 1.05% (1.73%)\\n1-SHOT CODEX RAW PASS@1000 25.02% (27.77%) 3.70% (7.94%) 3.23% (5.85%)\\n1-SHOT CODEX FILTERED PASS@1 22.78% (25.10%) 2.64% (5.78%) 3.04% (5.25%)\\n1-SHOT CODEX FILTERED PASS@5 24.52% (27.15%) 3.23% (7.13%) 3.08% (5.53%)\\n\\nBecause these projects contained untrusted code, it was im-\\nportant to run integration tests in the sandboxed environment\\n\\ndescribed above.\\nWhile there are millions of potential functions to curate\\nproblems from, we only collected about 40,000 because\\nnot all functions accept inputs and return outputs. Even\\nwhen they do, most objects captured at runtime cannot be\\npickled and restored outside the sandbox unless the project\\nwas installed.\\nSince our tracing methodology produced inputs and outputs\\n\\nfor all invoked functions, even builtin and library calls im-\\nported by the project were turned into problems. For this\\n\\nreason, functions from tracing tended to be the building\\nblocks of command-line utilities. To excel at these tasks,\\nthe model does not need to know advanced algorithms and\\n\\ndata structures. Rather, it needs to be able to follow in-\\nstructions to implement the functionality specified in the\\n\\ndocstring. Thus, tracing complements the puzzle nature of\\ncoding competition problems and broadens the distribution\\nof tasks.\\n4.3. Filtering Problems\\nIn the previous sections, we presented two methods we\\nused to automatically create training problems. However,\\nit is unclear how to control for quality. Some prompts\\nunderspecify the function that is implemented, in which\\ncase a perfectly valid solution may be wrongly penalized by\\nthe unit test. Some problems are stateful, and subsequent\\nexecutions can result in different outcomes.\\nTo address these issues, we use Codex-12B to generate 100\\nsamples per curated problem. If no samples pass the unit\\ntests, we consider the task to be either ambiguous or too\\ndifficult, and filter it out. We reran this verification several\\ntimes to remove stateful or non-deterministic problems.\\n\\n4.4. Methods\\nWe fine-tune Codex on these training problems to produce a\\n\\nset of “supervised fine-tuned” models, which we call Codex-\\nS. To produce examples from training problems, we assem-\\nble the problems into the format shown in Figure 2. If there\\n\\nare prompts of varying length in a batch, we left-pad shorter\\nprompts to the length of the longest prompt, so that the first\\ntokens in the reference solutions line up in context.\\nWe train to minimize negative log-likelihood of the reference\\nsolution, and mask out loss for any tokens in the prompt.\\nWe train using a learning rate 1/10 as large as used for\\nfine-tuning Codex, but adhere to the same learning rate\\nschedule, and train until validation loss plateaus (less than\\n10B tokens).\\n4.5. Results\\nAs with Codex, we first compute the optimal temperature for\\nevaluating pass@k for 1 ≤ k ≤ 100. We find that Codex-S\\nprefers slightly higher temperatures for all k > 1, which\\npossibly reflects the fact that Codex-S captures a narrower\\ndistribution than Codex. We use T\\n\\n∗ = 0 for computing\\n\\npass@1 and T\\n\\n∗ = 1 for computing pass@100.\\nNext, we compare Codex-S against Codex on pass@1 and\\npass@100. Codex-S outperforms the corresponding Codex\\nby an average margin of 6.5 percentage points on pass@1\\nand by a larger average margin of 15.1 percentage points on\\npass@100 across model size.\\nWe also plot the performance of different sample selection\\nheuristics for Codex-S-12B against the same heuristics for\\nCodex-12B. When ranking between 1 and 100 samples\\nby mean log probability, the average benefit over random\\nranking is 11.6 percentage points, which is over 2 percentage\\npoints higher than the corresponding benefit for Codex.\\n\\nEvaluating Large Language Models Trained on Code\\n\\nFigure 9. Optimal sampling temperatures as a function of the num-\\nber of samples generated for both Codex and Codex-S. Codex-S\\n\\ngenerally requires a higher temperature for any particular value of\\nk, possibly to compensate for the fact that it models a narrower\\ndistribution.\\n\\nFigure 10. Comparing Codex-S against Codex on the metrics pro-\\nposed in Section 3. Codex-S is one or two orders of magnitude\\n\\nmore parameter efficient on pass@1 and pass@100, and log-prob\\nsample ranking with Codex-S yields similar benefits over random\\nsampling that Codex does.\\n\\n5. Docstring Generation\\nGenerating code from docstrings is possible with Codex\\nbecause code typically follows after a docstring, but it is not\\n\\neasy to induce Codex to generate docstrings from code. Nev-\\nertheless, we are motivated to produce a docstring writing\\n\\nmodel for safety reasons, as such a model can be used to de-\\nscribe the intent behind generated code. Using the training\\n\\nproblems described in the previous section, we can eas-\\nily create a training dataset for code-conditional docstring\\n\\ngeneration.\\n\\nSpecifically, for each training problem, we assemble a train-\\ning example by concatenating the function signature, the\\n\\nreference solution, and then the docstring. Just as we train\\n\\nCodex-S by minimizing negative log-likelihood of the ref-\\nerence solution, we train the docstring generating models\\n\\nCodex-D by minimizing negative log-likelihood of the doc-\\nstring.\\n\\nWhen we benchmark our code generation models, we mea-\\nsure pass@k on the HumanEval dataset, where correctness\\n\\nis defined by passing a set of unit tests. However, there is\\nno similar way to evaluate docstring samples automatically.\\nTherefore, we grade sample docstrings by hand, considering\\na docstring correct if it uniquely and accurately specifies\\nthe code body. Due to the time consuming nature of this\\nprocess, we only grade 10 samples per problem, for a total\\nof 1640 problems, from Codex-D-12B at temperature 0.8.\\nCodex-D often generates incorrect unit tests along with a\\ndocstring, but we ignore these during grading. However,\\nwe do not consider the docstring correct when the model\\nsimply copies the code body into the docstring. The most\\ncommon failure modes we observe are when the docstring\\nmodel leaves out an important detail (such as “an answer\\nmust be to two decimal places”) or when it over-conditions\\non the function name and invents a problem unrelated to the\\nfunction body.\\nAs shown in Table 3, pass rates for Codex-D are lower but\\ncomparable to the corresponding pass rates for Codex-S at\\nthe same temperature. We do not have a strong hypothesis\\nfor which direction should yield higher pass rates. While\\n\\ngenerating docstrings may be more forgiving because natu-\\nral language syntax is less strict than code syntax, docstrings\\n\\nin our dataset may be lower quality because developers tend\\nto devote less time to writing docstrings. Indeed, our model\\nproduces docstrings like “I just found this function online”\\n\\nand “This test is not correctly written and it’s not my solu-\\ntion.”\\n\\nFinally, with a docstring model, we have yet another way\\n\\nto choose a single sample from a set of k samples. In-\\nstead of picking the sample with the best mean log proba-\\nbility as investigated in the previous two sections, we can\\n\\nchoose the sample that maximizes the back-translation ob-\\n\\nEvaluating Large Language Models Trained on Code\\n\\nTable 3. Pass rates for our docstring generating model Codex-D,\\nwhich is evaluated by hand-grading 10 samples per task due to the\\nlack of a ground-truth automatic evaluation. We find similar but\\nlower pass-rates compared to Codex-S.\\nMODEL PASS@1 PASS@10\\nCODEX-S-12B 32.2% 59.5%\\nCODEX-D-12B 20.3% 46.5%\\n\\njective P(ground truth docstring|generated sample) where\\nP is evaluated using Codex-D. Unfortunately, in Figure 7,\\n\\nwe show that ranking samples via back-translation under-\\nperforms mean log-probability ranking, though it outper-\\nforms random ranking. This heuristic also appears to overfit\\n\\nquickly.\\n6. Limitations\\nWhile Codex is able to sample correct solutions for the\\nmajority of HumanEval problems, we find that it has a\\nnumber of limitations.\\nFirst, Codex is not sample efficient to train. Our training\\ndataset comprises a significant fraction of publicly available\\nPython code on GitHub, totaling hundreds of millions of\\nlines of code. Even seasoned developers do not encounter\\n\\nanywhere near this amount of code over their careers. In-\\ndeed, a strong student who completes an introductory com-\\nputer science course is expected to be able to solve a larger\\n\\nfraction of problems than Codex-12B.\\nNext, we explore prompts on which Codex is likely to fail\\nor display counter-intuitive behavior. While evaluating code\\n\\ngeneration is well-studied (Xu et al., 2021; Helmuth & Spec-\\ntor, 2015; Pantridge et al., 2017), many existing metrics\\n\\nmeasure performance in tightly specified, constrained prob-\\nlem instances (e.g., string manipulation in FlashFill (Gul-\\nwani, 2011)). Therefore, we developed a set of qualitative\\n\\nmetrics for measuring the capabilities of code generating\\n\\nmodels while controlling for the complexity and abstrac-\\ntion level of the specifications (Appendix D). Applying this\\n\\nframework, we find that Codex can recommend syntacti-\\ncally incorrect or undefined code, and can invoke functions,\\n\\nvariables, and attributes that are undefined or outside the\\nscope of the codebase. Moreover, Codex struggles to parse\\nthrough increasingly long and higher-level or system-level\\nspecifications.\\nTo concretely illustrate model performance degradation as\\ndocstring length increases, we create a dataset of synthetic\\nproblems assembled from 13 basic building blocks, each of\\n\\nwhich modifies an input string in a deterministic way. Ex-\\nample building blocks are “convert the string to lowercase”\\n\\nor “remove every third character from the string” (the full\\n\\nlist is described in Appendix C). We find that as the number\\nof chained building blocks in the docstring increases, model\\n\\nperformance decreases exponentially. This behavior is un-\\ncharacteristic of a human programmer, who should be able\\n\\nto correctly implement a program for a chain of arbitrary\\nlength if they can do so for a chain of length two.\\n\\nFigure 11. Pass rates of Codex-12B samples against the number of\\nchained components in the synthetically generated docstring. With\\neach additional component, pass rate drops by roughly a factor of\\n2-3.\\nFurther, just as text-conditional generative models in other\\n\\nmodalities (Ramesh et al., 2021) have difficulty with bind-\\ning attributes to objects, Codex can make mistakes binding\\n\\noperations to variables, especially when the number of oper-\\nations and variables in the docstring is large. For instance,\\n\\nin the following prompt, Codex-12B does not decrement the\\nvariable w and also fails to return the product of all numbers.\\ndef do_work(x, y, z, w):\\n\"\"\" Add 3 to y, then subtract 4\\nfrom both x and w. Return the\\nproduct of the four numbers. \"\"\"\\nt = y + 3\\nu = x - 4\\nv = z * w\\nreturn v\\n\\nThis understanding of Codex’s limited system-level synthe-\\nsis capabilities helps inform our assessment of the potential\\n\\nhazards of using it in a generative capacity, as well as the\\nbroader societal impacts that such systems could have.\\n7. Broader Impacts and Hazard Analysis\\nCodex has the potential to be useful in a range of ways.\\nFor example, it could help onboard users to new codebases,\\nreduce context switching for experienced coders, enable\\nnon-programmers to write specifications and have Codex\\ndraft implementations, and aid in education and exploration.\\nHowever, Codex also raises significant safety challenges,\\ndoes not always produce code that is aligned with user intent,\\n\\nEvaluating Large Language Models Trained on Code\\n\\nand has the potential to be misused.\\nTo better understand some of the hazards of using Codex\\nin a generative capacity, we conducted a hazard analysis\\nfocused on identifying risk factors (Leveson, 2019) with\\nthe potential to cause harm.1 We outline some of our key\\nfindings across several risk areas below.\\nWhile some of our findings about the potential societal\\nimpacts of code generation systems were informed by work\\ntowards responsible deployment of the production-oriented\\nCodex models (which descended from the research-oriented\\nCodex models described in this paper), this section is not\\nintended to provide a full account of any particular product’s\\nsafety features. Unless otherwise specified, we anchor our\\nanalysis in the specific properties of the models described\\nin this paper. We share this analysis in the belief that some\\nof it generalizes to the broader class of code generation\\nsystems, and to encourage a norm of performing detailed\\nimpact analysis as part of major machine learning research\\nprojects.\\nNote that by focusing largely on risks in this section, we do\\nnot mean to imply that we expect the impact of this class of\\ntechnologies to be net-negative; rather, risks merit particular\\n\\nattention here because they may be subtle or require deliber-\\nate effort to address, whereas we expect the benefits to be\\n\\nmore obvious and “automatic” from the perspective of most\\nusers and affected stakeholders.\\n7.1. Over-reliance\\nOne of the key risks associated with using code generation\\nmodels in practice is over-reliance on generated outputs.\\nDue to the limitations described above as well as alignment\\nissues described below, Codex may suggest solutions that\\nsuperficially appear correct but do not actually perform the\\ntask the user intended. This could particularly affect novice\\nprogrammers, and could have significant safety implications\\ndepending on the context. We discuss a related issue in\\n\\nAppendix G, namely that code generation models can sug-\\ngest insecure code. For these reasons, human oversight and\\n\\nvigilance is required for safe use of code generation systems\\nlike Codex.\\nWe note several immediate ways to improve safety in the\\nsubsection on risk mitigation below, though over-reliance\\nin particular is one that we believe merits further inquiry\\n\\nin industry and academia. While it is conceptually straight-\\n1We sought to include harms spanning geographic and temporal\\n\\nscales. We also considered not only the severity and probability,\\nbut also the distribution of harms. However, we note that the\\nanalysis described here is only one milestone in what we hope will\\nbe a larger cross-sectoral and cross-organizational effort to steer\\ncode generation in a societally beneficial direction. As we describe\\nour findings, we note various specific uncertainties and areas for\\nfuture work in different sections.\\n\\nFigure 12. When the prompt includes subtle bugs, Codex tends to\\nproduce worse code than it is capable of. This persists when the\\nprompt also includes instructions to write correct code. This gap\\nincreases with model size.\\nforward to provide documentation to users reminding them\\n\\nabout model limitations, empirical investigation is neces-\\nsary in order to identify how to reliably ensure vigilance in\\n\\npractice across a range of user experience levels, UI designs,\\nand tasks. One challenge researchers should consider is that\\nas capabilities improve, it may become increasingly difficult\\nto guard against “automation bias.”\\n7.2. Misalignment\\nAs with other large language models trained on a next-token\\n\\nprediction objective, Codex will generate code that is as sim-\\nilar as possible to its training distribution. One consequence\\n\\nof this is that such models may do things that are unhelpful\\nfor the user, despite having the capability to be more helpful\\n(see Figure 12). For example, if the user has some subtle\\nmistakes in their code, Codex may “deliberately” suggest\\ncode that superficially appears good but is incorrect.\\nThis is an alignment failure - the model is not aligned with\\nthe user’s intentions. Informally, a system is misaligned if\\nthere’s some task X that we want it to do, and it is “capable”\\nof doing X but “chooses” not to. In contrast, if a system\\nfails to do X because it does not have the ability to do so,\\nthen this system is not misaligned; it is just incompetent.\\nSee Appendix E for more detail, including a more precise\\ndefinition of alignment.\\nIt is important to study misalignment because it is a problem\\n\\nthat is likely to become worse, not better, as the capabili-\\nties of our systems increase. For example, the model size\\n\\nscaling trend for the example in Figure 12 indicates that\\nmisalignment would likely persist and even get worse if\\ndata, parameters, and training time were scaled up.\\n\\nWhile we expect that misaligned behaviour like this is un-\\nlikely to cause significant harm in current models, it is likely\\n\\nto become more dangerous and harder to eliminate as model\\n\\nEvaluating Large Language Models Trained on Code\\n\\ncapabilities increase. A highly capable but sufficiently mis-\\naligned model trained on user approval might produce ob-\\nfuscated code that looks good to the user even on careful\\n\\ninspection, but in fact does something undesirable or even\\nharmful.\\n7.3. Bias and representation\\nMirroring what has been found in the case of other language\\n\\nmodels trained on Internet data (Bender et al., 2021; Blod-\\ngett et al., 2020; Abid et al., 2021; Brown et al., 2020), we\\n\\nfound that Codex can be prompted in ways that generate\\nracist, denigratory, and otherwise harmful outputs as code\\ncomments, meriting interventions such as those discussed\\nin the subsection on risk mitigation below. We also found\\n\\nthat code generation models raise further bias and represen-\\ntation issues beyond problematic natural language: Codex\\n\\ncan generate code with structure that reflects stereotypes\\nabout gender, race, emotion, class, the structure of names,\\nand other characteristics. Particularly in the context of users\\n\\nwho might over-rely on Codex or use it without first think-\\ning through project design, this issue could have significant\\n\\nsafety implications, giving further motivation to discourage\\nover-reliance. We discuss bias and representation issues\\nfurther in Appendix F. Filtration or modulation of generated\\noutputs, documentation, and other interventions may help\\nto mitigate these risks.\\n7.4. Economic and labor market impacts\\nCode generation and associated capabilities have several\\npossible economic and labor market impacts. While Codex\\nat its current capability level may somewhat reduce the cost\\n\\nof producing software by increasing programmer produc-\\ntivity, the size of this effect may be limited by the fact that\\n\\nengineers don’t spend their full day writing code (O*NET,\\n\\n2021). Other important tasks include conferring with col-\\nleagues, writing design specifications, and upgrading ex-\\nisting software stacks.2 We also found that Codex imports\\n\\npackages at different rates, which could advantage some\\npackage authors over others, particularly if programmers\\nand engineers come to rely on Codex’s suggestions. Over a\\nlonger time horizon, the effects of this class of technologies\\non software-related labor markets and on the economy more\\ngenerally could be more substantial as capabilities improve.\\n\\nMore study is needed both on the effects of code genera-\\ntion capabilities and on appropriate responses. We discuss\\n\\neconomic and labor market implications in more detail in\\nAppendix H.\\n2\\nIndeed, BLS classifies computer programmers and software\\ndevelopers separately, where developers are more highly paid than\\nprogrammers, have more tasks indirectly related to writing and\\ninteracting with code, and, in the US, are already projected to see\\ngreater demand over the next 10 years (Li et al., 2020; Bureau of\\nLabor Statistics, 2021a;b).\\n\\n7.5. Security implications\\nCodex could have various effects on the security landscape.\\nBecause Codex can produce vulnerable or misaligned code,3\\n\\nqualified operators should review its generations before ex-\\necuting or trusting them, absent appropriate precautions.\\n\\nFuture code generation models may be able to be trained\\nto produce more secure code than the average developer,\\nthough that is far from certain.\\nCodex could also be misused to aid cybercrime. Although\\nthis is worthy of concern, based on our testing, we believe\\nthat at their current level of capability, Codex models do\\n\\nnot materially lower the barrier to entry for malware devel-\\nopment.4 We expect that more powerful code generation\\n\\nmodels will lead to future advancements, and therefore fur-\\nther research into mitigations and continued study of model\\n\\ncapabilities are necessary.\\nThe non-deterministic nature of systems like Codex could\\nenable more advanced malware. This non-determinism\\nmakes it easier to create diverse software that accomplish\\nthe same tasks. While software diversity can sometimes\\naid defenders,5\\n\\nit presents unique challenges for traditional\\n\\nmalware detection and antivirus systems that rely on finger-\\nprinting and signature-matching against previously sampled\\n\\nbinaries. For example, a more capable code generation\\nmodel could conceivably advance techniques for generating\\n\\npolymorphic malware.6 We believe that application secu-\\nrity and model deployment strategies including rate-limiting\\n\\naccess and abuse monitoring can manage this threat in the\\nnear term; however, the efficacy of these mitigations may\\nscale sublinearly as more capable models are developed.\\nSimilar to large language models, Codex models can learn\\npatterns present in their training data (Carlini et al., 2021).\\n\\nSensitive data present in source code are liable to be pre-\\ndicted by the model. Because Codex is trained on public\\n\\nrepositories, we consider any sensitive data present in the\\ntraining data to have already been compromised. Similarly,\\nthe public data should generally be treated as untrusted, as\\nprevious work (Goldblum et al., 2021; Schuster et al., 2020)\\nhas found that attackers may be able to corrupt training data\\nto trigger specific model behaviors at runtime. We further\\ndiscuss security implications in Appendix G.\\n3\\n\\nSee Appendix G - Insecure Code for examples of Codex pro-\\nducing insecure code.\\n\\n4\\nFor more on characterizing Codex’s capability limitations, see\\nthe Limitations section and experiments in the security analysis in\\nAppendix G.\\n5\\nFor example, by helping to prevent certain types of memory\\ncorruption vulnerabilities. See (Davis, 2018) for more.\\n6\\n\\nPolymorphic malware is malicious code that mutates its im-\\nplementation while maintaining its function.\\n\\nEvaluating Large Language Models Trained on Code\\n\\n7.6. Environmental impacts\\nCodex, like other large generative models, has an energy\\nfootprint from both training and inference (Schwartz et al.,\\n\\n2019; Bender et al., 2021; Patterson et al., 2021). The origi-\\nnal training of GPT-3-12B consumed hundreds of petaflop/s-\\ndays of compute, while fine-tuning it to create Codex-12B\\n\\nconsumed a similar amount of compute. This training was\\nperformed on a platform (Azure) that purchases carbon\\ncredits and sources significant amounts of renewable energy,\\nreducing its carbon footprint.7 Compute consumption also\\n\\nhas costs in the wider supply chain that can be quite con-\\ncentrated on certain regions.8 Looking more globally and\\n\\nlong-term, the compute demands of code generation could\\ngrow to be much larger than Codex’s training if significant\\ninference is used to tackle challenging problems.9\\n7.7. Legal implications\\nThere are several legal considerations related to generated\\ncode. To begin with, the training of AI systems on Internet\\ndata, such as public GitHub repositories, has previously\\nbeen identified as an instance of “fair use” (O’Keefe et al.,\\n2019).\\nOur preliminary research also finds that Codex models rarely\\ngenerate code that is identical to the contents of training\\ndata. Such occurrences were < 0.1% in a study examining\\nthe frequency of code generations that appear to match code\\nsnippets in the training data (Ziegler, 2021). In these rare\\n\\ninstances, the generated code consisted of common expres-\\nsions or conventions within the programming language that\\n\\nappeared over and over again in the training data. We find\\nthat, to the extent the generated code appears identical to\\nthe training data, it is due to the predictive weightings in the\\nmodel rather than retention and copying of specific code.\\nGenerated code is also responsive and customized to the\\nuser’s input, and the user retains complete control over\\nediting and acceptance of the generated code. This can make\\ncode generation similar to auto-suggest or auto-completion\\n\\n7Microsoft made a commitment in 2020 to shift to 100 per-\\ncent renewable energy supply in its buildings and data centers\\n\\nby 2025. https://blogs.microsoft.com/blog/2020/01/16/microsoft-\\nwill-be-carbon-negative-by-2030/ A full assessment of the envi-\\nronmental impact of compute use is impossible to conduct without\\n\\ngrounding in context and making comparison to the counterfactual\\nimpacts of competing products or services. Such analysis is out of\\nscope for this paper.\\n\\n8While data center energy usage has become much more effi-\\ncient in recent years (Masanet et al., 2020), the production, use,\\n\\nand disposal of semiconductors still imposes environmental and\\nhuman costs. See, e.g., (Crawford, 2021)\\n9Given that code generation (and other forms of AI) might be\\ndeployed widely throughout the economy as discussed above, these\\nconsiderations suggest additional urgency in adopting renewable\\nenergy.\\n\\nfeatures that exist as features of other tools of authorship\\n(e.g., document editors), in the sense that the finished work\\nis still seen as the author’s.\\n\\nOur commitment to responsible and safe AI includes con-\\ntinued attention to the broader intellectual property impli-\\ncations of code generation systems. We intend to remain\\n\\nengaged with policymakers and experts on these issues so\\nthat the users of such systems can ultimately deploy them\\nwith confidence.\\n7.8. Risk mitigation\\nIn closing, given the above, models like Codex should be\\ndeveloped, used, and their capabilities explored carefully\\n\\nwith an eye towards maximizing their positive social im-\\npacts and minimizing intentional or unintentional harms that\\n\\ntheir use might cause. A contextual approach is critical to\\neffective hazard analysis and mitigation, though a few broad\\ncategories of mitigations are important to consider in any\\ndeployment of code generation models.\\n\\nCareful documentation and user interface design, code re-\\nview requirements, and/or content controls (e.g., filtering\\n\\nof outputs) may help to reduce harms associated with over-\\nreliance as well as offensive content or insecure code gener-\\nation. In the context of a model made available as a service\\n\\n(e.g., via an API), policies such as user review, use case\\nrestrictions, monitoring, and/or rate limiting may also help\\nto reduce harms associated with malicious use or prevent\\nits use in high-stakes domains for which the models are not\\nwell suited.\\nAppendices E, F, G, and H provide further detail on the risks\\ndescribed in this section and outline additional mitigation\\nand research opportunities.\\n8. Related Work\\nThe deep learning resurgence has led to strong advances in\\nthe field of program learning. Two popular approaches to\\nneural program learning are program induction and program\\nsynthesis.\\nIn program induction, a model generates program outputs\\ndirectly from a latent program representation. Learning to\\nExecute (Zaremba & Sutskever, 2014) demonstrated that\\n\\nmodels could execute simple tasks like addition and memo-\\nrization. Later attempts at program induction incorporated\\n\\ninductive biases based on modern computing devices, such\\nas the Neural Turing Machine (Graves et al., 2014), memory\\nnetworks (Weston et al., 2015; Sukhbaatar et al., 2015), the\\n\\nNeural GPU (Kaiser & Sutskever, 2015), and the differen-\\ntiable neural computer (Graves et al., 2016). More recent\\n\\napproaches like the Neural Program Interpreter (Reed &\\nde Freitas, 2016; Shin et al., 2018; Pierrot et al., 2021) and\\n\\nEvaluating Large Language Models Trained on Code\\n\\nUniversal Transformer (Dehghani et al., 2019) found recur-\\nrence to be a useful component in program induction.\\n\\nIn program synthesis, a model explicitly generates a pro-\\ngram, usually from a natural language specification. One\\n\\nof the most popular classical approaches used a probabilis-\\ntic context free grammar (PCFG) to generate a program’s\\n\\nabstract syntax tree (AST). Maddison & Tarlow (2014) im-\\nproved on this setup by learning a state vector used to con-\\ndition child node expansion. Later, Allamanis et al. (2015)\\n\\napplied this idea in text-to-code retrieval and Yin & Neu-\\nbig (2017) utilized it in text-conditional code generation.\\n\\nCode2seq (Alon et al., 2018) found that ASTs could also be\\nleveraged for code-to-text generation.\\nPrograms can also be synthesized without passing through\\nan AST representation. Hindle et al. (2012) investigated\\nn-gram language models of code, finding code to be more\\n\\npredictable than natural language. Latent Predictor Net-\\nworks (Ling et al., 2016) showed that character-level lan-\\nguage models could generate working code for implement-\\ning Magic the Gathering cards in an online arena, when\\n\\naided with a latent mode that allows card attributes to be\\ncopied into code. DeepCoder (Balog et al., 2017) trained\\na model to predict the functions appearing in source code,\\nwhich could be used to guide program search.\\n\\nFollowing the success of large natural language models (De-\\nvlin et al., 2018; Radford et al., 2019; Liu et al., 2019; Raffel\\n\\net al., 2020; Brown et al., 2020) large scale Transformers\\n\\nhave also been applied towards program synthesis. Code-\\nBERT (Feng et al., 2020) trained the BERT objective on\\n\\ndocstrings paired with functions, and obtained strong results\\non code search. PyMT5 (Clement et al., 2020) is similar in\\n\\nspirit to our work, and used the T5 objective to train a sys-\\ntem which can translate between non-overlapping subsets\\n\\nof {signature, docstring, body}.\\nWe used functional correctness to benchmark our models,\\n\\nand observed improvements on this metric with more sam-\\npling. SPoC (Kulal et al., 2019) considered the problem\\n\\nof producing functionally correct code from pseudocode\\nwith a fixed budget of compilations, which is similar to our\\npass@k metric. TransCoder (Lachaux et al., 2020) trained\\na system to translate between programming languages in\\nan unsupervised manner, and also observed that functional\\ncorrectness better captured the capabilities of their model\\nthan BLEU score. In fact, ContraCode (Jain et al., 2020)\\nleveraged the large space of functionally correct programs\\nto train a contrastive code model, which improved model\\n\\nperformance on tasks like type inference. Finally, Robust-\\nFill (Devlin et al., 2017) observed that the best way to find\\n\\na program consistent with input examples was to synthesize\\nmultiple samples through beam search.\\n\\nTwo early domain-specific datasets used to benchmark neu-\\nral programming systems were FlashFill (Gulwani, 2011;\\n\\nGulwani et al., 2012) and Hearthstone (Ling et al., 2016),\\nthough the community has trended towards broader and\\nmore difficult datasets. Barone & Sennrich (2017) proposed\\na large training and evaluation dataset consisting of Python\\ndeclarations, docstrings, and bodies scraped from GitHub.\\nThe CodeSearchNet challenge (Husain et al., 2019) built\\nan even larger corpus from GitHub with data from multiple\\npopular programming languages. Recently, CodeXGLUE\\n\\n(Lu et al., 2021) aggregated several programming bench-\\nmarks, making use of the recently proposed CodeBLEU\\n\\nmetric (Ren et al., 2020). Most relevant to our evaluation\\nwork is the APPS (Hendrycks et al., 2021) benchmark for\\nmeasuring functional correctness based on problems from\\nthe competitive programming website Codeforces.\\n\\nFinally, we note that coding is a broad activity which in-\\nvolves much more than synthesizing code from docstrings.\\n\\nTufano et al. (2020) use Transformers to generate unit tests\\nfor code which outperformed commercial offerings. Aye\\n\\net al. (2021) built an internal auto-complete tool for Face-\\nbook, and found that training on accepted user completions\\n\\nboosted system performance. Development also entails lo-\\ncating and fixing bugs. Early works used static or dynamic\\n\\ncode analysis (Agrawal et al., 1995; Korel & Rilling, 1997),\\nlearned association rules (Jeffrey et al., 2009), and genetic\\nprogramming (Goues et al., 2012) to debug faulty code.\\nThese approaches relied on running against a test suite to\\nnot only evaluate the correctness of suggestions but also\\nexpose problems in execution trace or search for a solution.\\nMore recent works (Tufano et al., 2019; Drain et al., 2021)\\nconsidered bug-fixing as neural machine translation from\\nbuggy to correct programs. However, these works used an\\n\\nexact match against a reference instead of functional cor-\\nrectness, citing Qi et al. (2015)’s finding that most of the\\n\\nproposed solutions by genetic search in (Goues et al., 2012)\\npassed through weak test suites by deleting functionality\\nthat failed. Human developers often write test suites with\\nlimited but targeted coverage, but this does not always work\\nwell against an algorithm, highlighting the challenges of\\nevaluating correctness of programs.\\n9. Conclusion\\n\\nWe investigated whether it was possible to train large lan-\\nguage models to produce functionally correct code bodies\\n\\nfrom natural language docstrings. By fine-tuning GPT on\\ncode from GitHub, we found that our models displayed\\nstrong performance on a dataset of human-written problems\\nwith difficulty level comparable to easy interview problems.\\nModel performance could be improved by training on a\\ndistribution more similar to the evaluation set, and also by\\nproducing multiple samples from a model. We also found\\nthat it was simple to train a model to complete the reverse\\n\\nEvaluating Large Language Models Trained on Code\\n\\ntask of producing docstrings from code bodies, and that the\\nperformance profiles of these models were similar. Finally,\\nwe expanded on the broader impacts of code generating\\nmodels, and discussed model limitations, finding significant\\nroom for improvement.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample keywords generated by LLaMA 70b chat. These keywords are generated by LLaMA 70b chat model based on the prompts given to the model. Given below is complete list of sample keywords generated for the research papers in our test case."
      ],
      "metadata": {
        "id": "x2s8XPTRymTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "\"What determines feelings of belonging and majoring in an academic field? Isolating factors by comparing psychology and philosophy\": [\n",
        "\"Gender Gaps\",\n",
        "\"Academic Belonging\",\n",
        "\"Personality Profiles\",\n",
        "\"Career Priorities\",\n",
        "\"Intellectual Combativeness\",\n",
        "\"Systematizing\",\n",
        "\"Empathizing\",\n",
        "\"Money and Status\",\n",
        "\"Family Friendly Environment\",\n",
        "\"Flexible Work Hours\",\n",
        "\"Child-care Funding\",\n",
        "\"Diverse Students\",\n",
        "\"Inclusivity in Research\",\n",
        "\"Intersectionality\",\n",
        "\"Non-WEIRD Samples\",\n",
        "\"Minoritized Groups\",\n",
        "\"Undergraduate Major Choice\",\n",
        "\"Mechanistic View\",\n",
        "\"Nuanced Perceived Personality\",\n",
        "\"Perceptions of Fitting In\",\n",
        "\"Prioritization of Money\",\n",
        "\"Prioritization of Status\",\n",
        "\"Individual Differences\",\n",
        "\"Efficacious Interventions\",\n",
        "\"Academic Fields\"\n",
        "],\n",
        "\n",
        "\"Toward the future of psychiatric diagnosis: the seven pillars of RDoC\": [\n",
        "\"Research Domain Criteria\",\n",
        "\"Mental Disorders\",\n",
        "\"Neural Circuits\",\n",
        "\"Behavioral Functions\",\n",
        "\"Integrative Approach\",\n",
        "\"Precision Medicine\",\n",
        "\"Genetics\",\n",
        "\"Neuroscience\",\n",
        "\"Behavioral Science\",\n",
        "\"Empirically-Based Approach\",\n",
        "\"Moving Target\",\n",
        "\"Flexibility\",\n",
        "\"Well-Validated Tasks\",\n",
        "\"Functioning Measures\",\n",
        "\"Circuit Implementations\",\n",
        "\"Construct Validation\",\n",
        "\"Neuroimaging\",\n",
        "\"Relevant Genetic Polymorphism\",\n",
        "\"Exploration of Candidate Genes\",\n",
        "\"Symptom Severity and Distress\",\n",
        "\"Behavioral Fear-Avoidance Test\",\n",
        "\"Anxiety Disorders\",\n",
        "\"Fear-Potentiated Startle\"\n",
        "],\n",
        "\n",
        "\"The effect of verbal praise on prospective memory\": [\n",
        "\"Verbal Praise\",\n",
        "\"Attentional Load\",\n",
        "\"Cue Focality\",\n",
        "\"Prospective Memory\",\n",
        "\"Memory Performance\",\n",
        "\"Bottom-Up Processing\",\n",
        "\"Top-Down Processing\",\n",
        "\"Motivational Cognitive Model\",\n",
        "\"Task Difficulty\",\n",
        "\"Ongoing Tasks\",\n",
        "\"Retrospective Component\",\n",
        "\"Prospective Component\",\n",
        "\"Focal Cues\",\n",
        "\"Non-Focal Cues\",\n",
        "\"Attention Monitoring\",\n",
        "\"Attention Resources\",\n",
        "\"Eye-Tracking Technology\",\n",
        "\"Individual Differences\",\n",
        "\"Infusion\",\n",
        "\"Interference\",\n",
        "\"Promoting Effect\",\n",
        "\"Experimental Design\",\n",
        "\"Data Collection\",\n",
        "\"Data Analysis\"\n",
        "],\n",
        "\n",
        "\"Perceiving societal pressure to be happy is linked to poor well‐being, especially in happy nations\": [\n",
        "\"Emotional Well-being\",\n",
        "\"Societal Pressure\",\n",
        "\"Multilevel Analysis\",\n",
        "\"Cross-cultural Differences\",\n",
        "\"Positive Emotion\",\n",
        "\"Negative Emotion\",\n",
        "\"Depression\",\n",
        "\"Anxiety\",\n",
        "\"Life Satisfaction\",\n",
        "\"Happiness Index\",\n",
        "\"Global Hierarchical Structure\",\n",
        "\"Country-level Variation\",\n",
        "\"Person-level Predictors\",\n",
        "\"Cognitive Well-being\",\n",
        "\"Emotional Well-being\",\n",
        "\"Clinical Well-being\",\n",
        "\"Perceived Societal Expectations\",\n",
        "\"Within-country Standardization\",\n",
        "\"Robustness Analysis\",\n",
        "\"Leave-one-out Multiverse Analysis\",\n",
        "\"PA and NA Operationalizations\",\n",
        "\"Model Parameters\",\n",
        "\"Significance Tests\",\n",
        "\"Identical Conclusions\"\n",
        "],\n",
        "\n",
        "\"Commonsense psychology in human infants and machines\": [\n",
        "\"Infant Cognition\",\n",
        "\"Goal-Directed Actions\",\n",
        "\"Object Permanence\",\n",
        "\"Novel Environment\",\n",
        "\"Commonsense Psychology\",\n",
        "\"Artificial Intelligence\",\n",
        "\"Machine Learning\",\n",
        "\"Neural Networks\",\n",
        "\"Cognitive Development\",\n",
        "\"Human-Like AI\",\n",
        "\"Agent's Goals\",\n",
        "\"Rationality Attribution\",\n",
        "\"Instrumentality Attribution\",\n",
        "\"Geometry\",\n",
        "\"Navigability\",\n",
        "\"Multi-Agent Settings\",\n",
        "\"Social Partnerships\",\n",
        "\"Place Recognition\",\n",
        "\"Object Recognition\",\n",
        "\"Action Signaling\",\n",
        "\"Learning Techniques\",\n",
        "\"Cognitive Sciences\",\n",
        "\"Computational Sciences\",\n",
        "\"Interanimating Research Program\"\n",
        "],\n",
        "\n",
        "\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\": [\n",
        "\"Multi-task Learning\",\n",
        "\"Transfer Learning\",\n",
        "\"Unsupervised/Self-supervised Learning\",\n",
        "\"Language Modeling\",\n",
        "\"Question Answering\",\n",
        "\"Sentence Similarity\",\n",
        "\"Natural Language Inference\",\n",
        "\"Text Classification\",\n",
        "\"Named Entity Recognition\",\n",
        "\"Question Generation\",\n",
        "\" Dialogue Systems\",\n",
        "\"Machine Translation\",\n",
        "\"Low-resource Languages\",\n",
        "\"Data Augmentation\",\n",
        "\"Human Performance\",\n",
        "\"Gender Parity\",\n",
        "\"Accuracy Metrics\",\n",
        "\"Diagnostic Set\",\n",
        "\"Crowdworker Annotation\",\n",
        "\"Qualification Threshold\",\n",
        "\"Annotator Performance\",\n",
        "\"Test Sets\",\n",
        "\"Training Phase\",\n",
        "\"Annotation Phase\"\n",
        "],\n",
        "\n",
        "\"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\": [\n",
        "\"Adaptation Matrix\",\n",
        "\"Fine-tuning\",\n",
        "\"LoRA\",\n",
        "\"Transformer Language Models\",\n",
        "\"Dense Layers\",\n",
        "\"Feature Learning\",\n",
        "\"Downstream Tasks\",\n",
        "\"Heuristics\",\n",
        "\"Rank-deficient Matrices\",\n",
        "\"Artificial Neural Networks\",\n",
        "\"Statistical Learning\",\n",
        "\"Task-switching\",\n",
        "\"Input Sequence Length\",\n",
        "\"Model Quality\",\n",
        "\"Inference Latency\",\n",
        "\"Storage/Switching Cost\",\n",
        "\"Independent Instances\",\n",
        "\"Neural Networks\",\n",
        "\"Efficient Adaptation Methods\",\n",
        "\"Orthogonal Improvement\",\n",
        "\"Mechanism Behind Fine-Tuning\",\n",
        "\"Pre-training Transformation\",\n",
        "\"Principled Ways\",\n",
        "\"Future Works\"\n",
        "],\n",
        "\n",
        "\"LLaMA: Open and Efficient Foundation Language Models\": [\n",
        "\"Natural Language Processing\",\n",
        "\"Machine Learning\",\n",
        "\"Deep Learning\",\n",
        "\"Transformers\",\n",
        "\"Smoothing Techniques\",\n",
        "\"Large Scale Training\",\n",
        "\"Dataset Scaling\",\n",
        "\"Model Scaling\",\n",
        "\"Performance Improvement\",\n",
        "\"Robustness Mitigation\",\n",
        "\"Toxicity Reduction\",\n",
        "\"Bias Reduction\",\n",
        "\"Finetuning\",\n",
        "\"Pretraining Corpora\",\n",
        "\"Constant Improvement\",\n",
        "\"Power Laws\",\n",
        "\"Learning Rate Schedule\",\n",
        "\"Abilities of Large Language Models\",\n",
        "\"State-of-the-Art Foundation Models\",\n",
        "\"Openly Released Models\",\n",
        "\"Competitive Performance\",\n",
        "\"Publicly Available Data\",\n",
        "\"Proprietary Datasets\",\n",
        "\"Research Community Acceleration\",\n",
        "\"Improved Robustness\",\n",
        "\"Mitigated Known Issues\"],\n",
        "\n",
        "\"EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION\": [\n",
        "\"Language Models\",\n",
        "\"Context Window\",\n",
        "\"Position Interpolation\",\n",
        "\"Long Document Summarization\",\n",
        "\"Retrieval-Augmented LLM\",\n",
        "\"Recurrent Transformers\",\n",
        "\"Memory Transformers\",\n",
        "\"Approximated Multi-head Attention\",\n",
        "\"Length Extrapolation\",\n",
        "\"Interpolation\",\n",
        "\"Vision Transformers\",\n",
        "\"Learnable Position Embeddings\",\n",
        "\"Optimization\",\n",
        "\"Reusability\",\n",
        "\"Practical Applications\",\n",
        "\"Extension\",\n",
        "\"Generic Language Models\",\n",
        "\"Task Variety\",\n",
        "\"Preservation\",\n",
        "\"Fine-Tuning\",\n",
        "\"Minimal Tuning\",\n",
        "\"Evaluation Metrics\",\n",
        "\"ROUGE Score\"\n",
        "],\n",
        "\n",
        "\"Evaluating Large Language Models Trained on Code\": [\n",
        "\"Transformers\",\n",
        "\"Code Generation\",\n",
        "\"Functional Correctness\",\n",
        "\"Natural Language Processing\",\n",
        "\"Program Synthesis\",\n",
        "\"Neural Networks\",\n",
        "\"Machine Learning\",\n",
        "\"Code Search\",\n",
        "\"Test Suites\",\n",
        "\"Bug Fixing\",\n",
        "\"Genetic Programming\",\n",
        "\"Code Evaluation\",\n",
        "\" GitHub\",\n",
        "\"APPS\",\n",
        "\"Codeforces\",\n",
        "\"FlashFill\",\n",
        "\"Hearthstone\",\n",
        "\"Robust-Fill\",\n",
        "\"CodeBERT\",\n",
        "\"PyMT5\",\n",
        "\"T5 Objective\",\n",
        "\"Pseudocode\",\n",
        "\"Unsupervised Translation\",\n",
        "\"Contrastive Code Model\"\n",
        "]\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "iwv07XLVcwLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below are the evaluation methods such as Jaccard distance, similarity score, coherance score. Coherence score is the best metric to evaluate upon and hence we consider it for our final evaluation criteria."
      ],
      "metadata": {
        "id": "T76l9t7szdTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the model (Only do this once)\n",
        "word_vectors = api.load('glove-wiki-gigaword-100')\n",
        "\n",
        "# Save the model for future use\n",
        "word_vectors.save(\"glove_wiki_embeddings.model\")\n",
        "\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import jaccard\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "3ShhuyAOIkV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of words\n",
        "words_list = list(word_vectors.key_to_index.keys())\n",
        "\n",
        "# Print the first 10 words as an example\n",
        "print(words_list[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH3EoM-QjgYe",
        "outputId": "b97e02d2-1aa1-4301-e004-72f113a104d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HJGcUDPGyhF",
        "outputId": "72e90237-0791-4a66-9637-62c23221c4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jaccard Distance (1 - Similarity):  0.9957532725290753\n"
          ]
        }
      ],
      "source": [
        "def vectorize_sets(topics):\n",
        "    all_words = set().union(*topics) # Union of all words\n",
        "    vocab = list(all_words)\n",
        "\n",
        "    vectorized_topics = []\n",
        "    for topic in topics:\n",
        "        binary_vector = np.zeros(len(vocab))\n",
        "        for i, word in enumerate(vocab):\n",
        "            if word in topic:\n",
        "                binary_vector[i] = 1\n",
        "        vectorized_topics.append(binary_vector)\n",
        "\n",
        "    return vectorized_topics\n",
        "\n",
        "from scipy.spatial.distance import jaccard\n",
        "# Jaccard Diversity with revised input handling\n",
        "def jaccard_diversity(topics):\n",
        "    all_pairs = combinations(range(len(topics)), 2)\n",
        "    jaccard_distances = []\n",
        "\n",
        "    for pair in all_pairs:\n",
        "        topic_1 = topics[pair[0]]\n",
        "        topic_2 = topics[pair[1]]\n",
        "        jaccard_distances.append(jaccard(topic_1, topic_2))\n",
        "\n",
        "    return np.mean(jaccard_distances)\n",
        "\n",
        "# Word Embedding Centroid Distance with revised input handling\n",
        "def word_embeddings_centroid_distance(topics, word_vectors):\n",
        "    all_pairs = combinations(range(len(topics)), 2)\n",
        "    centroid_distances = []\n",
        "\n",
        "    for pair in all_pairs:\n",
        "        topic_1_centroid = np.mean([word_vectors[word] for word in topics[pair[0]] if word in word_vectors], axis=0)\n",
        "        topic_2_centroid = np.mean([word_vectors[word] for word in topics[pair[1]] if word in word_vectors], axis=0)\n",
        "\n",
        "        # Handle potential cases where topic_1_centroid or topic_2_centroid are empty\n",
        "        if not topic_1_centroid.any() or not topic_2_centroid.any():\n",
        "            centroid_distances.append(0)\n",
        "        else:\n",
        "            centroid_distances.append(np.linalg.norm(topic_1_centroid - topic_2_centroid))\n",
        "\n",
        "    return np.mean(centroid_distances)\n",
        "\n",
        "\n",
        "# Calculate diversity across all topics\n",
        "topics = list(data.values()) # Extract topic word lists\n",
        "\n",
        "#print(f'topics: {topics}')\n",
        "vectorized_topics = vectorize_sets(topics) # Vectorize for Jaccard calculation\n",
        "#print(f'vectorized topics: {vectorized_topics}')\n",
        "\n",
        "jaccard_div = jaccard_diversity(vectorized_topics)\n",
        "# we_cd = word_embeddings_centroid_distance(topics, word_vectors)\n",
        "\n",
        "print(\"Average Jaccard Distance (1 - Similarity): \", jaccard_div)\n",
        "# print(\"Average Word Embedding Centroid Distance: \", we_cd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, KeyedVectors\n",
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "original_text = paper1raw\n",
        "\n",
        "generated_topics = [\n",
        "\"Transformers\",\n",
        "\"Code Generation\",\n",
        "\"Functional Correctness\",\n",
        "\"Natural Language Processing\",\n",
        "\"Program Synthesis\",\n",
        "\"Neural Networks\",\n",
        "\"Machine Learning\",\n",
        "\"Code Search\",\n",
        "\"Test Suites\",\n",
        "\"Bug Fixing\",\n",
        "\"Genetic Programming\",\n",
        "\"Code Evaluation\",\n",
        "\" GitHub\",\n",
        "\"APPS\",\n",
        "\"Codeforces\",\n",
        "\"FlashFill\",\n",
        "\"Hearthstone\",\n",
        "\"Robust-Fill\",\n",
        "\"CodeBERT\",\n",
        "\"PyMT5\",\n",
        "\"T5 Objective\",\n",
        "\"Pseudocode\",\n",
        "\"Unsupervised Translation\",\n",
        "\"Contrastive Code Model\"\n",
        "]\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return tokens\n",
        "\n",
        "original_text_processed = preprocess(paper1raw)\n",
        "generated_topics_processed = [preprocess(topic) for topic in generated_topics]\n",
        "\n",
        "\n",
        "print(f'original_text_processed:{original_text_processed}')\n",
        "print(f'generated_topics_processed:{generated_topics_processed}')\n",
        "\n",
        "\n",
        "# Coherence Calculation\n",
        "dictionary = Dictionary(generated_topics_processed)\n",
        "corpus = [dictionary.doc2bow(topic) for topic in generated_topics_processed]\n",
        "lda = LdaModel(corpus, num_topics=len(generated_topics), id2word=dictionary)\n",
        "\n",
        "coherence_model = gensim.models.CoherenceModel(model=lda, texts=generated_topics_processed, dictionary=dictionary, coherence='c_v')\n",
        "coherence_scores = coherence_model.get_coherence_per_topic()\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in word_vectors:\n",
        "        return word_vectors[word]\n",
        "    else:\n",
        "        return np.zeros(word_vectors.vector_size)  # Handle missing words\n",
        "\n",
        "# Semantic Similarity Calculation\n",
        "def calculate_similarity(topic_words, text_words):\n",
        "    topic_vector = np.mean([get_vector(word) for word in topic_words], axis=0)\n",
        "    text_vector = np.mean([get_vector(word) for word in text_words], axis=0)\n",
        "    return 1 - cosine(topic_vector, text_vector)\n",
        "try:\n",
        "    # Try loading the saved model\n",
        "    word_vectors = KeyedVectors.load(\"glove_wiki_embeddings.model\")\n",
        "except FileNotFoundError:\n",
        "    # Download and save the model if it doesn't exist\n",
        "    print(\"Downloading word embeddings (This happens only once)\")\n",
        "    word_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "    word_vectors.save(\"glove_wiki_embeddings.model\")\n",
        "\n",
        "\n",
        "similarity_scores = [calculate_similarity(topic, original_text_processed) for topic in generated_topics_processed]\n",
        "\n",
        "# Print Results\n",
        "print(\"Coherence Scores:\", coherence_scores)\n",
        "print(\"Similarity Scores:\", similarity_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDo4XcgML8Ze",
        "outputId": "8f01a76e-c4c8-4e4a-f1cf-ae7e5b7a69d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_text_processed:['abstract', '\\n\\n', 'introduce', 'codex', 'gpt', 'language', 'model', 'fine-', '\\n', 'tune', 'publicly', 'available', 'code', 'github', '\\n\\n', 'study', 'python', 'code', 'writing', 'capability', '\\n', 'distinct', 'production', 'version', 'codex', 'power', '\\n\\n', 'github', 'copilot', 'humaneval', 'new', 'evalua-', '\\n', 'tion', 'set', 'release', 'measure', 'functional', 'correct-', '\\n', 'ness', 'synthesize', 'program', 'docstring', '\\n\\n', 'model', 'solve', '28.8', 'problem', '\\n\\n', 'gpt-3', 'solve', '0', 'gpt', 'j', 'solve', '11.4', 'fur-', '\\n', 'thermore', 'find', 'repeat', 'sample', '\\n\\n', 'model', 'surprisingly', 'effective', 'strategy', 'pro-', '\\n', 'duce', 'work', 'solution', 'difficult', 'prompt', 'us-', '\\n', 'ing', 'method', 'solve', '70.2', 'problem', '\\n\\n', '100', 'sample', 'problem', 'careful', 'investiga-', '\\n', 'tion', 'model', 'reveal', 'limitation', 'include', '\\n\\n', 'difficulty', 'docstring', 'describe', 'long', 'chain', '\\n\\n', 'operation', 'bind', 'operation', 'vari-', '\\n', 'able', 'finally', 'discuss', 'potential', 'broad', '\\n\\n', 'impact', 'deploy', 'powerful', 'code', 'generation', '\\n\\n', 'technology', 'cover', 'safety', 'security', 'eco-', '\\n', 'nomic', '\\n\\n', 'equal', 'contribution', '\\n', '1openai', 'san', 'francisco', 'california', 'usa', '\\n\\n', '2anthropic', 'ai', 'san', 'francisco', 'california', 'usa', 'work', 'per-', '\\n', 'form', 'openai', '\\n\\n', '3zipline', 'south', 'san', 'francisco', 'california', 'usa', 'work', 'per-', '\\n', 'form', 'openai', '\\n\\n', 'correspondence', 'mark', 'chen', '<', 'mark@openai.com', '>', '\\n\\n', 'jerry', 'tworek', '<', 'jt@openai.com', '>', 'heewoo', 'jun', '<', 'hee-', '\\n', 'woo@openai.com', '>', 'qiming', 'yuan', '<', 'qiming@openai.com', '>', '\\n\\n', '1', 'introduction', '\\n', 'scalable', 'sequence', 'prediction', 'model', 'graves', '2014', '\\n', 'vaswani', 'et', 'al', '2017', 'child', 'et', 'al', '2019', '\\n', 'general', 'purpose', 'method', 'generation', 'representation', '\\n\\n', 'learn', 'domain', 'include', 'natural', 'language', 'pro-', '\\n', 'cesse', 'mikolov', 'et', 'al', '2013', 'sutskever', 'et', 'al', '2014', 'dai', '\\n\\n', 'le', '2015', 'peters', 'et', 'al', '2018', 'radford', 'et', 'al', '2018', 'devlin', '\\n', 'et', 'al', '2018', 'computer', 'vision', 'van', 'oord', 'et', 'al', '2016', 'menick', '\\n', 'kalchbrenner', '2018', 'chen', 'et', 'al', '2020', 'bao', 'et', 'al', '2021', '\\n\\n', 'audio', 'speech', 'processing', 'oord', 'et', 'al', '2016', '2018', 'dhari-', '\\n', 'wal', 'et', 'al', '2020', 'baevski', 'et', 'al', '2020', 'biology', 'alley', 'et', 'al', '\\n\\n', '2019', 'rives', 'et', 'al', '2021', 'multiple', 'modali-', '\\n', 'tie', 'das', 'et', 'al', '2017', 'lu', 'et', 'al', '2019', 'ramesh', 'et', 'al', '2021', '\\n\\n', 'zellers', 'et', 'al', '2021', 'recently', 'language', 'model', '\\n', 'fuel', 'progress', 'longstanding', 'challenge', '\\n', 'program', 'synthesis', 'simon', '1963', 'manna', 'waldinger', '\\n', '1971', 'spur', 'presence', 'code', 'large', 'dataset', '\\n\\n', 'husain', 'et', 'al', '2019', 'gao', 'et', 'al', '2020', 'result', 'pro-', '\\n', 'gramme', 'capability', 'language', 'model', 'train', '\\n\\n', 'dataset', 'wang', 'komatsuzaki', '2021', 'popular', 'language', '\\n', 'modeling', 'objective', 'like', 'mask', 'language', 'modeling', 'devlin', '\\n', 'et', 'al', '2018', 'span', 'prediction', 'raffel', 'et', 'al', '2020', '\\n', 'adapt', 'train', 'programming', 'counterpart', '\\n', 'codebert', 'feng', 'et', 'al', '2020', 'pymt5', 'clement', 'et', 'al', '\\n', '2020', '\\n', 'similarly', 'early', 'investigation', 'gpt-3', 'brown', 'et', 'al', '\\n', '2020', 'reveal', 'generate', 'simple', 'program', '\\n', 'python', 'docstring', 'rudimentary', 'capability', '\\n', 'exciting', 'gpt-3', 'explicitly', 'train', 'code', '\\n\\n', 'generation', 'give', 'considerable', 'success', 'large', 'lan-', '\\n', 'guage', 'model', 'modality', 'abundance', '\\n\\n', 'publicly', 'available', 'code', 'hypothesize', 'specialized', '\\n', 'gpt', 'model', 'call', 'codex', 'excel', 'variety', 'code', '\\n', 'task', 'paper', 'describe', 'early', 'codex', 'model', '\\n', 'descendant', 'power', 'github', 'copilot', 'codex', '\\n', 'model', 'openai', 'api', '\\n\\n', 'arxiv:2107.03374v2', 'cs', 'lg', '14', 'jul', '2021', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'figure', '1', 'pass', 'rate', 'model', 'humaneval', 'dataset', '\\n', 'function', 'model', 'size', 'single', 'sample', 'generate', '\\n', 'problem', 'gpt-12b', 'solve', 'problem', 'codex', 'fine', 'tune', '\\n', 'code', 'solve', '28.8', 'problem', 'codex', 's', '\\n', 'fine', 'tune', 'correctly', 'implement', 'standalone', 'function', 'solve', '\\n', '37.7', 'problem', 'gain', 'realize', '\\n', 'generate', '100', 'sample', 'problem', 'select', 'sample', '\\n', 'high', 'mean', 'log', 'probability', '44.5', 'solve', 'select', '\\n', 'sample', 'pass', 'unit', 'test', '77.5', 'solve', 'sample', '\\n', 'generate', 'temperature', '0.8', '\\n\\n', 'work', 'focus', 'task', 'generate', 'stan-', '\\n', 'dalone', 'python', 'function', 'docstring', 'evaluate', '\\n\\n', 'correctness', 'code', 'sample', 'automatically', 'unit', '\\n', 'test', 'contrast', 'natural', 'language', 'generation', '\\n', 'sample', 'typically', 'evaluate', 'heuristic', '\\n', 'human', 'evaluator', 'accurately', 'benchmark', 'model', '\\n', 'create', 'dataset', '164', 'original', 'programming', 'problem', '\\n\\n', 'unit', 'test', 'problem', 'assess', 'language', 'compre-', '\\n', 'hension', 'algorithm', 'simple', 'mathematic', '\\n\\n', 'comparable', 'simple', 'software', 'interview', 'question', '\\n', 'release', 'datum', 'evaluation', 'framework', '\\n', 'https://www.github.com/openai/human-eval', '\\n', 'solve', 'problem', 'test', 'set', 'generate', 'multiple', '\\n', 'sample', 'model', 'check', 'pass', '\\n', 'unit', 'test', 'single', 'sample', '12b', 'parameter', 'codex', '\\n', 'solve', '28.8', 'problem', '300', 'm', 'parameter', '\\n', 'codex', 'solve', '13.2', 'problem', 'contrast', '6b', '\\n', 'parameter', 'gpt', 'j', 'wang', 'komatsuzaki', '2021', 'achieve', '\\n', '11.4', 'dataset', 'gpt', 'model', 'achieve', '\\n', 'near', '0', 'improve', 'model', 'performance', 'task', '\\n', 'function', 'synthesis', 'docstring', 'fine', 'tune', 'codex', '\\n', 'standalone', 'correctly', 'implement', 'function', 'result', '\\n', 'model', 'codex', 's', 'solve', '37.7', 'problem', 'single', '\\n', 'sample', 'figure', '2', 'showcase', 'problem', 'vary', 'difficulty', '\\n', 'dataset', 'correct', 'model', 'generate', 'solution', '\\n', 'real', 'world', 'programming', 'task', 'involve', 'iteration', '\\n\\n', 'approach', 'bug', 'fix', 'approximate', 'gener-', '\\n', 'ate', 'sample', 'model', 'select', '\\n\\n', 'pass', 'unit', 'test', '100', 'sample', 'codex', 's', 'able', '\\n\\n', 'generate', 'correct', 'function', '77.5', 'prob-', '\\n', 'lem', 'result', 'suggest', 'accurate', 'code', 'sample', '\\n\\n', 'select', 'heuristic', 'ranking', 'instead', 'fully', 'evaluate', '\\n\\n', 'sample', 'possible', 'prac-', '\\n', 'tical', 'deployment', 'find', 'sample', '\\n\\n', 'high', 'mean', 'log', 'probability', 'pass', 'unit', 'test', '44.5', '\\n', 'problem', '\\n', 'conclude', 'discuss', 'limitation', 'potential', '\\n', 'broad', 'impact', 'codex', 'model', 'increasingly', '\\n', 'powerful', 'code', 'generating', 'model', 'generally', '\\n', '2', 'evaluation', 'framework', '\\n', 'section', 'discuss', 'detail', 'evaluation', '\\n', 'framework', 'begin', 'define', 'pass@k', 'metric', '\\n', 'explain', 'advantage', 'standard', 'match', 'base', 'metric', '\\n', 'describe', 'dataset', 'hand', 'write', 'problem', '\\n\\n', 'call', 'humaneval', 'create', 'order', 'bench-', '\\n', 'mark', 'model', 'finally', 'discuss', 'sandbox', 'environ-', '\\n', 'ment', 'safely', 'execute', 'model', 'generate', 'code', '\\n\\n', '2.1', 'functional', 'correctness', '\\n', 'generative', 'model', 'code', 'predominantly', 'benchmarke', '\\n', 'match', 'sample', 'reference', 'solution', '\\n\\n', 'match', 'exact', 'fuzzy', 'bleu', 'score', 'how-', '\\n', 'recent', 'work', 'surface', 'deficiency', 'match', 'base', '\\n\\n', 'metric', 'code', 'instance', 'ren', 'et', 'al', '2020', 'find', '\\n', 'bleu', 'problem', 'capture', 'semantic', 'feature', 'specific', '\\n', 'code', 'suggest', 'semantic', 'modification', '\\n', 'score', '\\n\\n', 'fundamentally', 'match', 'base', 'metric', 'unable', 'ac-', '\\n', 'count', 'large', 'complex', 'space', 'program', 'function-', '\\n', 'ally', 'equivalent', 'reference', 'solution', 'consequence', '\\n\\n', 'recent', 'work', 'unsupervised', 'code', 'translation', 'lachaux', '\\n', 'et', 'al', '2020', 'pseudocode', 'code', 'translation', 'kulal', 'et', 'al', '\\n', '2019', 'turn', 'functional', 'correctness', 'instead', '\\n', 'sample', 'consider', 'correct', 'pass', 'set', 'unit', 'test', '\\n\\n', 'argue', 'metric', 'apply', 'docstring-', '\\n', 'conditional', 'code', 'generation', '\\n\\n', 'convincing', 'reason', 'evaluate', 'functional', '\\n', 'correctness', 'human', 'developer', 'judge', '\\n\\n', 'code', 'framework', 'know', 'test', 'drive', 'development', 'dic-', '\\n', 'tat', 'software', 'requirement', 'convert', 'test', 'case', '\\n\\n', 'implementation', 'begin', 'success', 'define', '\\n\\n', 'program', 'pass', 'test', 'organiza-', '\\n', 'tion', 'employ', 'test', 'drive', 'development', 'integration', '\\n\\n', 'new', 'code', 'usually', 'dependent', 'create', 'pass', 'unit', '\\n', 'test', '\\n', 'kulal', 'et', 'al', '2019', 'evaluate', 'functional', 'correctness', '\\n', 'pass@k', 'metric', 'k', 'code', 'sample', 'generate', '\\n', 'problem', 'problem', 'consider', 'solve', 'sample', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'figure', '2', 'example', 'problem', 'humaneval', 'dataset', 'probability', 'single', 'sample', 'codex-12b', 'pass', 'unit', '\\n', 'test', '0.9', '0.17', '0.005', 'prompt', 'provide', 'model', 'show', 'white', 'background', 'successful', 'model', 'generate', '\\n', 'completion', 'show', 'yellow', 'background', 'guarantee', 'problem', 'novelty', 'problem', 'hand', 'write', '\\n', 'programmatically', 'copy', 'exist', 'source', 'random', 'problem', 'sample', 'find', 'appendix', 'b.', '\\n', 'pass', 'unit', 'test', 'total', 'fraction', 'problem', '\\n', 'solve', 'report', 'compute', 'pass@k', '\\n', 'way', 'high', 'variance', 'instead', 'evaluate', 'pass@k', '\\n', 'generate', 'n', '≥', 'k', 'sample', 'task', 'paper', '\\n', 'use', 'n', '=', '200', 'k', '≤', '100', 'count', 'number', 'correct', '\\n', 'sample', 'c', '≤', 'n', 'pass', 'unit', 'test', 'calculate', '\\n', 'unbiased', 'estimator', '\\n\\n', 'pass@k', '=', 'e', '\\n', 'problem', '\\n', '1', '−', '\\n', 'n−c', '\\n', 'k', '\\n\\n', 'n', '\\n', 'k', '\\n\\n', '\\n', '1', '\\n\\n', 'calculate', 'estimator', 'directly', 'result', 'large', 'num-', '\\n', 'ber', 'numerical', 'instability', 'figure', '3', 'include', '\\n\\n', 'numerically', 'stable', 'numpy', 'implementation', 'simplify', '\\n', 'expression', 'evaluate', 'product', 'term', 'term', '\\n', 'tempt', 'estimate', 'pass@k', '1−(1−pˆ', '\\n', 'k', '\\n', 'pˆ', 'empirical', 'estimate', 'pass@1', '\\n', 'bias', 'appendix', 'a.', '\\n\\n', 'def', 'pass_at_k(n', 'c', 'k', '\\n', '\\n', 'param', 'n', 'total', 'number', 'sample', '\\n', 'param', 'c', 'number', 'correct', 'sample', '\\n', 'param', 'k', 'k', 'pass@$k$', '\\n', '\\n', 'n', 'c', '<', 'k', 'return', '1.0', '\\n', 'return', '1.0', 'np.prod(1.0', 'k', '\\n', 'np.arange(n', 'c', '+', '1', 'n', '+', '1', '\\n', 'figure', '3', 'numerically', 'stable', 'script', 'calculate', 'unbiased', '\\n', 'estimate', 'pass@k', '\\n\\n', 'later', 'provide', 'evidence', 'bleu', 'score', '\\n', 'reliable', 'indicator', 'functional', 'correctness', 'show', '\\n', 'functionally', 'inequivalent', 'program', 'generate', '\\n', 'model', 'guarantee', 'disagree', 'reference', '\\n', 'solution', 'input', 'high', 'bleu', 'score', '\\n', 'functionally', 'equivalent', 'one', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', '2.2', 'humaneval', 'hand', 'write', 'evaluation', 'set', '\\n\\n', 'evaluate', 'functional', 'correctness', 'set', '164', 'hand-', '\\n', 'write', 'programming', 'problem', 'hu-', '\\n', 'maneval', 'dataset', 'problem', 'include', 'function', 'sig-', '\\n', 'nature', 'docstring', 'body', 'unit', 'test', 'av-', '\\n', 'erage', '7.7', 'test', 'problem', 'important', '\\n\\n', 'task', 'hand', 'write', 'model', 'train', '\\n', 'large', 'fraction', 'github', 'contain', 'solution', '\\n', 'problem', 'variety', 'source', 'example', '\\n', 'public', 'repository', 'contain', 'solution', '\\n', 'codeforces', 'problem', 'recently', '\\n', 'propose', 'apps', 'dataset', 'hendrycks', 'et', 'al', '2021', '\\n\\n', 'programming', 'task', 'humaneval', 'dataset', 'assess', 'lan-', '\\n', 'guage', 'comprehension', 'reasoning', 'algorithm', 'simple', '\\n\\n', 'mathematic', 'release', 'humaneval', 'dataset', '\\n', 'evaluate', 'functional', 'correctness', 'measure', '\\n', 'problem', 'solve', 'capability', 'model', 'dataset', '\\n', 'find', 'https://www.github.com/openai/human-eval', '\\n', '2.3', 'sandbox', 'execute', 'generated', 'programs', '\\n', 'publicly', 'available', 'program', 'unknown', 'intent', '\\n', 'generate', 'program', 'incorrect', 'execute', '\\n', 'program', 'pose', 'security', 'risk', 'github', 'know', '\\n', 'contain', 'malicious', 'program', 'alter', 'change', '\\n', 'environment', 'rokon', 'et', 'al', '2020', '\\n', 'develop', 'sandbox', 'environment', 'safely', '\\n', 'run', 'untrusted', 'program', 'unit', 'test', 'goal', '\\n', 'prevent', 'program', 'modify', 'gain', 'persistence', '\\n', 'access', 'sensitive', 'resource', 'exfiltrate', 'datum', '\\n', 'host', 'network', 'openai', 'training', 'infrastructure', '\\n', 'build', 'kubernetes', 'cloud', 'service', 'design', '\\n', 'sandbox', 'address', 'limitation', 'environment', '\\n', 'remain', 'idiomatic', 'pattern', 'use', '\\n', 'select', 'gvisor', 'container', 'runtime', 'lacasse', '2018', '\\n', 'main', 'host', 'protection', 'component', 'container', '\\n\\n', 'runtime', 'like', 'docker', 'share', 'host', 'resource', 'contain-', '\\n', 'er', 'malicious', 'container', 'potentially', 'compromise', '\\n\\n', 'host', 'gvisor', 'protect', 'host', 'emulate', 'resource', '\\n\\n', 'introduce', 'security', 'boundary', 'host', 'con-', '\\n', 'tainer', 'network', 'adjacent', 'host', 'service', 'protect', '\\n\\n', 'ebpf', 'base', 'firewall', 'rule', 'prevent', 'inbound', 'out-', '\\n', 'bind', 'connection', 'require', 'experiment', '\\n\\n', 'control', '\\n', '3', 'code', 'fine', 'tuning', '\\n', 'fine', 'tune', 'gpt', 'model', 'contain', '12b', 'parameter', '\\n', 'code', 'produce', 'codex', 'contrast', 'gpt', 'codex', '\\n', 'display', 'non', 'trivial', 'performance', 'humaneval', 'dataset', '\\n', 'fact', 'codex', 'able', 'solve', 'majority', 'problem', '\\n', 'humaneval', 'generate', 'evaluate', '100', 'sample', '\\n\\n', 'problem', 'pick', 'pass', 'unit', 'test', 'limit', '\\n', 'budget', 'evaluation', 'problem', 'produce', 'multiple', '\\n', 'sample', 'codex', 'choose', 'high', '\\n', 'mean', 'log', 'probability', 'provide', 'significant', 'gain', '\\n', '3.1', 'data', 'collection', '\\n\\n', 'training', 'dataset', 'collect', '2020', '54', 'mil-', '\\n', 'lion', 'public', 'software', 'repository', 'host', 'github', 'contain-', '\\n', 'ing', '179', 'gb', 'unique', 'python', 'file', '1', 'mb', 'filter', '\\n\\n', 'file', 'likely', 'auto', 'generate', 'average', 'line', '\\n', 'length', 'great', '100', 'maximum', 'line', 'length', 'great', '\\n', '1000', 'contain', 'small', 'percentage', 'alphanumeric', '\\n', 'character', 'filtering', 'final', 'dataset', 'total', '159', 'gb', '\\n', '3.2', 'method', '\\n', 'codex', 'evaluate', 'natural', 'language', 'prompt', '\\n', 'hypothesize', 'beneficial', 'fine', 'tune', '\\n', 'gpt-3', 'brown', 'et', 'al', '2020', 'model', 'family', '\\n\\n', 'contain', 'strong', 'natural', 'language', 'representation', 'surpris-', '\\n', 'ingly', 'observe', 'improvement', 'start', '\\n\\n', 'pre', 'train', 'language', 'model', 'possibly', 'fine-', '\\n', 'tuning', 'dataset', 'large', 'model', 'fine', 'tune', '\\n\\n', 'gpt', 'converge', 'quickly', 'apply', 'strategy', '\\n', 'subsequent', 'experiment', '\\n\\n', 'train', 'codex', 'learning', 'rate', 'corre-', '\\n', 'sponde', 'gpt', 'model', '175', 'step', 'linear', 'warmup', '\\n\\n', 'cosine', 'learning', 'rate', 'decay', 'train', 'total', '100', 'billion', '\\n', 'token', 'adam', 'optimizer', 'β1', '=', '0.9', 'β2', '=', '0.95', '\\n ', '=', '10−8', '\\n', 'weight', 'decay', 'coefficient', '0.1', '\\n', 'order', 'maximally', 'leverage', 'text', 'representation', '\\n', 'gpt', 'base', 'code', 'lexer', 'gpt-3', 'text', 'tokenizer', '\\n', 'distribution', 'word', 'github', 'code', 'differ', '\\n', 'natural', 'text', 'tokenizer', 'effective', '\\n', 'represent', 'code', 'large', 'source', 'inefficiency', 'arise', '\\n', 'encode', 'whitespace', 'add', 'additional', 'set', '\\n', 'token', 'represent', 'whitespace', 'run', 'different', 'length', '\\n', 'allow', 'represent', 'code', 'approximately', '30', '\\n', 'few', 'token', '\\n\\n', 'compute', 'pass@k', 'assemble', 'humaneval', 'prob-', '\\n', 'lem', 'prompt', 'consist', 'header', 'signature', '\\n\\n', 'docstring', 'illustrate', 'figure', '2', 'sample', '\\n', 'token', 'codex', 'encounter', 'follow', '\\n', 'stop', 'sequence', '\\\\nclass', '\\\\ndef', '\\\\n', '\\\\nif', '\\n\\n', '\\\\nprint', 'model', 'continue', 'generate', 'addi-', '\\n', 'tional', 'function', 'statement', 'use', 'nucleus', '\\n\\n', 'sample', 'holtzman', 'et', 'al', '2020', 'p', '=', '0.95', '\\n', 'sample', 'evaluation', 'work', '\\n', '3.3', 'result', '\\n', 'figure', '4', 'plot', 'test', 'loss', 'hold', 'validation', 'set', '\\n', 'codex', 'model', 'size', 'find', 'language', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'figure', '4', 'model', 'cross', 'entropy', 'test', 'loss', 'measure', 'hold', '\\n', 'split', 'python', 'github', 'code', 'corpus', 'smooth', 'power', 'law', '\\n', 'scaling', 'performance', 'model', 'size', 'observe', 'gpt-3', 'appear', '\\n', 'hold', 'code', 'fine', 'tuning', '\\n\\n', 'model', 'test', 'loss', 'follow', 'power', 'law', 'model', 'size', 'kaplan', '\\n', 'et', 'al', '2020', 'test', 'loss', 'code', 'fine', 'tuning', 'follow', 'similar', '\\n', 'power', 'law', 'functional', 'form', '\\n', 'n', '\\n', '5.92×107', '\\n', '−0.13', 'n', '\\n', 'number', 'non', 'embedding', 'parameter', 'model', '\\n\\n', 'evaluate', 'pass@k', 'important', 'optimize', 'sam-', '\\n', 'ple', 'temperature', 'particular', 'value', 'k.', 'figure', '5', '\\n\\n', 'plot', 'pass@k', 'number', 'sample', 'k', '\\n', 'sample', 'temperature', 'find', 'high', 'temperature', '\\n', 'optimal', 'large', 'k', 'result', 'set', 'sample', '\\n', 'high', 'diversity', 'metric', 'reward', '\\n', 'model', 'generate', 'correct', 'solution', '\\n\\n', 'particular', '679', 'm', 'parameter', 'model', 'optimal', 'tem-', '\\n', 'perature', 'pass@1', 't', '\\n\\n', '∗', '=', '0.2', 'optimal', 'tempera-', '\\n', 'ture', 'pass@100', 't', '\\n\\n', '∗', '=', '0.8', 'temperature', '\\n', 'find', 'pass@1', 'pass@100', 'scale', 'smoothly', '\\n', 'function', 'model', 'size', 'figure', '6', '\\n', 'pass@k', 'interpret', 'result', 'evaluate', '\\n', 'good', 'k', 'sample', 'good', 'sample', 'pick', '\\n', 'oracle', 'prior', 'knowledge', 'unit', 'test', '\\n\\n', 'practical', 'perspective', 'interested', 'set-', '\\n', 'ting', 'select', 'single', 'sample', 'k', 'sample', '\\n\\n', 'have', 'access', 'oracle', 'instance', '\\n', 'model', 'autocomplete', 'tool', 'user', 'provide', '\\n', 'prompt', 'unit', 'test', 'like', 'return', '\\n', 'single', 'completion', 'user', 'evaluation', '\\n', 'overwhelm', '\\n', 'inspire', 'similar', 'work', 'language', 'modeling', 'find', '\\n', 'choose', 'sample', 'high', 'mean', 'token', 'log', '\\n', 'probability', 'outperform', 'evaluate', 'random', 'sample', '\\n\\n', 'choose', 'sample', 'base', 'sum', 'log', 'probability', 'per-', '\\n', 'form', 'slightly', 'bad', 'pick', 'randomly', 'figure', '7', 'demon-', '\\n', 'strate', 'benefit', 'apply', 'heuristic', 'sample', '\\n\\n', 'temperature', '0.8', 'codex-12b.', '\\n\\n', 'figure', '5', 'panel', 'plot', 'pass@k', 'number', '\\n', 'sample', 'k', 'temperature', 'setting', 'high', 'temperature', '\\n', 'well', 'number', 'sample', 'large', 'likely', '\\n', 'increase', 'sample', 'diversity', 'panel', 'plot', 'good', '\\n', 'temperature', 'setting', 'k', 'obtain', 'take', 'upper', 'hull', '\\n', 'panel', '\\n\\n', 'figure', '6', 'optimal', 'temperature', '0.2', '0.8', 'pass@1', '\\n', 'pass@100', 'plot', 'metric', 'function', 'model', '\\n\\n', 'size', 'performance', 'appear', 'scale', 'smoothly', 'sigmoid', 'log-', '\\n', 'parameter', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'figure', '7', 'model', 'performance', 'setting', 'generate', '\\n\\n', 'multiple', 'sample', 'evaluate', 'well', 'ran-', '\\n', 'domly', 'select', 'sample', 'choose', 'solution', 'high', '\\n\\n', 'mean', 'log', 'probability', 'red', 'high', 'translation', '\\n', 'score', 'orange', 'describe', 'sec', '5', 'blue', 'line', 'represent', '\\n', 'theoretical', 'good', 'performance', 'obtain', 'oracle', 'prior', '\\n', 'knowledge', 'unit', 'test', '\\n\\n', 'finally', 'compute', 'bleu', 'score', 'codex-12b', 'hu-', '\\n', 'maneval', 'sample', 'temperature', '0.8', 'reference', '\\n\\n', 'solution', 'problem', 'plot', 'distribution', '\\n', 'bleu', 'score', 'correct', 'incorrect', 'solution', '\\n', 'notice', 'significant', 'overlap', 'figure', '8)', 'incorrect', '\\n', 'solution', 'guarantee', 'functionally', 'inequivalent', '\\n', 'reference', 'solution', 'conclude', 'improvement', '\\n', 'bleu', 'score', 'indicate', 'improve', 'rate', 'functional', '\\n', 'correctness', 'practice', '\\n', '3.4', 'comparative', 'analysis', 'related', 'models', '\\n', 'systems', '\\n', 'recent', 'work', 'similar', 'spirit', 'codex', 'gpt', 'neo', '\\n', 'black', 'et', 'al', '2021', 'gpt', 'j', 'wang', 'komatsuzaki', '\\n', '2021', 'train', 'pile', 'gao', 'et', 'al', '2020', '\\n', 'dataset', 'contain', 'text', 'variety', 'source', '\\n', '8', 'github', 'code', 'broad', 'research', 'community', '\\n', 'find', 'model', 'outperform', 'exist', 'gpt', 'system', '\\n', 'qualitative', 'programming', 'evaluation', 'woolf', '2021', '\\n', 'confirm', 'finding', 'humaneval', 'dataset', '\\n', 'show', 'gpt', 'neo', 'achieve', '6.4', 'pass@1', '21.3', '\\n', 'pass@100', 'gpt', 'model', 'comparable', 'size', 'achieve', '\\n', 'near', '0', 'metric', 'remarkable', 'progression', '\\n', 'capability', 'gpt', 'neo-2.7b', 'roughly', 'equivalent', '\\n', 'codex-85', 'm', '30×', 'few', 'parameter', 'similarly', 'gpt', 'j-6b', '\\n', 'achieve', '11.6', 'pass@1', '27.7', 'pass@100', '\\n', 'roughly', 'equivalent', 'codex-300', 'm', '20×', 'few', 'parameter', '\\n\\n', 'pass', 'rate', 'obtain', 'take', 'good', 'result', 'eval-', '\\n', 'figure', '8', 'bleu', 'score', 'probability', 'density', 'correct', 'blue', '\\n\\n', 'wrong', 'green', 'solution', 'codex-12b', '4', 'random', 'task', '\\n', 'humaneval', 'note', 'distribution', 'cleanly', 'separable', '\\n', 'suggest', 'optimize', 'bleu', 'score', 'equivalent', '\\n', 'optimize', 'functional', 'correctness', '\\n', 'uate', 'temperature', '0.2', '0.4', '0.8', 'gpt', 'neo', '\\n', 'temperature', '0.2', '0.8', 'gpt', 'j.', 'detailed', 'result', '\\n', 'multiple', 'model', 'size', 'find', 'table', '1', '\\n', 'finally', 'benchmark', 'codex', 'large', 'free', 'model', '\\n', 'tabnine', 'lead', 'code', 'autocomplete', 'system', '\\n', 'achieve', '2.6', 'pass@1', 't', '=', '0.4', '7.6', 'pass@100', '\\n', 't', '=', '0.8', 'roughly', 'equivalent', 'codex-12', 'm', '\\n', 'small', 'model', 'suite', '\\n', '3.5', 'result', 'apps', 'dataset', '\\n', 'recently', 'hendrycks', 'et', 'al', '2021', 'introduce', 'apps', '\\n\\n', 'dataset', 'measure', 'code', 'challenge', 'competence', 'lan-', '\\n', 'guage', 'model', 'apps', 'dataset', 'consist', '5000', 'training', '\\n\\n', '5000', 'test', 'example', 'code', 'problem', 'set', '\\n\\n', 'unit', 'test', 'training', 'datum', 'set', 'correct', 'solu-', '\\n', 'tion', 'apps', 'test', 'problem', 'formulate', '\\n\\n', 'single', 'function', 'synthesis', 'task', 'program', '\\n', 'synthesis', 'read', 'input', 'stdin', 'print', 'output', '\\n', 'stdout', 'contrast', 'main', 'codex', 'training', 'datum', '\\n', 'paper', 'introduce', 'apps', 'author', 'benchmark', '\\n', 'language', 'model', 'report', 'metric', 'percentage', '\\n', 'problem', 'model', 'find', 'correct', 'solution', 'call', '\\n', 'strict', 'accuracy', 'percentage', 'unit', 'test', 'pass', '\\n\\n', 'solution', 'incorrect', 'measure', 're-', '\\n', 'port', 'reduce', 'variance', 'measurement', '\\n\\n', 'result', 'metric', 'low', 'avoid', '\\n', 'metric', 'focus', 'strict', 'accuracy', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'table', '1', 'codex', 'gpt', 'neo', 'tabnine', 'evaluation', 'humaneval', '\\n\\n', 'find', 'gpt', 'j', 'pass@1', 'codex-85', 'm', 'codex-', '\\n', '300', 'm', 'performance', '\\n\\n', 'pass@k', '\\n', 'k', '=', '1', 'k', '=', '10', 'k', '=', '100', '\\n', 'gpt', 'neo', '125', 'm', '0.75', '1.88', '2.97', '\\n', 'gpt', 'neo', '1.3b', '4.79', '7.47', '16.30', '\\n', 'gpt', 'neo', '2.7b', '6.41', '11.27', '21.37', '\\n', 'gpt', 'j', '6b', '11.62', '15.74', '27.74', '\\n', 'tabnine', '2.58', '4.35', '7.59', '\\n', 'codex-12', 'm', '2.00', '3.62', '8.58', '\\n', 'codex-25', 'm', '3.21', '7.1', '12.89', '\\n', 'codex-42', 'm', '5.06', '8.8', '15.55', '\\n', 'codex-85', 'm', '8.22', '12.81', '22.4', '\\n', 'codex-300', 'm', '13.17', '20.37', '36.27', '\\n', 'codex-679', 'm', '16.22', '25.7', '40.95', '\\n', 'codex-2.5b', '21.36', '35.42', '59.5', '\\n', 'codex-12b', '28.81', '46.81', '72.31', '\\n\\n', 'previous', 'section', 'report', 'pass@k', 'number', 'vari-', '\\n', 'ous', 'k', 'table', '2', '2', 'additional', 'factor', 'know', '\\n\\n', 'code', 'competition', 'account', '\\n', 'code', 'competition', 'apps', 'dataset', 'task', '\\n', 'provide', '3', 'input', 'output', 'example', 'include', '\\n', 'task', 'description', 'utilize', 'sample', '1000', '\\n', 'solution', 'model', 'filter', '\\n', 'pass', '3', 'unit', 'test', 'solution', 'exist', '\\n', 'calculate', 'pass', 'rate', 'filter', 'set', '\\n', 'filter', 'pass@k', 'result', 'filtering', 'present', '\\n', 'raw', 'pass@k', '\\n', 'case', 'code', 'competition', '\\n', 'result', 'codex', 'correct', 'solution', 'find', '\\n\\n', 'algorithmically', 'efficient', 'con-', '\\n', 'sidere', 'passing', 'acceptable', '\\n\\n', 'competition', 'report', 'number', 'solution', '\\n', 'codex', 'produce', 'fail', 'unit', 'test', '\\n', 'time', 'use', 'timeout', '\\n', '3', 'second', 'evaluation', '\\n', 'compensate', 'fact', 'codex', 'fine', 'tune', '\\n', 'apps', 'append', 'single', 'input', 'output', 'example', '\\n\\n', 'task', 'description', 'docstring', 'formatting', 'hint', 'de-', '\\n', 'note', 'setting', '1', 'shot', 'table', '2', 'find', 'codex-', '\\n', '12b', 'evaluate', '1', 'shot', 'achieve', 'comparable', 'performance', '\\n\\n', 'gpt', 'neo', 'model', 'fine', 'tune', 'apps', 'consistent', '\\n', 'early', 'finding', 'large', 'benefit', 'generate', '\\n', 'evaluate', '1000', 'sample', 'task', '\\n', 'difficult', 'problem', 'solution', 'efficient', '\\n', 'pass', 'time', 'limit', 'finally', 'evaluate', '\\n', 'sample', 'pass', '3', 'public', 'unit', 'test', 'problem', '\\n', 'yield', 'high', 'performance', 'raw', 'pass@100', 'sample', '\\n\\n', '4', 'supervised', 'fine', 'tuning', '\\n', 'addition', 'standalone', 'function', 'python', 'code', 'find', '\\n', 'github', 'contain', 'class', 'implementation', 'configuration', 'file', '\\n\\n', 'script', 'file', 'store', 'datum', 'code', 'seem-', '\\n', 'ingly', 'unrelated', 'synthesize', 'function', 'docstring', '\\n\\n', 'hypothesize', 'distribution', 'mismatch', 'reduce', '\\n', 'humaneval', 'performance', '\\n\\n', 'order', 'adapt', 'codex', 'distribution', 'task', 'in-', '\\n', 'terest', 'construct', 'set', 'training', 'problem', 'correctly', '\\n\\n', 'implement', 'standalone', 'function', 'use', 'addi-', '\\n', 'tional', 'supervise', 'fine', 'tuning', 'describe', 'approach', '\\n\\n', 'collect', 'example', 'competitive', 'program-', '\\n', 'ming', 'website', 'repository', 'continuous', 'inte-', '\\n', 'gration', 'supervised', 'fine', 'tune', 'model', 'codex', 's', '\\n\\n', 'produce', 'consistent', 'gain', 'model', '\\n', 'size', '\\n', '4.1', 'problem', 'competitive', 'programming', '\\n', 'programming', 'contest', 'interview', 'preparation', 'website', '\\n\\n', 'use', 'hidden', 'unit', 'test', 'automatically', 'judge', 'func-', '\\n', 'tional', 'correctness', 'submission', 'problem', 'self-', '\\n', 'contain', 'come', 'write', 'problem', 'statement', '\\n\\n', 'generally', 'excellent', 'test', 'coverage', 'additionally', '\\n', 'problem', 'test', 'algorithmic', 'reasoning', 'broad', 'range', '\\n', 'core', 'skill', 'difficulty', '\\n', 'collect', 'problem', 'statement', 'function', 'signature', '\\n', 'solution', 'popular', 'programming', 'contest', '\\n', 'interview', 'preparation', 'website', 'assemble', '\\n', 'programming', 'task', 'similar', 'humaneval', '\\n', 'problem', 'description', 'docstring', 'complete', 'test', '\\n', 'suite', 'hide', 'create', 'unit', 'test', 'example', '\\n', 'find', 'problem', 'statement', 'extract', 'additional', 'test', '\\n', 'case', 'submit', 'incorrect', 'solution', 'total', '\\n', 'curate', '10,000', 'problem', 'way', '\\n', '4.2', 'problem', 'continuous', 'integration', '\\n', 'curate', 'program', 'problem', 'open', 'source', '\\n', 'project', 'take', 'advantage', 'sys.setprofile', '\\n', 'able', 'trace', 'collect', 'input', 'output', '\\n', 'function', 'call', 'integration', 'test', 'datum', '\\n', 'create', 'unit', 'test', 'function', '\\n', 'project', 'employ', 'continuous', 'integration', 'ci', 'ideal', '\\n', 'candidate', 'tracing', 'follow', 'command', 'ci', '\\n', 'configuration', 'file', 'contain', 'build', 'test', 'command', '\\n', 'set', 'virtual', 'environment', 'install', 'dependency', '\\n', 'run', 'integration', 'test', '\\n', 'consider', 'github', 'repos', 'travis', 'tox', 'ci', '\\n', 'framework', 'popular', 'ci', 'tool', '\\n', 'additionally', 'publicly', 'available', 'source', 'code', '\\n', 'pip', 'package', 'find', 'python', 'package', 'index', 'pypi', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'table', '2', 'finetune', 'gpt', 'neo', 'number', 'apps', 'paper', 'reference', 'codex-12b', 'number', 'pass', 'program', '\\n', 'timeout', 'test', 'bracket', 'temperature', '0.6', 'sample', 'cover', 'k', 'pass@k', 'raw', 'pass@1', 'result', '\\n', 'improve', 'low', 'temperature', '\\n\\n', 'introductory', 'interview', 'competition', '\\n', 'gpt', 'neo', '2.7b', 'raw', 'pass@1', '3.90', '0.57', '0.00', '\\n', 'gpt', 'neo', '2.7b', 'raw', 'pass@5', '5.50', '0.80', '0.00', '\\n', '1', 'shot', 'codex', 'raw', 'pass@1', '4.14', '4.33', '0.14', '0.30', '0.02', '0.03', '\\n', '1', 'shot', 'codex', 'raw', 'pass@5', '9.65', '10.05', '0.51', '1.02', '0.09', '0.16', '\\n', '1', 'shot', 'codex', 'raw', 'pass@100', '20.20', '21.57', '2.04', '3.99', '1.05', '1.73', '\\n', '1', 'shot', 'codex', 'raw', 'pass@1000', '25.02', '27.77', '3.70', '7.94', '3.23', '5.85', '\\n', '1', 'shot', 'codex', 'filtered', 'pass@1', '22.78', '25.10', '2.64', '5.78', '3.04', '5.25', '\\n', '1', 'shot', 'codex', 'filtered', 'pass@5', '24.52', '27.15', '3.23', '7.13', '3.08', '5.53', '\\n\\n', 'project', 'contain', 'untrusted', 'code', 'im-', '\\n', 'portant', 'run', 'integration', 'test', 'sandboxed', 'environment', '\\n\\n', 'describe', '\\n', 'million', 'potential', 'function', 'curate', '\\n', 'problem', 'collect', '40,000', '\\n', 'function', 'accept', 'input', 'return', 'output', '\\n', 'object', 'capture', 'runtime', '\\n', 'pickle', 'restore', 'outside', 'sandbox', 'project', '\\n', 'instal', '\\n', 'tracing', 'methodology', 'produce', 'input', 'output', '\\n\\n', 'invoke', 'function', 'builtin', 'library', 'call', 'im-', '\\n', 'port', 'project', 'turn', 'problem', '\\n\\n', 'reason', 'function', 'trace', 'tend', 'building', '\\n', 'block', 'command', 'line', 'utility', 'excel', 'task', '\\n', 'model', 'need', 'know', 'advanced', 'algorithm', '\\n\\n', 'datum', 'structure', 'need', 'able', 'follow', 'in-', '\\n', 'struction', 'implement', 'functionality', 'specify', '\\n\\n', 'docstring', 'trace', 'complement', 'puzzle', 'nature', '\\n', 'code', 'competition', 'problem', 'broaden', 'distribution', '\\n', 'task', '\\n', '4.3', 'filtering', 'problem', '\\n', 'previous', 'section', 'present', 'method', '\\n', 'automatically', 'create', 'training', 'problem', '\\n', 'unclear', 'control', 'quality', 'prompt', '\\n', 'underspecify', 'function', 'implement', '\\n', 'case', 'perfectly', 'valid', 'solution', 'wrongly', 'penalize', '\\n', 'unit', 'test', 'problem', 'stateful', 'subsequent', '\\n', 'execution', 'result', 'different', 'outcome', '\\n', 'address', 'issue', 'use', 'codex-12b', 'generate', '100', '\\n', 'sample', 'curate', 'problem', 'sample', 'pass', 'unit', '\\n', 'test', 'consider', 'task', 'ambiguous', '\\n', 'difficult', 'filter', 'rerun', 'verification', '\\n', 'time', 'remove', 'stateful', 'non', 'deterministic', 'problem', '\\n\\n', '4.4', 'method', '\\n', 'fine', 'tune', 'codex', 'training', 'problem', 'produce', '\\n\\n', 'set', 'supervised', 'fine', 'tune', 'model', 'codex-', '\\n', 's.', 'produce', 'example', 'training', 'problem', 'assem-', '\\n', 'ble', 'problem', 'format', 'show', 'figure', '2', '\\n\\n', 'prompt', 'vary', 'length', 'batch', 'leave', 'pad', 'short', '\\n', 'prompt', 'length', 'long', 'prompt', '\\n', 'token', 'reference', 'solution', 'line', 'context', '\\n', 'train', 'minimize', 'negative', 'log', 'likelihood', 'reference', '\\n', 'solution', 'mask', 'loss', 'token', 'prompt', '\\n', 'train', 'learning', 'rate', '1/10', 'large', '\\n', 'fine', 'tuning', 'codex', 'adhere', 'learning', 'rate', '\\n', 'schedule', 'train', 'validation', 'loss', 'plateaus', '\\n', '10b', 'token', '\\n', '4.5', 'result', '\\n', 'codex', 'compute', 'optimal', 'temperature', '\\n', 'evaluate', 'pass@k', '1', '≤', 'k', '≤', '100', 'find', 'codex', 's', '\\n', 'prefer', 'slightly', 'high', 'temperature', 'k', '>', '1', '\\n', 'possibly', 'reflect', 'fact', 'codex', 's', 'capture', 'narrow', '\\n', 'distribution', 'codex', 'use', 't', '\\n\\n', '∗', '=', '0', 'computing', '\\n\\n', 'pass@1', 't', '\\n\\n', '∗', '=', '1', 'compute', 'pass@100', '\\n', 'compare', 'codex', 's', 'codex', 'pass@1', '\\n', 'pass@100', 'codex', 's', 'outperform', 'corresponding', 'codex', '\\n', 'average', 'margin', '6.5', 'percentage', 'point', 'pass@1', '\\n', 'large', 'average', 'margin', '15.1', 'percentage', 'point', '\\n', 'pass@100', 'model', 'size', '\\n', 'plot', 'performance', 'different', 'sample', 'selection', '\\n', 'heuristic', 'codex', 's-12b', 'heuristic', '\\n', 'codex-12b.', 'rank', '1', '100', 'sample', '\\n', 'mean', 'log', 'probability', 'average', 'benefit', 'random', '\\n', 'ranking', '11.6', 'percentage', 'point', '2', 'percentage', '\\n', 'point', 'high', 'corresponding', 'benefit', 'codex', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'figure', '9', 'optimal', 'sampling', 'temperature', 'function', 'num-', '\\n', 'ber', 'sample', 'generate', 'codex', 'codex', 's.', 'codex', 's', '\\n\\n', 'generally', 'require', 'high', 'temperature', 'particular', 'value', '\\n', 'k', 'possibly', 'compensate', 'fact', 'model', 'narrow', '\\n', 'distribution', '\\n\\n', 'figure', '10', 'compare', 'codex', 's', 'codex', 'metric', 'pro-', '\\n', 'pose', 'section', '3', 'codex', 's', 'order', 'magnitude', '\\n\\n', 'parameter', 'efficient', 'pass@1', 'pass@100', 'log', 'prob', '\\n', 'sample', 'rank', 'codex', 's', 'yield', 'similar', 'benefit', 'random', '\\n', 'sampling', 'codex', '\\n\\n', '5', 'docstre', 'generation', '\\n', 'generating', 'code', 'docstring', 'possible', 'codex', '\\n', 'code', 'typically', 'follow', 'docstring', '\\n\\n', 'easy', 'induce', 'codex', 'generate', 'docstring', 'code', 'nev-', '\\n', 'ertheless', 'motivate', 'produce', 'docstring', 'writing', '\\n\\n', 'model', 'safety', 'reason', 'model', 'de-', '\\n', 'scribe', 'intent', 'generate', 'code', 'training', '\\n\\n', 'problem', 'describe', 'previous', 'section', 'eas-', '\\n', 'ily', 'create', 'training', 'dataset', 'code', 'conditional', 'docstring', '\\n\\n', 'generation', '\\n\\n', 'specifically', 'training', 'problem', 'assemble', 'train-', '\\n', 'ing', 'example', 'concatenate', 'function', 'signature', '\\n\\n', 'reference', 'solution', 'docstring', 'train', '\\n\\n', 'codex', 's', 'minimize', 'negative', 'log', 'likelihood', 'ref-', '\\n', 'erence', 'solution', 'train', 'docstring', 'generating', 'model', '\\n\\n', 'codex', 'd', 'minimize', 'negative', 'log', 'likelihood', 'doc-', '\\n', 'string', '\\n\\n', 'benchmark', 'code', 'generation', 'model', 'mea-', '\\n', 'sure', 'pass@k', 'humaneval', 'dataset', 'correctness', '\\n\\n', 'define', 'pass', 'set', 'unit', 'test', '\\n', 'similar', 'way', 'evaluate', 'docstre', 'sample', 'automatically', '\\n', 'grade', 'sample', 'docstring', 'hand', 'consider', '\\n', 'docstring', 'correct', 'uniquely', 'accurately', 'specify', '\\n', 'code', 'body', 'time', 'consume', 'nature', '\\n', 'process', 'grade', '10', 'sample', 'problem', 'total', '\\n', '1640', 'problem', 'codex', 'd-12b', 'temperature', '0.8', '\\n', 'codex', 'd', 'generate', 'incorrect', 'unit', 'test', '\\n', 'docstring', 'ignore', 'grading', '\\n', 'consider', 'docstring', 'correct', 'model', '\\n', 'simply', 'copy', 'code', 'body', 'docstring', '\\n', 'common', 'failure', 'mode', 'observe', 'docstring', '\\n', 'model', 'leave', 'important', 'detail', 'answer', '\\n', 'decimal', 'place', 'condition', '\\n', 'function', 'invent', 'problem', 'unrelated', '\\n', 'function', 'body', '\\n', 'show', 'table', '3', 'pass', 'rate', 'codex', 'd', 'low', '\\n', 'comparable', 'corresponding', 'pass', 'rate', 'codex', 's', '\\n', 'temperature', 'strong', 'hypothesis', '\\n', 'direction', 'yield', 'high', 'pass', 'rate', '\\n\\n', 'generate', 'docstring', 'forgiving', 'natu-', '\\n', 'ral', 'language', 'syntax', 'strict', 'code', 'syntax', 'docstring', '\\n\\n', 'dataset', 'low', 'quality', 'developer', 'tend', '\\n', 'devote', 'time', 'write', 'docstring', 'model', '\\n', 'produce', 'docstring', 'like', 'find', 'function', 'online', '\\n\\n', 'test', 'correctly', 'write', 'solu-', '\\n', 'tion', '\\n\\n', 'finally', 'docstring', 'model', 'way', '\\n\\n', 'choose', 'single', 'sample', 'set', 'k', 'sample', 'in-', '\\n', 'stead', 'pick', 'sample', 'good', 'mean', 'log', 'proba-', '\\n', 'bility', 'investigate', 'previous', 'section', '\\n\\n', 'choose', 'sample', 'maximize', 'translation', 'ob-', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'table', '3', 'pass', 'rate', 'docstring', 'generating', 'model', 'codex', 'd', '\\n', 'evaluate', 'hand', 'grade', '10', 'sample', 'task', '\\n', 'lack', 'ground', 'truth', 'automatic', 'evaluation', 'find', 'similar', '\\n', 'low', 'pass', 'rate', 'compare', 'codex', 's.', '\\n', 'model', 'pass@1', 'pass@10', '\\n', 'codex', 's-12b', '32.2', '59.5', '\\n', 'codex', 'd-12b', '20.3', '46.5', '\\n\\n', 'jective', 'p(ground', 'truth', 'docstring|generate', 'sample', '\\n', 'p', 'evaluate', 'codex', 'd.', 'unfortunately', 'figure', '7', '\\n\\n', 'rank', 'sample', 'translation', 'under-', '\\n', 'perform', 'mean', 'log', 'probability', 'ranking', 'outper-', '\\n', 'form', 'random', 'ranking', 'heuristic', 'appear', 'overfit', '\\n\\n', 'quickly', '\\n', '6', 'limitation', '\\n', 'codex', 'able', 'sample', 'correct', 'solution', '\\n', 'majority', 'humaneval', 'problem', 'find', '\\n', 'number', 'limitation', '\\n', 'codex', 'sample', 'efficient', 'train', 'training', '\\n', 'dataset', 'comprise', 'significant', 'fraction', 'publicly', 'available', '\\n', 'python', 'code', 'github', 'total', 'hundred', 'million', '\\n', 'line', 'code', 'seasoned', 'developer', 'encounter', '\\n\\n', 'near', 'code', 'career', 'in-', '\\n', 'deed', 'strong', 'student', 'complete', 'introductory', 'com-', '\\n', 'puter', 'science', 'course', 'expect', 'able', 'solve', 'large', '\\n\\n', 'fraction', 'problem', 'codex-12b.', '\\n', 'explore', 'prompt', 'codex', 'likely', 'fail', '\\n', 'display', 'counter', 'intuitive', 'behavior', 'evaluate', 'code', '\\n\\n', 'generation', 'study', 'xu', 'et', 'al', '2021', 'helmuth', 'spec-', '\\n', 'tor', '2015', 'pantridge', 'et', 'al', '2017', 'exist', 'metric', '\\n\\n', 'measure', 'performance', 'tightly', 'specify', 'constrained', 'prob-', '\\n', 'lem', 'instance', 'e.g.', 'string', 'manipulation', 'flashfill', 'gul-', '\\n', 'wani', '2011', 'develop', 'set', 'qualitative', '\\n\\n', 'metric', 'measure', 'capability', 'code', 'generating', '\\n\\n', 'model', 'control', 'complexity', 'abstrac-', '\\n', 'tion', 'level', 'specification', 'appendix', 'd', 'apply', '\\n\\n', 'framework', 'find', 'codex', 'recommend', 'syntacti-', '\\n', 'cally', 'incorrect', 'undefined', 'code', 'invoke', 'function', '\\n\\n', 'variable', 'attribute', 'undefined', 'outside', '\\n', 'scope', 'codebase', 'codex', 'struggle', 'parse', '\\n', 'increasingly', 'long', 'high', 'level', 'system', 'level', '\\n', 'specification', '\\n', 'concretely', 'illustrate', 'model', 'performance', 'degradation', '\\n', 'docstre', 'length', 'increase', 'create', 'dataset', 'synthetic', '\\n', 'problem', 'assemble', '13', 'basic', 'building', 'block', '\\n\\n', 'modify', 'input', 'string', 'deterministic', 'way', 'ex-', '\\n', 'ample', 'building', 'block', 'convert', 'string', 'lowercase', '\\n\\n', 'remove', 'character', 'string', '\\n\\n', 'list', 'describe', 'appendix', 'c', 'find', 'number', '\\n', 'chain', 'building', 'block', 'docstring', 'increase', 'model', '\\n\\n', 'performance', 'decrease', 'exponentially', 'behavior', 'un-', '\\n', 'characteristic', 'human', 'programmer', 'able', '\\n\\n', 'correctly', 'implement', 'program', 'chain', 'arbitrary', '\\n', 'length', 'chain', 'length', '\\n\\n', 'figure', '11', 'pass', 'rate', 'codex-12b', 'sample', 'number', '\\n', 'chain', 'component', 'synthetically', 'generate', 'docstring', '\\n', 'additional', 'component', 'pass', 'rate', 'drop', 'roughly', 'factor', '\\n', '2', '3', '\\n', 'text', 'conditional', 'generative', 'model', '\\n\\n', 'modality', 'ramesh', 'et', 'al', '2021', 'difficulty', 'bind-', '\\n', 'ing', 'attribute', 'object', 'codex', 'mistake', 'bind', '\\n\\n', 'operation', 'variable', 'especially', 'number', 'oper-', '\\n', 'ation', 'variable', 'docstring', 'large', 'instance', '\\n\\n', 'following', 'prompt', 'codex-12b', 'decrement', '\\n', 'variable', 'w', 'fail', 'return', 'product', 'number', '\\n', 'def', 'do_work(x', 'y', 'z', 'w', '\\n', 'add', '3', 'y', 'subtract', '4', '\\n', 'x', 'w.', 'return', '\\n', 'product', 'number', '\\n', 't', '=', 'y', '+', '3', '\\n', 'u', '=', 'x', '4', '\\n', 'v', '=', 'z', 'w', '\\n', 'return', 'v', '\\n\\n', 'understanding', 'codex', 'limited', 'system', 'level', 'synthe-', '\\n', 'sis', 'capability', 'help', 'inform', 'assessment', 'potential', '\\n\\n', 'hazard', 'generative', 'capacity', '\\n', 'broad', 'societal', 'impact', 'system', '\\n', '7', 'broad', 'impacts', 'hazard', 'analysis', '\\n', 'codex', 'potential', 'useful', 'range', 'way', '\\n', 'example', 'help', 'onboard', 'user', 'new', 'codebase', '\\n', 'reduce', 'context', 'switching', 'experienced', 'coder', 'enable', '\\n', 'non', 'programmer', 'write', 'specification', 'codex', '\\n', 'draft', 'implementation', 'aid', 'education', 'exploration', '\\n', 'codex', 'raise', 'significant', 'safety', 'challenge', '\\n', 'produce', 'code', 'align', 'user', 'intent', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'potential', 'misuse', '\\n', 'well', 'understand', 'hazard', 'codex', '\\n', 'generative', 'capacity', 'conduct', 'hazard', 'analysis', '\\n', 'focus', 'identify', 'risk', 'factor', 'leveson', '2019', '\\n', 'potential', 'cause', 'harm.1', 'outline', 'key', '\\n', 'finding', 'risk', 'area', '\\n', 'finding', 'potential', 'societal', '\\n', 'impact', 'code', 'generation', 'system', 'inform', 'work', '\\n', 'responsible', 'deployment', 'production', 'orient', '\\n', 'codex', 'model', 'descend', 'research', 'orient', '\\n', 'codex', 'model', 'describe', 'paper', 'section', '\\n', 'intend', 'provide', 'account', 'particular', 'product', '\\n', 'safety', 'feature', 'specify', 'anchor', '\\n', 'analysis', 'specific', 'property', 'model', 'describe', '\\n', 'paper', 'share', 'analysis', 'belief', '\\n', 'generalize', 'broad', 'class', 'code', 'generation', '\\n', 'system', 'encourage', 'norm', 'perform', 'detail', '\\n', 'impact', 'analysis', 'major', 'machine', 'learn', 'research', '\\n', 'project', '\\n', 'note', 'focus', 'largely', 'risk', 'section', '\\n', 'mean', 'imply', 'expect', 'impact', 'class', '\\n', 'technology', 'net', 'negative', 'risk', 'merit', 'particular', '\\n\\n', 'attention', 'subtle', 'require', 'deliber-', '\\n', 'eat', 'effort', 'address', 'expect', 'benefit', '\\n\\n', 'obvious', 'automatic', 'perspective', '\\n', 'user', 'affected', 'stakeholder', '\\n', '7.1', 'reliance', '\\n', 'key', 'risk', 'associate', 'code', 'generation', '\\n', 'model', 'practice', 'reliance', 'generate', 'output', '\\n', 'limitation', 'describe', 'alignment', '\\n', 'issue', 'describe', 'codex', 'suggest', 'solution', '\\n', 'superficially', 'appear', 'correct', 'actually', 'perform', '\\n', 'task', 'user', 'intend', 'particularly', 'affect', 'novice', '\\n', 'programmer', 'significant', 'safety', 'implication', '\\n', 'depend', 'context', 'discuss', 'related', 'issue', '\\n\\n', 'appendix', 'g', 'code', 'generation', 'model', 'sug-', '\\n', 'g', 'insecure', 'code', 'reason', 'human', 'oversight', '\\n\\n', 'vigilance', 'require', 'safe', 'use', 'code', 'generation', 'system', '\\n', 'like', 'codex', '\\n', 'note', 'immediate', 'way', 'improve', 'safety', '\\n', 'subsection', 'risk', 'mitigation', 'reliance', '\\n', 'particular', 'believe', 'merit', 'inquiry', '\\n\\n', 'industry', 'academia', 'conceptually', 'straight-', '\\n', '1we', 'seek', 'include', 'harm', 'span', 'geographic', 'temporal', '\\n\\n', 'scale', 'consider', 'severity', 'probability', '\\n', 'distribution', 'harm', 'note', '\\n', 'analysis', 'describe', 'milestone', 'hope', '\\n', 'large', 'cross', 'sectoral', 'cross', 'organizational', 'effort', 'steer', '\\n', 'code', 'generation', 'societally', 'beneficial', 'direction', 'describe', '\\n', 'finding', 'note', 'specific', 'uncertainty', 'area', '\\n', 'future', 'work', 'different', 'section', '\\n\\n', 'figure', '12', 'prompt', 'include', 'subtle', 'bug', 'codex', 'tend', '\\n', 'produce', 'bad', 'code', 'capable', 'persist', '\\n', 'prompt', 'include', 'instruction', 'write', 'correct', 'code', 'gap', '\\n', 'increase', 'model', 'size', '\\n', 'forward', 'provide', 'documentation', 'user', 'remind', '\\n\\n', 'model', 'limitation', 'empirical', 'investigation', 'neces-', '\\n', 'sary', 'order', 'identify', 'reliably', 'ensure', 'vigilance', '\\n\\n', 'practice', 'range', 'user', 'experience', 'level', 'ui', 'design', '\\n', 'task', 'challenge', 'researcher', 'consider', '\\n', 'capability', 'improve', 'increasingly', 'difficult', '\\n', 'guard', 'automation', 'bias', '\\n', '7.2', 'misalignment', '\\n', 'large', 'language', 'model', 'train', 'token', '\\n\\n', 'prediction', 'objective', 'codex', 'generate', 'code', 'sim-', '\\n', 'ilar', 'possible', 'training', 'distribution', 'consequence', '\\n\\n', 'model', 'thing', 'unhelpful', '\\n', 'user', 'despite', 'have', 'capability', 'helpful', '\\n', 'figure', '12', 'example', 'user', 'subtle', '\\n', 'mistake', 'code', 'codex', 'deliberately', 'suggest', '\\n', 'code', 'superficially', 'appear', 'good', 'incorrect', '\\n', 'alignment', 'failure', 'model', 'align', '\\n', 'user', 'intention', 'informally', 'system', 'misalign', '\\n', 'task', 'x', 'want', 'capable', '\\n', 'x', 'choose', 'contrast', 'system', '\\n', 'fail', 'x', 'ability', '\\n', 'system', 'misalign', 'incompetent', '\\n', 'appendix', 'e', 'detail', 'include', 'precise', '\\n', 'definition', 'alignment', '\\n', 'important', 'study', 'misalignment', 'problem', '\\n\\n', 'likely', 'bad', 'well', 'capabili-', '\\n', 'tie', 'system', 'increase', 'example', 'model', 'size', '\\n\\n', 'scale', 'trend', 'example', 'figure', '12', 'indicate', '\\n', 'misalignment', 'likely', 'persist', 'bad', '\\n', 'datum', 'parameter', 'training', 'time', 'scale', '\\n\\n', 'expect', 'misaligned', 'behaviour', 'like', 'un-', '\\n', 'likely', 'cause', 'significant', 'harm', 'current', 'model', 'likely', '\\n\\n', 'dangerous', 'hard', 'eliminate', 'model', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'capability', 'increase', 'highly', 'capable', 'sufficiently', 'mis-', '\\n', 'align', 'model', 'train', 'user', 'approval', 'produce', 'ob-', '\\n', 'fuscated', 'code', 'look', 'good', 'user', 'careful', '\\n\\n', 'inspection', 'fact', 'undesirable', '\\n', 'harmful', '\\n', '7.3', 'bias', 'representation', '\\n', 'mirror', 'find', 'case', 'language', '\\n\\n', 'model', 'train', 'internet', 'datum', 'bender', 'et', 'al', '2021', 'blod-', '\\n', 'gett', 'et', 'al', '2020', 'abid', 'et', 'al', '2021', 'brown', 'et', 'al', '2020', '\\n\\n', 'find', 'codex', 'prompt', 'way', 'generate', '\\n', 'racist', 'denigratory', 'harmful', 'output', 'code', '\\n', 'comment', 'merit', 'intervention', 'discuss', '\\n', 'subsection', 'risk', 'mitigation', 'find', '\\n\\n', 'code', 'generation', 'model', 'raise', 'bias', 'represen-', '\\n', 'tation', 'issue', 'problematic', 'natural', 'language', 'codex', '\\n\\n', 'generate', 'code', 'structure', 'reflect', 'stereotype', '\\n', 'gender', 'race', 'emotion', 'class', 'structure', 'name', '\\n', 'characteristic', 'particularly', 'context', 'user', '\\n\\n', 'rely', 'codex', 'use', 'think-', '\\n', 'ing', 'project', 'design', 'issue', 'significant', '\\n\\n', 'safety', 'implication', 'give', 'motivation', 'discourage', '\\n', 'reliance', 'discuss', 'bias', 'representation', 'issue', '\\n', 'appendix', 'f.', 'filtration', 'modulation', 'generate', '\\n', 'output', 'documentation', 'intervention', 'help', '\\n', 'mitigate', 'risk', '\\n', '7.4', 'economic', 'labor', 'market', 'impact', '\\n', 'code', 'generation', 'associate', 'capability', '\\n', 'possible', 'economic', 'labor', 'market', 'impact', 'codex', '\\n', 'current', 'capability', 'level', 'somewhat', 'reduce', 'cost', '\\n\\n', 'produce', 'software', 'increase', 'programmer', 'produc-', '\\n', 'tivity', 'size', 'effect', 'limit', 'fact', '\\n\\n', 'engineer', 'spend', 'day', 'write', 'code', 'o*net', '\\n\\n', '2021', 'important', 'task', 'include', 'confer', 'col-', '\\n', 'league', 'write', 'design', 'specification', 'upgrade', 'ex-', '\\n', 'isting', 'software', 'stacks.2', 'find', 'codex', 'import', '\\n\\n', 'package', 'different', 'rate', 'advantage', '\\n', 'package', 'author', 'particularly', 'programmer', '\\n', 'engineer', 'come', 'rely', 'codex', 'suggestion', '\\n', 'long', 'time', 'horizon', 'effect', 'class', 'technology', '\\n', 'software', 'relate', 'labor', 'market', 'economy', '\\n', 'generally', 'substantial', 'capability', 'improve', '\\n\\n', 'study', 'need', 'effect', 'code', 'genera-', '\\n', 'tion', 'capability', 'appropriate', 'response', 'discuss', '\\n\\n', 'economic', 'labor', 'market', 'implication', 'detail', '\\n', 'appendix', 'h.', '\\n', '2', '\\n', 'bls', 'classifie', 'computer', 'programmer', 'software', '\\n', 'developer', 'separately', 'developer', 'highly', 'pay', '\\n', 'programmer', 'task', 'indirectly', 'relate', 'writing', '\\n', 'interact', 'code', 'project', '\\n', 'great', 'demand', '10', 'year', 'li', 'et', 'al', '2020', 'bureau', '\\n', 'labor', 'statistics', '2021a;b', '\\n\\n', '7.5', 'security', 'implication', '\\n', 'codex', 'effect', 'security', 'landscape', '\\n', 'codex', 'produce', 'vulnerable', 'misaligned', 'code,3', '\\n\\n', 'qualified', 'operator', 'review', 'generation', 'ex-', '\\n', 'ecuting', 'trust', 'absent', 'appropriate', 'precaution', '\\n\\n', 'future', 'code', 'generation', 'model', 'able', 'train', '\\n', 'produce', 'secure', 'code', 'average', 'developer', '\\n', 'far', 'certain', '\\n', 'codex', 'misuse', 'aid', 'cybercrime', '\\n', 'worthy', 'concern', 'base', 'testing', 'believe', '\\n', 'current', 'level', 'capability', 'codex', 'model', '\\n\\n', 'materially', 'low', 'barrier', 'entry', 'malware', 'devel-', '\\n', 'opment.4', 'expect', 'powerful', 'code', 'generation', '\\n\\n', 'model', 'lead', 'future', 'advancement', 'fur-', '\\n', 'ther', 'research', 'mitigation', 'continue', 'study', 'model', '\\n\\n', 'capability', 'necessary', '\\n', 'non', 'deterministic', 'nature', 'system', 'like', 'codex', '\\n', 'enable', 'advanced', 'malware', 'non', 'determinism', '\\n', 'make', 'easy', 'create', 'diverse', 'software', 'accomplish', '\\n', 'task', 'software', 'diversity', '\\n', 'aid', 'defenders,5', '\\n\\n', 'present', 'unique', 'challenge', 'traditional', '\\n\\n', 'malware', 'detection', 'antivirus', 'system', 'rely', 'finger-', '\\n', 'printing', 'signature', 'matching', 'previously', 'sample', '\\n\\n', 'binary', 'example', 'capable', 'code', 'generation', '\\n', 'model', 'conceivably', 'advance', 'technique', 'generate', '\\n\\n', 'polymorphic', 'malware.6', 'believe', 'application', 'secu-', '\\n', 'rity', 'model', 'deployment', 'strategy', 'include', 'rate', 'limit', '\\n\\n', 'access', 'abuse', 'monitoring', 'manage', 'threat', '\\n', 'near', 'term', 'efficacy', 'mitigation', '\\n', 'scale', 'sublinearly', 'capable', 'model', 'develop', '\\n', 'similar', 'large', 'language', 'model', 'codex', 'model', 'learn', '\\n', 'pattern', 'present', 'training', 'datum', 'carlini', 'et', 'al', '2021', '\\n\\n', 'sensitive', 'datum', 'present', 'source', 'code', 'liable', 'pre-', '\\n', 'dicte', 'model', 'codex', 'train', 'public', '\\n\\n', 'repository', 'consider', 'sensitive', 'datum', 'present', '\\n', 'training', 'datum', 'compromise', 'similarly', '\\n', 'public', 'datum', 'generally', 'treat', 'untrusted', '\\n', 'previous', 'work', 'goldblum', 'et', 'al', '2021', 'schuster', 'et', 'al', '2020', '\\n', 'find', 'attacker', 'able', 'corrupt', 'training', 'datum', '\\n', 'trigger', 'specific', 'model', 'behavior', 'runtime', '\\n', 'discuss', 'security', 'implication', 'appendix', 'g.', '\\n', '3', '\\n\\n', 'appendix', 'g', 'insecure', 'code', 'example', 'codex', 'pro-', '\\n', 'duce', 'insecure', 'code', '\\n\\n', '4', '\\n', 'characterize', 'codex', 'capability', 'limitation', '\\n', 'limitations', 'section', 'experiment', 'security', 'analysis', '\\n', 'appendix', 'g.', '\\n', '5', '\\n', 'example', 'help', 'prevent', 'certain', 'type', 'memory', '\\n', 'corruption', 'vulnerability', 'davis', '2018', '\\n', '6', '\\n\\n', 'polymorphic', 'malware', 'malicious', 'code', 'mutate', 'im-', '\\n', 'plementation', 'maintain', 'function', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', '7.6', 'environmental', 'impact', '\\n', 'codex', 'like', 'large', 'generative', 'model', 'energy', '\\n', 'footprint', 'training', 'inference', 'schwartz', 'et', 'al', '\\n\\n', '2019', 'bender', 'et', 'al', '2021', 'patterson', 'et', 'al', '2021', 'origi-', '\\n', 'nal', 'training', 'gpt-3', '12b', 'consume', 'hundred', 'petaflop', 's-', '\\n', 'day', 'compute', 'fine', 'tune', 'create', 'codex-12b', '\\n\\n', 'consume', 'similar', 'compute', 'training', '\\n', 'perform', 'platform', 'azure', 'purchase', 'carbon', '\\n', 'credit', 'source', 'significant', 'amount', 'renewable', 'energy', '\\n', 'reduce', 'carbon', 'footprint.7', 'compute', 'consumption', '\\n\\n', 'cost', 'wide', 'supply', 'chain', 'con-', '\\n', 'centrate', 'certain', 'regions.8', 'look', 'globally', '\\n\\n', 'long', 'term', 'compute', 'demand', 'code', 'generation', '\\n', 'grow', 'large', 'codex', 'training', 'significant', '\\n', 'inference', 'tackle', 'challenge', 'problems.9', '\\n', '7.7', 'legal', 'implication', '\\n', 'legal', 'consideration', 'relate', 'generate', '\\n', 'code', 'begin', 'training', 'ai', 'system', 'internet', '\\n', 'datum', 'public', 'github', 'repository', 'previously', '\\n', 'identify', 'instance', 'fair', 'use', 'o’keefe', 'et', 'al', '\\n', '2019', '\\n', 'preliminary', 'research', 'find', 'codex', 'model', 'rarely', '\\n', 'generate', 'code', 'identical', 'content', 'training', '\\n', 'datum', 'occurrence', '<', '0.1', 'study', 'examine', '\\n', 'frequency', 'code', 'generation', 'appear', 'match', 'code', '\\n', 'snippet', 'training', 'datum', 'ziegler', '2021', 'rare', '\\n\\n', 'instance', 'generate', 'code', 'consist', 'common', 'expres-', '\\n', 'sion', 'convention', 'programming', 'language', '\\n\\n', 'appear', 'training', 'datum', 'find', '\\n', 'extent', 'generate', 'code', 'appear', 'identical', '\\n', 'training', 'datum', 'predictive', 'weighting', '\\n', 'model', 'retention', 'copying', 'specific', 'code', '\\n', 'generate', 'code', 'responsive', 'customize', '\\n', 'user', 'input', 'user', 'retain', 'complete', 'control', '\\n', 'editing', 'acceptance', 'generated', 'code', '\\n', 'code', 'generation', 'similar', 'auto', 'suggest', 'auto', 'completion', '\\n\\n', '7microsoft', 'commitment', '2020', 'shift', '100', 'per-', '\\n', 'cent', 'renewable', 'energy', 'supply', 'building', 'datum', 'center', '\\n\\n', '2025', 'https://blogs.microsoft.com/blog/2020/01/16/microsoft-', '\\n', 'carbon', 'negative', 'by-2030/', 'assessment', 'envi-', '\\n', 'ronmental', 'impact', 'compute', 'use', 'impossible', 'conduct', '\\n\\n', 'ground', 'context', 'make', 'comparison', 'counterfactual', '\\n', 'impact', 'compete', 'product', 'service', 'analysis', '\\n', 'scope', 'paper', '\\n\\n', '8while', 'datum', 'center', 'energy', 'usage', 'effi-', '\\n', 'cient', 'recent', 'year', 'masanet', 'et', 'al', '2020', 'production', 'use', '\\n\\n', 'disposal', 'semiconductor', 'impose', 'environmental', '\\n', 'human', 'cost', 'e.g.', 'crawford', '2021', '\\n', '9given', 'code', 'generation', 'form', 'ai', '\\n', 'deploy', 'widely', 'economy', 'discuss', '\\n', 'consideration', 'suggest', 'additional', 'urgency', 'adopt', 'renewable', '\\n', 'energy', '\\n\\n', 'feature', 'exist', 'feature', 'tool', 'authorship', '\\n', 'e.g.', 'document', 'editor', 'sense', 'finished', 'work', '\\n', 'see', 'author', '\\n\\n', 'commitment', 'responsible', 'safe', 'ai', 'include', 'con-', '\\n', 'tinue', 'attention', 'broad', 'intellectual', 'property', 'impli-', '\\n', 'cation', 'code', 'generation', 'system', 'intend', 'remain', '\\n\\n', 'engaged', 'policymaker', 'expert', 'issue', '\\n', 'user', 'system', 'ultimately', 'deploy', '\\n', 'confidence', '\\n', '7.8', 'risk', 'mitigation', '\\n', 'closing', 'give', 'model', 'like', 'codex', '\\n', 'develop', 'capability', 'explore', 'carefully', '\\n\\n', 'eye', 'maximize', 'positive', 'social', 'im-', '\\n', 'pact', 'minimize', 'intentional', 'unintentional', 'harm', '\\n\\n', 'use', 'cause', 'contextual', 'approach', 'critical', '\\n', 'effective', 'hazard', 'analysis', 'mitigation', 'broad', '\\n', 'category', 'mitigation', 'important', 'consider', '\\n', 'deployment', 'code', 'generation', 'model', '\\n\\n', 'careful', 'documentation', 'user', 'interface', 'design', 'code', 're-', '\\n', 'view', 'requirement', 'and/or', 'content', 'control', 'e.g.', 'filtering', '\\n\\n', 'output', 'help', 'reduce', 'harm', 'associate', 'over-', '\\n', 'reliance', 'offensive', 'content', 'insecure', 'code', 'gener-', '\\n', 'ation', 'context', 'model', 'available', 'service', '\\n\\n', 'e.g.', 'api', 'policy', 'user', 'review', 'use', 'case', '\\n', 'restriction', 'monitoring', 'and/or', 'rate', 'limiting', 'help', '\\n', 'reduce', 'harm', 'associate', 'malicious', 'use', 'prevent', '\\n', 'use', 'high', 'stake', 'domain', 'model', '\\n', 'suited', '\\n', 'appendices', 'e', 'f', 'g', 'h', 'provide', 'detail', 'risk', '\\n', 'describe', 'section', 'outline', 'additional', 'mitigation', '\\n', 'research', 'opportunity', '\\n', '8', 'related', 'work', '\\n', 'deep', 'learning', 'resurgence', 'lead', 'strong', 'advance', '\\n', 'field', 'program', 'learning', 'popular', 'approach', '\\n', 'neural', 'program', 'learning', 'program', 'induction', 'program', '\\n', 'synthesis', '\\n', 'program', 'induction', 'model', 'generate', 'program', 'output', '\\n', 'directly', 'latent', 'program', 'representation', 'learn', '\\n', 'execute', 'zaremba', 'sutskever', '2014', 'demonstrate', '\\n\\n', 'model', 'execute', 'simple', 'task', 'like', 'addition', 'memo-', '\\n', 'rization', 'later', 'attempt', 'program', 'induction', 'incorporate', '\\n\\n', 'inductive', 'bias', 'base', 'modern', 'computing', 'device', '\\n', 'neural', 'turing', 'machine', 'graves', 'et', 'al', '2014', 'memory', '\\n', 'network', 'weston', 'et', 'al', '2015', 'sukhbaatar', 'et', 'al', '2015', '\\n\\n', 'neural', 'gpu', 'kaiser', 'sutskever', '2015', 'differen-', '\\n', 'tiable', 'neural', 'computer', 'graves', 'et', 'al', '2016', 'recent', '\\n\\n', 'approach', 'like', 'neural', 'program', 'interpreter', 'reed', '\\n', 'de', 'freitas', '2016', 'shin', 'et', 'al', '2018', 'pierrot', 'et', 'al', '2021', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'universal', 'transformer', 'dehghani', 'et', 'al', '2019', 'find', 'recur-', '\\n', 'rence', 'useful', 'component', 'program', 'induction', '\\n\\n', 'program', 'synthesis', 'model', 'explicitly', 'generate', 'pro-', '\\n', 'gram', 'usually', 'natural', 'language', 'specification', '\\n\\n', 'popular', 'classical', 'approach', 'probabilis-', '\\n', 'tic', 'context', 'free', 'grammar', 'pcfg', 'generate', 'program', '\\n\\n', 'abstract', 'syntax', 'tree', 'ast', 'maddison', 'tarlow', '2014', 'im-', '\\n', 'prove', 'setup', 'learn', 'state', 'vector', 'con-', '\\n', 'dition', 'child', 'node', 'expansion', 'later', 'allamanis', 'et', 'al', '2015', '\\n\\n', 'apply', 'idea', 'text', 'code', 'retrieval', 'yin', 'neu-', '\\n', 'big', '2017', 'utilize', 'text', 'conditional', 'code', 'generation', '\\n\\n', 'code2seq', 'alon', 'et', 'al', '2018', 'find', 'ast', '\\n', 'leveraged', 'code', 'text', 'generation', '\\n', 'program', 'synthesize', 'pass', '\\n', 'ast', 'representation', 'hindle', 'et', 'al', '2012', 'investigate', '\\n', 'n', 'gram', 'language', 'model', 'code', 'find', 'code', '\\n\\n', 'predictable', 'natural', 'language', 'latent', 'predictor', 'net-', '\\n', 'work', 'ling', 'et', 'al', '2016', 'show', 'character', 'level', 'lan-', '\\n', 'guage', 'model', 'generate', 'work', 'code', 'implement-', '\\n', 'ing', 'magic', 'gathering', 'card', 'online', 'arena', '\\n\\n', 'aid', 'latent', 'mode', 'allow', 'card', 'attribute', '\\n', 'copy', 'code', 'deepcoder', 'balog', 'et', 'al', '2017', 'train', '\\n', 'model', 'predict', 'function', 'appear', 'source', 'code', '\\n', 'guide', 'program', 'search', '\\n\\n', 'follow', 'success', 'large', 'natural', 'language', 'model', 'de-', '\\n', 'vlin', 'et', 'al', '2018', 'radford', 'et', 'al', '2019', 'liu', 'et', 'al', '2019', 'raffel', '\\n\\n', 'et', 'al', '2020', 'brown', 'et', 'al', '2020', 'large', 'scale', 'transformer', '\\n\\n', 'apply', 'program', 'synthesis', 'code-', '\\n', 'bert', 'feng', 'et', 'al', '2020', 'train', 'bert', 'objective', '\\n\\n', 'docstring', 'pair', 'function', 'obtain', 'strong', 'result', '\\n', 'code', 'search', 'pymt5', 'clement', 'et', 'al', '2020', 'similar', '\\n\\n', 'spirit', 'work', 't5', 'objective', 'train', 'sys-', '\\n', 'tem', 'translate', 'non', 'overlapping', 'subset', '\\n\\n', 'signature', 'docstring', 'body', '\\n', 'functional', 'correctness', 'benchmark', 'model', '\\n\\n', 'observe', 'improvement', 'metric', 'sam-', '\\n', 'pling', 'spoc', 'kulal', 'et', 'al', '2019', 'consider', 'problem', '\\n\\n', 'produce', 'functionally', 'correct', 'code', 'pseudocode', '\\n', 'fix', 'budget', 'compilation', 'similar', '\\n', 'pass@k', 'metric', 'transcoder', 'lachaux', 'et', 'al', '2020', 'train', '\\n', 'system', 'translate', 'programming', 'language', '\\n', 'unsupervised', 'manner', 'observe', 'functional', '\\n', 'correctness', 'well', 'capture', 'capability', 'model', '\\n', 'bleu', 'score', 'fact', 'contracode', 'jain', 'et', 'al', '2020', '\\n', 'leverage', 'large', 'space', 'functionally', 'correct', 'program', '\\n', 'train', 'contrastive', 'code', 'model', 'improve', 'model', '\\n\\n', 'performance', 'task', 'like', 'type', 'inference', 'finally', 'robust-', '\\n', 'fill', 'devlin', 'et', 'al', '2017', 'observe', 'good', 'way', 'find', '\\n\\n', 'program', 'consistent', 'input', 'example', 'synthesize', '\\n', 'multiple', 'sample', 'beam', 'search', '\\n\\n', 'early', 'domain', 'specific', 'dataset', 'benchmark', 'neu-', '\\n', 'ral', 'programming', 'system', 'flashfill', 'gulwani', '2011', '\\n\\n', 'gulwani', 'et', 'al', '2012', 'hearthstone', 'ling', 'et', 'al', '2016', '\\n', 'community', 'trend', 'broad', '\\n', 'difficult', 'dataset', 'barone', 'sennrich', '2017', 'propose', '\\n', 'large', 'training', 'evaluation', 'dataset', 'consist', 'python', '\\n', 'declaration', 'docstring', 'body', 'scrape', 'github', '\\n', 'codesearchnet', 'challenge', 'husain', 'et', 'al', '2019', 'build', '\\n', 'large', 'corpus', 'github', 'datum', 'multiple', '\\n', 'popular', 'programming', 'language', 'recently', 'codexglue', '\\n\\n', 'lu', 'et', 'al', '2021', 'aggregate', 'programming', 'bench-', '\\n', 'mark', 'make', 'use', 'recently', 'propose', 'codebleu', '\\n\\n', 'metric', 'ren', 'et', 'al', '2020', 'relevant', 'evaluation', '\\n', 'work', 'apps', 'hendrycks', 'et', 'al', '2021', 'benchmark', '\\n', 'measure', 'functional', 'correctness', 'base', 'problem', '\\n', 'competitive', 'programming', 'website', 'codeforces', '\\n\\n', 'finally', 'note', 'code', 'broad', 'activity', 'in-', '\\n', 'volve', 'synthesize', 'code', 'docstring', '\\n\\n', 'tufano', 'et', 'al', '2020', 'use', 'transformer', 'generate', 'unit', 'test', '\\n', 'code', 'outperform', 'commercial', 'offering', 'aye', '\\n\\n', 'et', 'al', '2021', 'build', 'internal', 'auto', 'complete', 'tool', 'face-', '\\n', 'book', 'find', 'training', 'accept', 'user', 'completion', '\\n\\n', 'boost', 'system', 'performance', 'development', 'entail', 'lo-', '\\n', 'cat', 'fix', 'bug', 'early', 'work', 'static', 'dynamic', '\\n\\n', 'code', 'analysis', 'agrawal', 'et', 'al', '1995', 'korel', 'rilling', '1997', '\\n', 'learn', 'association', 'rule', 'jeffrey', 'et', 'al', '2009', 'genetic', '\\n', 'programming', 'goues', 'et', 'al', '2012', 'debug', 'faulty', 'code', '\\n', 'approach', 'rely', 'run', 'test', 'suite', '\\n', 'evaluate', 'correctness', 'suggestion', '\\n', 'expose', 'problem', 'execution', 'trace', 'search', 'solution', '\\n', 'recent', 'work', 'tufano', 'et', 'al', '2019', 'drain', 'et', 'al', '2021', '\\n', 'consider', 'bug', 'fixing', 'neural', 'machine', 'translation', '\\n', 'buggy', 'correct', 'program', 'work', '\\n\\n', 'exact', 'match', 'reference', 'instead', 'functional', 'cor-', '\\n', 'rectness', 'cite', 'qi', 'et', 'al', '2015', 'find', '\\n\\n', 'propose', 'solution', 'genetic', 'search', 'goues', 'et', 'al', '2012', '\\n', 'pass', 'weak', 'test', 'suite', 'delete', 'functionality', '\\n', 'fail', 'human', 'developer', 'write', 'test', 'suite', '\\n', 'limited', 'target', 'coverage', 'work', '\\n', 'algorithm', 'highlight', 'challenge', '\\n', 'evaluate', 'correctness', 'program', '\\n', '9', 'conclusion', '\\n\\n', 'investigate', 'possible', 'train', 'large', 'lan-', '\\n', 'guage', 'model', 'produce', 'functionally', 'correct', 'code', 'body', '\\n\\n', 'natural', 'language', 'docstring', 'fine', 'tune', 'gpt', '\\n', 'code', 'github', 'find', 'model', 'display', '\\n', 'strong', 'performance', 'dataset', 'human', 'write', 'problem', '\\n', 'difficulty', 'level', 'comparable', 'easy', 'interview', 'problem', '\\n', 'model', 'performance', 'improve', 'training', '\\n', 'distribution', 'similar', 'evaluation', 'set', '\\n', 'produce', 'multiple', 'sample', 'model', 'find', '\\n', 'simple', 'train', 'model', 'complete', 'reverse', '\\n\\n', 'evaluate', 'large', 'language', 'models', 'train', 'code', '\\n\\n', 'task', 'produce', 'docstring', 'code', 'body', '\\n', 'performance', 'profile', 'model', 'similar', 'finally', '\\n', 'expand', 'broad', 'impact', 'code', 'generating', '\\n', 'model', 'discuss', 'model', 'limitation', 'find', 'significant', '\\n', 'room', 'improvement']\n",
            "generated_topics_processed:[['transformer'], ['code', 'generation'], ['functional', 'correctness'], ['natural', 'language', 'processing'], ['program', 'synthesis'], ['neural', 'networks'], ['machine', 'learning'], ['code', 'search'], ['test', 'suites'], ['bug', 'fixing'], ['genetic', 'programming'], ['code', 'evaluation'], [' ', 'github'], ['apps'], ['codeforce'], ['flashfill'], ['hearthstone'], ['robust', 'fill'], ['codebert'], ['pymt5'], ['t5', 'objective'], ['pseudocode'], ['unsupervised', 'translation'], ['contrastive', 'code', 'model']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py:636: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Scores: [0.8094543616023125, 0.7723977975196783, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8006068308105746, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.7836173126366838, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125]\n",
            "Similarity Scores: [0.1410338666883787, 0.7620050436457337, 0.4871886095959882, 0.7610897644097143, 0.7053743592409473, 0.47644301862481675, 0.7249934443864524, 0.7592191340723908, 0.5891625298870745, 0.4060535734621584, 0.6665172594578315, 0.7347238878817562, -0.07419335777559, 0.20664233562396395, 1, 1, -0.3530848381418863, 0.6548273361926876, 1, 1, 0.38261442329873474, -0.013808379626013778, 0.4298215390004536, 0.6800305780127143]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel, KeyedVectors\n",
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "for metric in ['c_v', 'c_uci']:  # Include c_uci\n",
        "    coherence_model = gensim.models.CoherenceModel(\n",
        "        model=lda, texts=generated_topics_processed,\n",
        "        dictionary=dictionary, coherence=metric\n",
        "    )\n",
        "    coherence_scores = coherence_model.get_coherence_per_topic()\n",
        "    print(f\"{metric.upper()} Scores:\", coherence_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shyB5FqeJGKQ",
        "outputId": "e5591d6e-53e8-4e50-8f48-9ac37fe85975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_V Scores: [0.8094543616023125, 0.7723977975196783, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8006068308105746, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.7836173126366838, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125, 0.8094543616023125]\n",
            "C_UCI Scores: [-20.631414316137796, -20.2552444409739, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.512644096611844, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.38394426879287, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796, -20.631414316137796]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "x_LAnMPmdLi7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}